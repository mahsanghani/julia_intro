{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "systematic-certification",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Hello, and welcome! We're excited to be your gateway into machine learning. ML is a rapidly growing field that's buzzing with opportunity. Why? Because the tools and skills employed by ML specialists are extremely powerful and allow them to draw conclusions from large data sets quickly and with relative ease. \n",
    "\n",
    "Take the Celeste project, for example. This is a project that took 178 **tera**bytes of data on the visible sky and used it to catalogue 188 millions stars and galaxies. \"Cataloguing\" these stars meant identifying characteristics like their locations, colors, sizes, and morphologies. This is an amazing feat, *especially* because this entire calculation took under 15 minutes.\n",
    "\n",
    "<img src=\"data/Celeste.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n",
    "How are Celeste's calculations so fast? To achieve performance on this scale, the Celeste team uses the Julia programming language to write their software and supercomputers from Lawrence Berkeley National Lab's NERSC as their hardware. In this course, we unfortunately won't be able to give you access to a top 10 supercomputer, but we will teach you how to use Julia!\n",
    "\n",
    "We're confident that this course will put you on your way to understanding many of the important concepts and \"buzz words\" in ML. To get you started, we'll teach you how to teach a machine to tell the difference between images of apples and bananas, i.e to **classify** images as being one or the other type of fruit.\n",
    "\n",
    "Like Project Celeste, we'll use the [Julia programming language](https://julialang.org/) to do this. In particular, we'll be working in [Jupyter notebooks](http://jupyter.org/) like this one! (Perhaps you already know that the \"ju\" in Jupyter comes from Julia.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-capability",
   "metadata": {},
   "source": [
    "## What do the images we want to classify look like?\n",
    "\n",
    "We can use the `Images.jl` package in Julia to load sample images from this dataset. Most of the data we will use live in the `daa` folder in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"Statistics\")\n",
    "using Images\n",
    "using Plots\n",
    "using Statistics\n",
    "\n",
    "apple = load(\"data/10_100.jpg\")\n",
    "banana = load(\"data/104_100.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-programmer",
   "metadata": {},
   "source": [
    "The ultimate goal is to feed one of these images to the computer and for it to identify whether the image represents an apple or a banana!  To do so, we will **train** the computer to learn **for itself** how to \n",
    "distinguish the two images.\n",
    "\n",
    "The following notebooks will walk you step by step through the underlying math and machine learning concepts you need to know in order to accomplish this classification.\n",
    "\n",
    "They alternate between two different types of notebooks: those labelled **ML** (Machine Learning), which are designed to give a high-level overview of the concepts we need for machine learning, but which gloss over some of the technical details; and those labelled **Tools**, which dive into the details of coding in Julia that will be key to actually implement the machine learning algorithms ourselves.\n",
    "\n",
    "The notebooks contain many **Exercises**. By doing these exercises in Julia, you will learn the basics of machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-sacramento",
   "metadata": {},
   "source": [
    "## Course outline\n",
    "\n",
    "- [01. Representing data in a computer](01. ML - Representing data in a computer.ipynb)\n",
    "- [02. Using arrays to store data](02. Tools - Using arrays to store data.ipynb)\n",
    "\n",
    "- [03. Representing data with models](03. ML - Representing data with models.ipynb)\n",
    "- [04. Functions](04. Tools - Functions.ipynb)\n",
    "- [05. Building models](05. ML - Building models.ipynb)\n",
    "- [06. Adding a function parameter](06. Tools - Adding a function parameter.ipynb)\n",
    "- [07. Model complexity](07. ML - Model complexity.ipynb)\n",
    "- [08. Multiple function parameters](08. Tools - Multiple function parameters.ipynb)\n",
    "- [09. What is learning](09. ML - What is learning.ipynb)\n",
    "- [10. Minimizing functions - how a computer learns](10. Tools - Minimizing functions - how a computer learns.ipynb)\n",
    "- [11. Intro to Neurons](11. ML - Intro to Neurons.ipynb)\n",
    "- [12. Learning with a single neuron](12. Tools - Learning with a single neuron.ipynb)\n",
    "- [13. Intro to Flux.jl](13. ML - Intro to Flux.jl.ipynb)\n",
    "- [14. Learning with a single neuron using Flux.jl](14. Tools - Learning with a single neuron using Flux.jl.ipynb)\n",
    "- [15. Intro to neural networks](15. ML - Intro to neural networks.ipynb)\n",
    "- [16. Using Flux to build a single layer neural net](16. Tools - Using Flux to build a single layer neural net.ipynb)\n",
    "- [17. Introduction to deep learning](17. ML - Introduction to deep learning.ipynb)\n",
    "- [18. Multi-layer neural networks with Flux](18. Tools - Multi-layer neural networks with Flux.ipynb)\n",
    "- [19. Recognizing handwriting using a neural network](19. Recognizing handwriting using a neural network.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-promise",
   "metadata": {},
   "source": [
    "# Representing data in a computer\n",
    "\n",
    "The core of data science and machine learning is **data**: we are interested in extracting knowledge from data. \n",
    "\n",
    "But how exactly do computers represent data? Let's find out exactly what an \"artificial intelligence\" has at its disposal to learn from.\n",
    "\n",
    "## Data is represented as arrays\n",
    "\n",
    "Let's take a look at some fruit. Using the `Images.jl` library, we can load in some images:\n",
    "\n",
    "Here we have images of apples and bananas. We would eventually like to build a program that can automatically distinguish between the two. However, the computer doesn't \"see\" an apple or a banana; instead, it just sees numbers. \n",
    "\n",
    "An image is encoded in something called an **array**, which is like a container that has boxes or slots for individual pieces of data:\n",
    "\n",
    "![attachment:array_cartoon.png](data/array_cartoon.png)\n",
    "\n",
    "An array is a bunch of numbers in connected boxes; the figure above shows a 1-dimensional array. Our images are instead 2-dimensional arrays, or matrices, of numbers, arranged something like this:\n",
    "\n",
    "![attachment:array2d.png](data/array2d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1 2 3; 4 5 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "size(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(typeof(apple[40, 60]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple[18:20, 29:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-brook",
   "metadata": {},
   "source": [
    "We see that Julia displays a coloured box! Julia, via the `Colors.jl` package, is clever enough to display colours in a way that is useful to us humans!\n",
    "\n",
    "So, in fact, an image is a 2D array, in which each element of the array is an object (a collection of numbers) describing a coloured pixel.\n",
    "\n",
    "## Colors as numbers\n",
    "\n",
    "How, then, are these colors actually stored? Computers store colors in RGB format, that is they store a value between 0 and 1 for each of three \"channels\": red, green, and blue. Here, 0 means none of that color and 1 means the brightest form of that color. The overall color is a combination of those three colors. \n",
    "\n",
    "For example, we can pull out the `red` value using the function `red` applied to the color. Since internally the actual value is stored in a special format, we choose to convert it to a standard floating-point number using the `Float64` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "Float64(red(apple[40, 60]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "[mean(float.(c.(img))) for c = [red,green,blue], img = [apple,banana]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "histogram(float.(green.(apple[:])), color=\"red\",label=\"apple\",normalize=true,nbins=25)\n",
    "histogram(float.(green.(banana[:])), color=\"red\",label=\"banana\",normalize=true,nbins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(red(banana[50,20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "banana[50,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = apple[40,60]\n",
    "\n",
    "red_value = Float64(red(pixel))\n",
    "green_value = Float64(green(pixel))\n",
    "blue_value = Float64(blue(pixel))\n",
    "\n",
    "print(\"The RGB value are ($red_value, $green_value, $blue_value)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = banana[1,1]\n",
    "\n",
    "red_value = Float64(red(pixel))\n",
    "green_value = Float64(green(pixel))\n",
    "blue_value = Float64(blue(pixel))\n",
    "\n",
    "print(\"The RGB value are ($red_value, $green_value, $blue_value)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "redpartofapple = Float64.(red.(apple))\n",
    "mean(redpartofapple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(redpartofapple[:],color=\"red\",label=\"redness in the apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(Float64.(red.(apple)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(Float64.(red.(banana)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-finger",
   "metadata": {},
   "source": [
    "# Tools - Using arrays to store data\n",
    "\n",
    "### Introduction to arrays\n",
    "\n",
    "**Arrays are collections of boxes that we can use to store data.** In the last notebook, we saw an image that can help us to picture a 1-dimensional array:\n",
    "\n",
    "<img src=\"data/array_cartoon.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "*Why do we want an object like this to store our data?*\n",
    "\n",
    "An alternative to using an array in some contexts would be to name every individual piece of data, as follows:\n",
    "\n",
    "```julia\n",
    "a = 1.1\n",
    "b = 2.2\n",
    "c = 3.3\n",
    "```\n",
    "\n",
    "We can visualize how this data is stored:\n",
    "\n",
    "<img src=\"data/without_arrays.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "This is like having a separate box for every piece of data, rather than a series of connected boxes for all our data.\n",
    "\n",
    "The more data we have, the more annoying it becomes to keep track of all these boxes and their names. Furthermore, if we want to do the same thing with many pieces of data, it's much easier to put all of these pieces of data in one place to work with them at once.\n",
    "\n",
    "For example, we may want to multiply `a`, `b`, and `c` by `2`. We could multiply three times:\n",
    "\n",
    "```julia\n",
    "a * 2\n",
    "b * 2\n",
    "c * 2\n",
    "```\n",
    "\n",
    "Or, instead, we could create one array (let's call it `numbers`) and multiply that array by `2`:\n",
    "```julia\n",
    "numbers * 2\n",
    "```\n",
    "\n",
    "The syntax for creating this array, `numbers`, is\n",
    "\n",
    "```julia\n",
    "numbers = [a, b, c]\n",
    "```\n",
    "\n",
    "Or, we could have just written\n",
    "\n",
    "```julia\n",
    "numbers = [1.1, 2.2, 3.3]\n",
    "```\n",
    "\n",
    "It's worth noting that in Julia, 1-dimensional arrays are also called \"vectors\".\n",
    "\n",
    "### Creating arrays\n",
    "\n",
    "In the last section, we saw that we could create the array `numbers` by typing our elements, `a`, `b`, and `c` (or `1.1`, `2.2`, and `3.3`), inside square brackets.\n",
    "\n",
    "#### Exercise 1 \n",
    "\n",
    "Create an array called `first_array` that stores the numbers 10 through 20.\n",
    "\n",
    "Reminder: ESC+b to open a box below.\n",
    "\n",
    "### Array comprehensions\n",
    "\n",
    "Alternatively we can create arrays via *\"array comprehensions\"*. This is a nice way to automate creating an array, if you don't want to have to type long lists of numbers inside square brackets.\n",
    "\n",
    "In an array comprehension, you write code inside square brackets that will generate the array as that code is executed. Let's see an example.\n",
    "\n",
    "```julia\n",
    "counting = [2 * i for i in 1:10]\n",
    "```\n",
    "\n",
    "The above line of code is saying that we want to create an array called `counting` that stores two times each of the integers between 1 and 10. This array comprehension has a few different parts, so let's dissect them:\n",
    "\n",
    "<img src=\"data/array_comprehension.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "If we wanted to create another array, `roots`, that stored the square roots of all integers between 1 and 10, we could modify the code above to\n",
    "\n",
    "```julia\n",
    "roots = [sqrt(i) for i in 1:10]\n",
    "```\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Create an array, `squares`, that stores the squares of all integers between 1 and 100.\n",
    "\n",
    "### Looking inside an array\n",
    "\n",
    "We can \"index\" into an array to grab contents inside the array at a particular position. To index into our `counting` array, we place square brackets after the name of the array and put the position number of the element/data we want inside those brackets.\n",
    "\n",
    "For example, we might say\n",
    "\n",
    "```julia\n",
    "counting[3]\n",
    "```\n",
    "\n",
    "to grab the third element in the array called `counting`.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Execute the code in the next cell. It will generate an array called `myprimes` that stores the first 168 prime numbers. Index into `myprimes` to grab the 89th smallest prime. What is this prime?\n",
    "\n",
    "### Slicing\n",
    "\n",
    "Instead of grabbing a single number in an array, we can also take a **slice** of an array, i.e. a subset of the array, which can include multiple values. To take a slice of `counting` that includes the 3rd, 4th, and 5th entries, we use the following syntax with a colon (`:`):\n",
    "\n",
    "```julia\n",
    "counting[3:5]\n",
    "```\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Index into `myprimes` to grab the 89th through the 99th smallest primes (inclusive).\n",
    "\n",
    "### Modifying an array\n",
    "\n",
    "In this section, we'll see how to edit an array we've already created. We'll see how to\n",
    "\n",
    "1) Update an item at an existing position in an array;\n",
    "\n",
    "2) Add an item to the end of an array;\n",
    "\n",
    "3) Remove an item from the end of an array.\n",
    "\n",
    "#### Update an item at an existing position in an array\n",
    "\n",
    "First off, we can update an item at an existing position in an array by indexing into the array and changing the value at the desired index. For example, let's say we have an array called `myfriends`:\n",
    "\n",
    "```julia\n",
    "myfriends = [\"Ted\", \"Robin\", \"Barney\", \"Lily\", \"Marshall\"]\n",
    "```\n",
    "\n",
    "We can grab Barney's name from `myfriends` via\n",
    "\n",
    "```julia\n",
    "myfriends[3]\n",
    "```\n",
    "\n",
    "and we can change \"Barney\" to something else by reassigning `myfriends[3]`:\n",
    "\n",
    "```julia\n",
    "myfriends[3] = \"Baby Bop\"\n",
    "```\n",
    "\n",
    "Note that a single `=` assigns a new variable to the value on the left of the `=` sign.\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "Use an array comprehension to create an array, `mysequence`, that stores the numbers 4 through 10. Index into `mysequence` and update it to replace the last element, `10`, with `11`.\n",
    "\n",
    "#### Add an item to the end of an array\n",
    "\n",
    "We can add an item to the end of an array with the `push!` function. Don't worry about the exclamation mark at the end of `push!`; we'll talk about why that's there later, when we discuss functions.\n",
    "\n",
    "For now, note that when you call `push!` on an array and a value, you add that value to the end of the array. For example,\n",
    "\n",
    "```julia\n",
    "push!(counting, 1000)\n",
    "```\n",
    "\n",
    "will turn `counting`, which was formerly \n",
    "\n",
    "```julia\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "``` \n",
    "into \n",
    "```julia\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1000]\n",
    "```\n",
    "\n",
    "Prove to yourself this works!\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Copy the following code to declare the array `fibonacci`:\n",
    "\n",
    "```julia\n",
    "fibonacci = [1, 1, 2, 3, 5, 8, 13]\n",
    "```\n",
    "\n",
    "Use `push!` to add `21` to `fibonacci` after the number `13`.\n",
    "\n",
    "#### Remove an item from the end of an array\n",
    "\n",
    "To remove an item from the end of an array, use the `pop!` function. When using `pop!` on arrays, `pop!` only needs one input argument, namely the array you want to change. For example, if\n",
    "\n",
    "```julia\n",
    "counting == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1000]\n",
    "```\n",
    "\n",
    "then\n",
    "```julia\n",
    "pop(counting)\n",
    "```\n",
    "will remove `1000` from the end of `counting`.\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Use `pop!` to remove `21` from `fibonacci`. What does this function call return?\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "What is the last element of `fibonacci`? Try\n",
    "\n",
    "```julia\n",
    "fibonacci[end]\n",
    "```\n",
    "\n",
    "### Arrays of arrays and multidimensional arrays\n",
    "\n",
    "So far we've only seen 1D arrays of scalars, but arrays can also store other arrays and can have an arbitrary number of dimensions. \n",
    "<br><br>\n",
    "For example, the following are arrays of arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Primes\n",
    "\n",
    "myprimes = primes(1000); \n",
    "# The semicolon suppresses the output, try removing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "favorites = [ [\"koobideh\", \"chocolate\", \"eggs\"], [\"penguins\", \"cats\", \"sugargliders\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [ [1, 2, 3], [4, 5], [6, 7, 8, 9] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand(4,3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-emphasis",
   "metadata": {},
   "source": [
    "If we want to grab a value from a 2D array, we index into the array, specifying the row and column of the element of interest! For example, if we have the array `A` given by\n",
    "\n",
    "```julia\n",
    "A = [ 1  2  3\n",
    "      4  5  6\n",
    "      7  8  9 ]\n",
    " ```\n",
    " we could grab the number 6 by saying\n",
    " \n",
    " ```\n",
    " A[2, 3]\n",
    " ```\n",
    " \n",
    " since 6 is in the 2nd row and 3rd column of `A`.\n",
    " \n",
    " #### Exercise 8\n",
    " \n",
    " Copy and execute the following code to get an array, `myprimematrix`, which stores the first 100 primes\n",
    " \n",
    " ```\n",
    " myprimematrix = reshape(primes(541), (10, 10))\n",
    " ```\n",
    " \n",
    " Grab the prime in the 8th row and 5th column via indexing. What is the value of this prime?\n",
    " \n",
    " ### Copying arrays\n",
    "\n",
    "Be careful when you want to copy arrays!\n",
    "\n",
    "Let's say you want to copy an existing array, like `fibonacci`, to an array called `somenumbers`. Remember how we defined `fibonacci`:\n",
    "\n",
    "```\n",
    "fibonacci = [1, 1, 2, 3, 5, 8, 13]\n",
    "```julia\n",
    "\n",
    "What if we try to copy fibonacci by saying\n",
    "\n",
    "```\n",
    "somenumbers = fibonacci\n",
    "```julia\n",
    "?\n",
    "Execute this code to try to copy fibonacci. Look at `somenumbers` to see if it stores what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "fibonacci = [1, 1, 2, 3, 5, 8, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "somenumbers = fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "somenumbers[1]=404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "fibonacci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-prerequisite",
   "metadata": {},
   "source": [
    "#### Exercise 9\n",
    "\n",
    "What is the first element in `fibonacci`?\n",
    "\n",
    "### Copying or not?\n",
    "\n",
    "Did copying `fibonacci` like this work?\n",
    "\n",
    "No, unfortunately not. When we tried to copy, all we did was give `fibonacci` a new name, `somenumbers`. Now when we update `somenumbers`, we're also updating `fibonacci` because they are the same object!\n",
    "\n",
    "If we actually want to make a *copy* of the array bound to `fibonacci`, we can use the `copy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, restore fibonacci\n",
    "\n",
    "fibonacci[1] = 1\n",
    "fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "somemorenumbers = copy(fibonacci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "somemorenumbers[1] = 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "fibonacci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-economy",
   "metadata": {},
   "source": [
    "In this last example, fibonacci was not updated. Therefore we see that the arrays bound to `somemorenumbers` and `fibonacci` are distinct.\n",
    "\n",
    "#### Exercise 10\n",
    "\n",
    "Copy `myprimematrix` to `mynewprimematrix`. Update `mynewprimematrix[3,3]` to `1234`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-example",
   "metadata": {},
   "source": [
    "# Modeling data 1\n",
    "\n",
    "Machine learning and data science is about modeling data. **Modeling** is the representation of an idea with some parameters and  a mathematical representation which we will encode in software. All machine learning methods are about training a computer to fit a model to some data. Even the fanciest neural networks are simply choices for models. In this notebook, we will begin to start building our first computational model of data.\n",
    "\n",
    "## Modeling data is hard!\n",
    "\n",
    "Let's pick up where we left off in notebook 1 with fruit. We were left with a riddle: when we load images of apples and bananas,\n",
    "\n",
    "and then compare their average value for the color red, we end up with something that is perhaps surprising:\n",
    "\n",
    "We see that the banana's mean red value is higher than the apple's, even though the apple looks much redder. Can you guess why? \n",
    "\n",
    "There are actually two reasons. One of the reasons is the background: the image of the banana has a lot more white background than the apple, and that white background has a red value of 1! In our minds we ignore the background and say \"the banana is bright yellow, the apple is dark red\", but a computer just has a bundle of numbers and does not know where it should be looking.\n",
    "\n",
    "The other issue is that \"bright yellow\" isn't a color that exists in a computer. The computer has three colors: red, green, and blue. \"Bright yellow\" in a computer is a mixture of red and green, and it just so happens that to get this color yellow, it needs more red than the apple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"Statistics\")\n",
    "using Images\n",
    "using Statistics\n",
    "\n",
    "apple = load(\"data/10_100.jpg\")\n",
    "banana = load(\"data/104_100.jpg\")\n",
    "\n",
    "apple_red_amount = mean(Float64.(red.(apple)))\n",
    "banana_red_amount = mean(Float64.(red.(banana)))\n",
    "\n",
    "apple_green_amount = mean(Float64.(green.(apple)))\n",
    "banana_green_amount = mean(Float64.(green.(banana)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The average value of red in the apple is $apple_red_amount, \" *\n",
    "\"while the average value of red in the banana is $banana_red_amount.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The average value of green in the apple is $apple_green_amount, \" *\n",
    "\"while the average value of green in the banana is $banana_green_amount.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The amount of red in the apple at (60,60) is $(Float64(red(apple[60,60]))), \" *\n",
    "\"while the amount of red in the banana at (60,60) is $(Float64(red(banana[60,60]))).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple[60,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "banana[60,60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-government",
   "metadata": {},
   "source": [
    "### A note on string interpolation\n",
    "\n",
    "In the last two input cells, we *interpolated a string*. This means that when we write the string using quotation marks (`\"  \"`), we insert a placeholder for some **value** we want the string to include. When the string is evaluated, the value we want the string to include replaces the placeholder. For example, in the following string,\n",
    "\n",
    "```julia\n",
    "mystring = \"The average value of red in the apple is $apple_red_amount\"\n",
    "```\n",
    "\n",
    "`$apple_red_amount` is a placeholder for the value stored in the variable `apple_red_amount`. Julia knows that we want to use the value bound to the variable `apple_red_amount` and *not* the word \"apple_red_amount\" because of the dollar sign, `$`, that comes before `apple_red_amount`.\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Execute the following code to see what the dollar sign does:\n",
    "\n",
    "```julia\n",
    "mypi = 3.14159\n",
    "println(\"I have a variable called mypi that has a value of $mypi.\")\n",
    "```\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Alter and execute the code that creates `mystring` below \n",
    "\n",
    "```julia\n",
    "apple_blue_amount = mean(Float64.(blue.(apple)))\n",
    "mystring = \"The average amount of blue in the apple is apple_blue_amount\"\n",
    "```\n",
    "\n",
    "so that `println(mystring)` prints a string that reports the mean value of blue coloration in our image of an apple.\n",
    "\n",
    "## Take some time to think about the data\n",
    "\n",
    "Apples and bananas are very different, but how could we use the array of RGB values (which is how the images are represented in the computer, as we saw in notebook 1) to tell the difference between the two? Here are some quick ideas:\n",
    "\n",
    "- We could use the shape of the object in the image. But how can we encode ideas about shape from an array?\n",
    "- We could use the size of the object in the image. But how do we calculate that size?\n",
    "- We could use another color, or combinations of colors, from the image. Which colors?\n",
    "\n",
    "Let's go with the last route. The banana is yellow, which is a combination of red and green, while the apple is red. This means that the color that clearly differentiates between the two is not red, but green!\n",
    "\n",
    "The processes that we just went through are assigned fancy names: feature selection and data munging. \n",
    "\n",
    "**Feature selection** is the process of subsetting the data to a more relevant and informative set. We took the full image data and decided to select out the green channel. \n",
    "\n",
    "**Data munging** is transforming the data into a format more suitable for modeling. Here, instead of keeping the full green channel, we transformed it down to a single data point: the average amount of green.\n",
    "\n",
    "## Building a model\n",
    "\n",
    "We want to model the connection between \"the average amount of green\" and \"is an apple or banana\". \n",
    "\n",
    "<img src=\"data/data_flow.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "This model is a mathematical function which takes in our data and spits out a number that we will interpret as \"is an apple\" or \"is a banana\".\n",
    "\n",
    "<img src=\"data/what_is_model.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "We will interpret the output of the function as \"is an apple\" if the output is close to 0, and \"is a banana\" if it's close to 1. Anything in the middle is something we are unsure about. Here we're using a mathematical function to perform a **classification**.\n",
    "\n",
    "Knowing how to declare and work with functions will allow us to model our data in the coming sections, so this is the subject of the next notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-variance",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "\n",
    "In the last notebook, we talked about modeling data with functions. A **function** is one of the most fundamental concepts in computing (and also in mathematics). \n",
    "\n",
    "A function is a piece of a program that receives **input arguments**, processes them by doing certain calculations on them, and returns **outputs**.\n",
    "\n",
    "For example, we might have a function `g` that takes a number as an input and returns the square of that number as an output. How can we define this function `g` on a computer? Julia gives us a few different ways to do this.\n",
    "\n",
    "### Defining functions\n",
    "\n",
    "Firstly, we could write `g` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x) = x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Machine learning is fun\"\n",
    "g(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-momentum",
   "metadata": {},
   "source": [
    "Alternatively, we could declare this function using the `function` and `end` keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "function g1(x)\n",
    "    return x^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-brisbane",
   "metadata": {},
   "source": [
    "The third way we could have declared this function is as an \"anonymous\" or \"lambda\" function. \"Anonymous\" functions are functions that truly don't need names! For example, we could have declared a function that squares its input as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x -> x^2)(\"I ♡ Julia. \") # \\heartsuit + <tab>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-large",
   "metadata": {},
   "source": [
    "Now that we've done that, we can't access the function `x -> x^2` again because we have no name to call! That seems a little silly, doesn't it?\n",
    "\n",
    "Actually, there are times where functions without names are useful to us. We'll see that later in this notebook. For now, note that you have the *option* to access an \"anonymous\" function later by binding a variable to it when you declare it. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = x -> x^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-bruce",
   "metadata": {},
   "source": [
    "This syntax says, \"I want to use the variable g to access a function that takes some input called `x` and maps that input to the square of `x`.\n",
    "\n",
    "## An important sigmoidal function\n",
    "\n",
    "A particular function that is used a lot in machine learning is a so-called \"sigmoidal\" function (meaning a function that is S-shaped, i.e. the graph of the function looks like an `S`).\n",
    "\n",
    "The sigmoid function that we will use is given the name $\\sigma$, and is defined by the following mathematical expression:\n",
    "\n",
    "$$\\sigma(x) := \\frac{1}{1 + \\exp(-x)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "σ(w*x) = 1/ (1+e^(-w*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-ridge",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Use the first syntax given above to define the function `σ` in Julia. Note that Julia actually allows us to use the symbol σ as a variable name! To do so, type `\\sigma<TAB>` in the code cell.\n",
    "\n",
    "## Plotting functions\n",
    "\n",
    "Let's draw the function σ to see what it looks like. Throughout this course, we'll use the Julia package `Plots.jl` for all of the graphics. This package provides a flexible syntax for plotting, in which options to change attributes like the width of the lines used in the figure are given as named keyword arguments.\n",
    "\n",
    "In addition, it allows us to use different \"backends\", which are the other libraries that actually carry out the plotting following the instructions from `Plots.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Interact\")\n",
    "Pkg.add(\"Plots\")\n",
    "using Interact\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(σ, -5, 5)\n",
    "\n",
    "hline!([0, 1], ls=:dash, lw=3)  # add horizontal lines at 0 and 1, with dashed style and linewidth 3\n",
    "vline!([0], ls=:dash, lw=3)     # add a vertical line at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-dairy",
   "metadata": {},
   "source": [
    "mWe can think of $\\sigma$ as a smooth version of a step or threshold function (often called a \"Heaviside\" function). To see this, let's modify the steepness of the jump in $\\sigma$ and compare it to the Heaviside function; we'll see how all this works in more detail later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "heaviside(x) = x < 0 ? 0.0 : 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "@manipulate for w in 0.1:0.1:20\n",
    "    plot(x -> σ(w*x), -5, 5, label=\"sigma\", lw=2)\n",
    "    plot!(heaviside, ls=:dash, label=\"step\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1 2 3;4 5 6]\n",
    "f(t) = √t + 5\n",
    "f.(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-italy",
   "metadata": {},
   "source": [
    "This particular function takes any real number as input, and gives an output between $0$ and $1$. It is continuous and smooth.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Declare the sigmoid function above as an anonymous function with a different name.\n",
    "\n",
    "### Mutating functions: `...!`\n",
    "\n",
    "To generate our plot of σ above, we used some functions that end with `!`. What does a `!` at the end of a function name mean in Julia?\n",
    "\n",
    "Functions that change or modify their inputs are called **mutating functions**. But wait, don't all functions do that?\n",
    "\n",
    "No, actually. Functions typically take *inputs* and use those *inputs* to generate *outputs*, but the inputs themselves usually don't actually get changed by a function. For example, copy and execute the following code:\n",
    "\n",
    "```julia\n",
    "v1 = [9, 4, 7, 11]\n",
    "v2 = sort(v1)\n",
    "```\n",
    "\n",
    "`v2` is a sorted version of `v1`, but after calling `sort`, `v1` is still unsorted.\n",
    "\n",
    "However, now trying adding an exclamation point after `sort` and executing the following code:\n",
    "\n",
    "```julia\n",
    "sort!(v1)\n",
    "```\n",
    "\n",
    "Look at the values in `v1` now!\n",
    "\n",
    "This time, the original vector itself was changed (mutated), and is now sorted. Unlike `sort`, `sort!` is a mutating function. Did the `!` make `sort!` mutating? Well, no, not really. In Julia, `!` indicates mutating functions by convention. When the author of `sort!` wrote `sort!`, they added a `!` to let you to know that `sort!` is mutating, but the `!` isn't what makes a function mutating or non-mutating in the first place.\n",
    " \n",
    "#### Exercise\n",
    "\n",
    "Some of our plotting commands end with `!`. Copy and execute the following code:\n",
    "\n",
    "```julia\n",
    "r = -5:0.1:5\n",
    "g(x) = x^2\n",
    "h(x) = x^3\n",
    "plot(r, g, label=\"g\")\n",
    "plot!(r, h, label=\"h\")\n",
    "```\n",
    "\n",
    "Then change the code slightly to remove the `!` after `plot!(r, h)`. How does this change your output? What do you think it means to add `!` after plotting commands?\n",
    "\n",
    "## Pointwise application of functions, `.(...)` (known as \"broadcasting\")\n",
    "\n",
    "We saw in a previous notebook that we needed to add `.` after the names of some functions, as in \n",
    "\n",
    "```julia\n",
    "green_amount = mean(Float64.(green.(apple)))\n",
    "```\n",
    "\n",
    "What are those extra `.`s really doing?\n",
    "\n",
    "When we add a `.` after a function's name, we are telling Julia that we want to \"**broadcast**\" that function over the inputs passed to the function. This means that we want to apply that function *element-wise* over the inputs; in other words, it will apply the function to each element of the input, and return an array with the newly-calculated values.\n",
    "\n",
    "For example, copy and execute the following code:\n",
    "```julia\n",
    "g.(r)\n",
    "```\n",
    "Since the function `g` squares it's input, this squares all the elements of the range `r`.\n",
    "\n",
    "What happens if instead we just call `g` on `r` via\n",
    "\n",
    "```julia\n",
    "g(r)\n",
    "```\n",
    "? Try this out and see what happens.\n",
    "\n",
    "You should see an error message after calling `g(r)`, which says that Julia cannot multiply two vectors. When we call `g(r)`, we ask Julia to multiply `r` by `r`. When we call `g.(r)`, we ask Julia to multiply *each element* in `r` by itself.\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Copy and execute the following code to get the type of the object `numbers = [1, 2, \"three\", 4.0]`:\n",
    "\n",
    "```julia\n",
    "numbers = [1, 2, \"three\", 4.0]\n",
    "typeof(numbers)\n",
    "```\n",
    "\n",
    "What is the type of `numbers`?\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "Broadcast `typeof` over `numbers` to see what the types of the elements stored inside `numbers` are.\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Write a `for` loop that applies `g` to each of the elements of `r` and prints the results. Verify that the numbers printed by this `for` loop are equal to the entries of the output of `g.(r)`.\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Define a range `xs` between -5 and 5 with steps of 0.5. \n",
    "Apply the $\\sigma$ function pointwise to this range and define `ys` as the result.\n",
    "What does the result look like? Plot these as points and join them with lines.\n",
    "\n",
    "Make the plot interactive where you can vary the step size. Fix the range of the plot in the `x` and `y` directions using the functions `xlims!` and `ylims!`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-helen",
   "metadata": {},
   "source": [
    "# Modeling data 2\n",
    "\n",
    "## Building a model\n",
    "\n",
    "Recall that in notebook 3, we saw that we could use a mathematical function to classify an image as an apple or a banana, based on the average amount of green in an image:\n",
    "\n",
    "<img src=\"data/data_flow.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"data/what_is_model.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "A common function for performing this kind of **classification** is the sigmoid that we saw in the last notebook, and that we will now extend by adding two **parameters**, $w$ and $b$:\n",
    "\n",
    "$$\\sigma(x; w, b) := \\frac{1}{1 + \\exp(-wx + b)}$$\n",
    "\n",
    "$$ x = \\mathrm{data} $$\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x;w,b) &\\approx 0 \\implies \\mathrm{apple} \\\\\n",
    "\\sigma(x;w,b) &\\approx 1 \\implies \\mathrm{banana}\n",
    "\\end{align}\n",
    "\n",
    "In our mathematical notation above, the `;` in the function differentiates between the **data** and the **parameters**. `x` is the data and is determined from the image. The parameters, `w` and `b`, are numbers which we choose to make our function match the results it should be modeling.\n",
    "\n",
    "Note that in the code below, we don't distinguish between data and parameters - both are just inputs to our function, σ!\n",
    "\n",
    "What we want is that when we give σ as input the average green for the apple, roughly `x = 0.3385`, it should return as output something close to 0, meaning \"apple\". And when we give σ the input `x = 0.8808`, it should output something close to 1, meaning \"banana\".\n",
    "\n",
    "By changing the parameters of the function, we can change the shape of the function, and hence make it represent, or **fit**, the data better!\n",
    "\n",
    "## Data fitting by varying parameters\n",
    "\n",
    "We can understand how our choice of `w` and `b` affects our model by seeing how our values for `w` and `b` change the plot of the $\\sigma$ function.\n",
    "\n",
    "To do so, we will use the `Interact.jl` Julia package, which provides \"widgets\" for controlling parameters interactively via sliders:\n",
    "\n",
    "Run the code in the next cell. You should see two \"sliders\" appear, one for `w` and one for `b`.\n",
    "\n",
    "**Game**: \n",
    "Move both of those sliders around until the blue curve, labeled \"model\", which is the graph of the `\\sigma` function, passes through *both* of the data points at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Images\n",
    "\n",
    "apple = load(\"data/10_100.jpg\")\n",
    "banana = load(\"data/104_100.jpg\")\n",
    "\n",
    "apple_green_amount = mean(Float64.(green.(apple)))\n",
    "banana_green_amount = mean(Float64.(green.(banana)))\n",
    "\n",
    "\"Average green for apple = $apple_green_amount; \" *\n",
    "\"Average green for banana = $banana_green_amount; \"\n",
    "\n",
    "σ(x, w, b) = 1 / (1 + exp(-w * x + b))\n",
    "\n",
    "using Plots; gr()   # GR works better for interactive manipulations\n",
    "using Interact      # package for interactive manipulation\n",
    "\n",
    "@manipulate for w in -10:0.01:30, b in 0:0.1:20\n",
    "    \n",
    "    plot(x -> σ(x, w, b), xlim=(-0,1), ylim=(-0.1,1.1), label=\"model\", legend=:topleft, lw=3)\n",
    "    \n",
    "    scatter!([apple_green_amount],  [0.0], label=\"apple\", ms=5)   # marker size = 5\n",
    "    scatter!([banana_green_amount], [1.0], label=\"banana\", ms=5)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-chance",
   "metadata": {},
   "source": [
    "Notice that the two parameters do two very different things. The **weight**, `w`, determines *how fast* the transition between 0 and 1 occurs. It encodes how trustworthy we think our data  actually is, and in what range we should be putting points between 0 and 1 and thus calling them \"unsure\". The **bias**, `b`, encodes *where* on the $x$-axis the switch should take place. It can be seen as shifting the function left-right. We'll come to understand these *parameters* more in notebook 6.\n",
    "\n",
    "Here are some parameter choices that work well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 25.58; b = 15.6\n",
    "\n",
    "plot(x -> σ(x, w, b), xlim=(0,1), ylim=(-0.1,1.1), label=\"model\", legend=:topleft, lw=3)\n",
    "\n",
    "scatter!([apple_green_amount], [0.0], label=\"apple\")\n",
    "scatter!([banana_green_amount],[1.0], label=\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-touch",
   "metadata": {},
   "source": [
    "(Note that in this problem there are many combinations of `w` and `b` that fit the data well.)\n",
    "\n",
    "Once we have a model, we have a computational representation for how to choose between \"apple\" and \"banana\". So let's pull in some new images and see what our model says about them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple2 = load(\"data/107_100.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_amount = mean(Float64.(green.(apple2)))\n",
    "@show green_amount\n",
    "\n",
    "scatter!([green_amount], [0.0], label=\"new apple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-motivation",
   "metadata": {},
   "source": [
    "Our model successfully says that our new image is an apple! Pat yourself on the back: you've actually just trained your first neural network!\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Load the image of a banana in `data/8_100.jpg` as `mybanana`. Edit the code below to calculate the amount of green in `mybanana` and to overlay data for this image with the existing model and data points.\n",
    "\n",
    "# To get the desired overlay, the code we need is\n",
    "\n",
    "```julia\n",
    "mybanana = load(\"data/8_100.jpg\")\n",
    "mybanana_green_amount = mean(Float64.(green.(banana)))\n",
    "scatter!([mybanana_green_amount], [1.0], label=\"my banana\")\n",
    "```\n",
    "\n",
    "## Closing remarks: bigger models, more data, more accuracy\n",
    "\n",
    "That last apple should start making you think: not all apples are red; some are yellow. \"Redness\" is one attribute of being an apple, but isn't the whole thing. What we need to do is incorporate more ideas into our model by allowing more inputs. However, more inputs would mean more parameters to play with. Also, we would like to have the computer start \"learning\" on its own, instead of modifying the parameters ourselves until we think it \"looks right\". How do we take the next step?\n",
    "\n",
    "The first thing to think about is, if you wanted to incorporate more data into the model, how would you change the sigmoid function? Play around with some ideas. But also, start thinking about how you chose parameters. What process did you do to finally end up at good parameters? These two problems (working with models with more data and automatically choosing parameters) are the last remaining step to understanding deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-algebra",
   "metadata": {},
   "source": [
    "## Adding a function parameter\n",
    "\n",
    "In the last notebook, we saw an example of adding **parameters** to functions. These are values that control the behavior of a function. Let's look at that in some more detail.\n",
    "\n",
    "Let's go back to our original version of the σ function:\n",
    "\n",
    "Instead of working with a single function, we can work with a whole class (set) of functions that look similar but differ in the value of a **parameter**. Let's make a new function that uses the previous $\\sigma$ function, but also has a parameter, $w$. Mathematically, we could write\n",
    "\n",
    "$$f_w(x) = f(x; w) = \\sigma(w \\, x).$$\n",
    "\n",
    "(Here, $w$ and $x$ are multiplied in the argument of $\\sigma$; we could write $w \\times x$ or $w * x$, or even $w \\cdot x$, but usually the symbols are not written.)\n",
    "\n",
    "Mathematically speaking, we can think of $f_w$ as a different function for each different value of the parameter $w$.\n",
    "\n",
    "In Julia, we write this as follows:\n",
    "\n",
    "     f(x, w) = σ(w * x)\n",
    "\n",
    "Note that Julia just treats parameters as additional *arguments* to the function; the function `f` has two arguments, the value of `x` and the value of `w` that we want to use.\n",
    "\n",
    "We can now investigate the effect of $w$ interactively. To do so, we need a way of writing in Julia \"the function of one variable $x$ that we obtain when we fix the value of $w$\". We write this as an \"anonymous function\", as we saw in the notebook on functions:\n",
    "\n",
    "    x -> f(x, w)\n",
    "    \n",
    "We can read this as \"the function that maps $x$ to the value of $f(x, w)$, for a value of $w$ that was previously given\".\n",
    "\n",
    "Now we are ready to draw the function. For each plot, we *fix* a value of the parameter $w$ and draw the resulting function as a function of $x$. However, `Interact.jl` then allows us to modify interactively the value of $w$, and plot the new function that comes out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the graphics packages\n",
    "\n",
    "using Plots; gr()\n",
    "using Interact\n",
    "\n",
    "σ(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "f(x, w) = σ(w * x)\n",
    "\n",
    "@manipulate for w in -2:0.01:2\n",
    "    \n",
    "    plot(x->f(x, w), -5, 5, ylims=(0,1), label=\"sigmoid\")\n",
    "    plot!(x->(x>0), -5,5, label=\"Square Wave\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-oklahoma",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Try writing your own function that takes a parameter. Start by copying and executing\n",
    "\n",
    "```julia\n",
    "square(x) = x^2\n",
    "```\n",
    "\n",
    "Then use `square` to declare a new function `square_and_scale` that takes two inputs, `a` and `x` such that\n",
    "\n",
    "$$\\mathrm{square\\_and\\_scale}(x; a) := a \\cdot x^2$$\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Once you have declared `square_and_scale`, uncomment the code below and see how the parameter `a` scales the function `square` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "function square(x)\n",
    "    return x^2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "function square_and_scale(x,a)\n",
    "    return a*x^2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -10:10\n",
    "@manipulate for a in 0:0.01:10\n",
    "    plot(x, square.(x), label=\"x^2\")\n",
    "    plot!(x, square_and_scale.(x, a), ls=:dash, label=\"ax^2\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-livestock",
   "metadata": {},
   "source": [
    "## Fitting a function to data\n",
    "\n",
    "As we saw in the previous notebook, what we would like to do is use the fact that we now have a parameter in our function in order to do something useful! Namely, we want to model data with it.\n",
    "\n",
    "So suppose that we are given a single data point $(x_0, y_0) = (2, 0.8)$. We can try to \"fit\" the function $f_w$ by adjusting the parameter $w$ until the function passes through the data.\n",
    "\n",
    "**Game**: Move the slider until the graph of the function hits the data point. Which value of $w$ does that correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, y0 = 2, 0.8\n",
    "\n",
    "@manipulate for w in -2:0.01:2\n",
    "    plot(x->f(x, w), -5, 5, ylims=(0, 1), label=\"f\")\n",
    "    scatter!([x0], [y0], label=\"data\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-apache",
   "metadata": {},
   "source": [
    "## Quantifying how far we are from the goal: the *loss function*\n",
    "\n",
    "We can see visually when the graph of the function passes through the data point. But the goal is to be able to automate this process so that the computer can do it unaided. \n",
    "\n",
    "So we will need a more precise way of deciding and quantifying (i.e. measuring with a number) *how far away we are from the goal*; here, the goal means hitting the data point with the function.\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Can you think of a way of measuring how far away our function is from the data point?\n",
    "\n",
    "### Defining the loss function\n",
    "\n",
    "We need to measure how far away the curve is from the data point when we choose a particular value of $w$.\n",
    "One way to do this is by finding the vertical distance $d$ from the curve to the data point.\n",
    "\n",
    "Instead of just taking the distance, it is common to take the *square* of the distance, $d^2$.\n",
    "\n",
    "Since we are taking the vertical distance, we need the distance at the given value of $x_0$ where the data point lies. For a given value of the parameter $w$, the height of the point on the curve with that value of $x_0$ is $f(x_0, w)$.\n",
    "\n",
    "So we take\n",
    "$$d := y_0 - f(x_0, w)$$\n",
    "\n",
    "and\n",
    "$$d^2 = [y_0 - f(x_0, w)]^2.$$\n",
    "\n",
    "This is our measure of distance. It will change when $w$ changes -- in other words, it is itself a *function of $w$*; we will denote this function by $L(w)$, and call it the **loss function**:\n",
    "\n",
    "$$L(w) := [y_0 - f(x_0, w)]^2.$$\n",
    "\n",
    "So the goal is to find the value $w^*$ of $w$ where the loss function is *least*; in other words, we need to *minimize* the loss function!\n",
    "\n",
    "(Another name for a loss function is a *cost function*.)\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "(a) Define the loss function `L(w)` in Julia.\n",
    "\n",
    "(b) Draw the data point and the function `x -> f(x, w)`. Also draw a vertical line from the data point to the function `x -> f(x, w)`.\n",
    "\n",
    "(c) Make the plot interactive.\n",
    "\n",
    "(d) Add as the plot title the value of the loss function for the current value of $w$.\n",
    "\n",
    "(e) Use the slider to find the value $w^*$ of $w$ for which the loss function reaches its minimum value. What is $w^*$? What is the value of the loss function there, $L(w^*)$?\n",
    "\n",
    "## What does the loss function look like?\n",
    "\n",
    "The loss function $L(w)$ tells us how far away the function $f_w$ is from the data when the parameter value is $w$, represented visually as the vertical line in the previous plot. When the data are fixed, this is a function only of the parameter $w$. What does this function look like as a function of $w$? Let's draw it!\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Draw the function $L(w)$ as a function of $w$.\n",
    "\n",
    "### Features of the loss function\n",
    "\n",
    "This graph quantifies how far we are from the data point for a given value of $w$.\n",
    "What features can we see from the graph?\n",
    "\n",
    "Firstly, we see that $L(w)$ is always bigger than $0$, for any value of $w$. This is because we want $L$ to be some kind of measure of *distance*, and distances cannot be negative. \n",
    "\n",
    "Secondly, we see that there is a special value $w^*$ of $w$ where the function $L$ reaches its minimum value. In this particular case, it actually reaches all the way down to $0$!\n",
    "This means that the original function $f$ (the one we manipulated above) passes exactly through the data point $(x_0, y_0)$.\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Draw a zoomed-in version of the graph to find the place $w^*$ where the function hits $0$ more precisely.\n",
    "\n",
    "### A different way of defining the loss function\n",
    "\n",
    "**Why did we use such a complicated function $L$ with those squares inside?** We could instead just have used the absolute distance, instead of the distance squared, using the *absolute value* function, written mathematically as $| \\cdot |$, and in Julia as `abs`.\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "Define a new loss function, `L_abs`, using the absolute value, and see what it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-surge",
   "metadata": {},
   "source": [
    "## Model complexity\n",
    "\n",
    "In the last notebook, we saw that we could customize a model by adding a parameter. Doing so, we were able to fit that model to a data point. This fit was perfect, insofar as numerics would allow.\n",
    "\n",
    "In the next notebook, we'll see that as we add more data to our data set, fitting a model to our data usually becomes more challenging and the result will be less perfect.\n",
    "\n",
    "For one thing, we will find that we can add complexity to our model to capture added complexity in the data. We can do this by adding more parameters to our model. We'll see that for a data set with two data points, we can again get a \"perfect\" fit to our model by adding a second parameter to our model.\n",
    "\n",
    "However, we can't simply add a parameter to our model every time we add a data point to our data set, since this will lead to a phenomenon called **overfitting**.\n",
    "\n",
    "In the image below, we depict a data set that is close to linear, and models that exhibit underfitting, fitting well, and overfitting, from left to right:\n",
    "\n",
    "<img src=\"data/model_fitting.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "In the first image, the model accounts for the slope along which the data falls, but not the offset. \n",
    "\n",
    "In the second image, the model accounts for both the slope and offset of the  data. Adding this second parameter (the offset) to the model creates a much better fit.\n",
    "\n",
    "However, we can imagine that a model can have too many parameters, where we begin to fit  not only the high level features of the data, but also the noise. This overfitting is depicted in the third image.\n",
    "\n",
    "Our aim will be to fit the data well, but avoiding *over*fitting the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-milan",
   "metadata": {},
   "source": [
    "## Multiple function parameters\n",
    "\n",
    "In notebook 6, we saw how we could adjust a parameter to make a curve fit a single data point. What if there is more data to fit?\n",
    "\n",
    "We'll see that as we acquire more data to fit, it can sometimes be useful to add complexity to our model via additional parameters.\n",
    "\n",
    "## Adding more data\n",
    "\n",
    "Suppose there are now two data points to fit, the previous $(x_0, y_0)$ used in notebook 6 and also $(x_1, y_1) = (-3, 0.3)$.\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Make an interactive plot of the function $f_w$ together with the two data points. Can you make the graph of $f_w$ pass through *both* data points at the *same* time?\n",
    "\n",
    "You should have found that it's actually *impossible* to fit both data points at the same time! The best we could do is to *minimise* how far away the function is from the data. To do so, we need to somehow balance the distance from each of the two data points.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Play with the slider to find the value $w^*$ of $w$ that you think has the \"least error\" in an intuitive sense.\n",
    "\n",
    "## Defining a loss function\n",
    "\n",
    "How can we *quantify* some kind of overall measure of the distance from *all* of the data? Well, we just need to define a new loss function! One way to do so would be to sum up the loss functions for each data point, i.e. the sum of the squared vertical distances from the graph to each data point:\n",
    "\n",
    "$$L(w) = [y_0 - f_w(x_0)]^2 + [y_1 - f_w(x_1)]^2.$$\n",
    "\n",
    "Since the two pieces that we are adding have the same mathematical \"shape\" or structure, we can abbreviate this by writing it as a sum:\n",
    "\n",
    "$$L(w) = \\sum_{i=0}^1 [y_i - f_w(x_i)]^2.$$\n",
    "\n",
    "So now we want to find the value $w^*$ of $w$ that minimizes this new function $L$!\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Make an interactive visualization to show the function $f_w$ and to visualize the distance from each of the data points.\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "After playing with this for a while, it is intuitively obvious that we cannot make the function pass through both data points for any value of $w$. In other words, our loss function, `L(w)`, is never zero.\n",
    "\n",
    "What is the minimum value of `L(w)` that you can find by altering `w`? What is the corresponding value of `w`?\n",
    "\n",
    "### Sums in Julia\n",
    "\n",
    "To generate the above plot we used the `sum` function. `sum` can add together all the elements of a collection or range, or it can add together the outputs of applying a function to all the elements of a collection or range. \n",
    "\n",
    "Look up the docs for `sum` via\n",
    "\n",
    "```julia\n",
    "?sum\n",
    "```\n",
    "if you need more information.\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Use `sum` to add together all integers in the range 1 to 16, inclusive. What is the result?\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "What is the sum of the absolute values of all integers between -3 and 3? Use `sum` and the `abs` function.\n",
    "\n",
    "## What does the loss function $L$ look like?\n",
    "\n",
    "In our last attempt to minimize `L(w)` by varying `w`, we saw that `L(w)` always seemed to be greater than 0.  Is that true? Let's plot it to find out!\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "Plot the new loss function $L(w)$ as a function of $w$.\n",
    "\n",
    "### Features of the loss function\n",
    "\n",
    "The first thing to notice is that $L$ is always positive. Since it is the sum of squares, and squares cannot be negative, the sum cannot be negative either! \n",
    "\n",
    "However, we also see that although $L$ dips close to $0$ for a single, special value $w^* \\simeq 0.4$, it never actually *reaches* 0! Again we could zoom in on that region of the graph to estimate it more precisely.\n",
    "\n",
    "We might start suspecting that there should be a better way of using the computer to minimize $L$ to find the location $w^*$ of the minimum, rather than still doing everything by eye. Indeed there is, as we will see in the next two notebooks!\n",
    "\n",
    "## Adding more parameters to the model\n",
    "\n",
    "If we add more parameters to a function, we may be able to improve how it fits to data. For example, we could define a new function $g$ with another parameter, a shift or **bias**:\n",
    "\n",
    "$$g(x; w, b) := \\sigma(w \\, x) + b.$$\n",
    "\n",
    "*Note*: In the last notebook, we added parameters to a sigmoid function to get the form $$\\sigma(w \\, x + b)$$ and here we are working with the form $$\\sigma(w \\, x) + b$$\n",
    "instead. Both of these are valid ways to apply a bias to a function! In machine learning terminology, the first corresponds to a neural network with a single neuron, while the second can be thought of as a network with two layers.\n",
    "\n",
    "#### Exercise 8\n",
    "\n",
    "Make an interactive visualization with two sliders for $w$ and $b$. Play with the sliders to try and fit *both* data points at once!\n",
    "\n",
    "#### Exercise 9\n",
    "\n",
    "For what values of `w` and `b` does the line pass through both points?\n",
    "\n",
    "## Fitting both data points: a loss function\n",
    "\n",
    "Following the procedure that we used when we had a single parameter, we can think of the fitting procedure once again as minimizing a suitable *loss function*. The expression for the loss function is almost the same, except that now $f$ has two parameters $w$ and $b$, so the loss function $L_2$ is itself a function of $w$ *and* $b$:\n",
    "\n",
    "$$L_2(w, b) = \\sum_i [y_i - f(x_i; w, b)]^2.$$\n",
    "\n",
    "So we want to minimize this loss function over *both* of the parameters $w$ and $b$! Let's plot it.\n",
    "\n",
    "#### Exercise 10\n",
    "\n",
    "Define the function `L2` in Julia.\n",
    "\n",
    "## More data\n",
    "\n",
    "If we add more data, however, we will again not be able to fit all of the data; we will only be able to attain a \"best fit\".\n",
    "\n",
    "Let's create `xs` and `ys` with some more data:\n",
    "\n",
    "#### Exercise 11\n",
    "\n",
    "a) Make an interactive visualization of the function $f$ and the data. Try to find the values of $w$ and $b$ that give the best fit.\n",
    "\n",
    "b) Define the loss function and plot it.\n",
    "\n",
    "#### Exercise 12\n",
    "\n",
    "We've seen the loss function, $$L_{2D}(w, b) = \\sum_i[\\mathrm{ys}_i - g(\\mathrm{xs}_i, w, b)]^2,$$ written in Julia as\n",
    "\n",
    "```julia\n",
    "L2(w, b) = sum( (ys .- g.(xs, w, b)) .^ 2 )\n",
    "```\n",
    "\n",
    "a few times now. To ensure you understand what this function is doing, implement your own loss function using the commented code below. Do this without using `sum`, `abs2`, or broadcasting dot syntax (for example, `.-`). Hint: you'll want to use a `for` loop to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the graphics packages\n",
    "\n",
    "using Plots; gr()\n",
    "using Interact\n",
    "\n",
    "# Run this cell to use a couple of definitions from the previous Tools notebook\n",
    "\n",
    "σ(x) = 1 / (1 + exp(-x))\n",
    "f(x, w) = σ(w * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x, w, b) = σ(w*x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [2, -3, -1, 1]\n",
    "ys = [0.8, 0.3, 0.4, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "function myL2(w, b)\n",
    "    loss = 0.0\n",
    "    \n",
    "    return loss\n",
    "end\n",
    "\n",
    "import Pkg\n",
    "Pkg.add(\"PlotlyJS\")\n",
    "using PlotlyJS\n",
    "\n",
    "plotlyjs()\n",
    "\n",
    "ws = -2:0.05:2\n",
    "bs = -2:0.05:2\n",
    "\n",
    "surface(ws, bs, L2, alpha=0.8, zlims=(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-fabric",
   "metadata": {},
   "source": [
    "## What is learning?\n",
    "\n",
    "Computers read data, as we saw in notebooks 1 and 2. We can then build functions that model that data to make decisions, as we saw in notebooks 3 and 5. \n",
    "\n",
    "But how do you make sure that the model actually fits the data well? In the last notebook, we saw that we can fiddle with the parameters of our function defining the model to reduce the loss function. However, we don't want to have to pick the model parameters ourselves. Choosing parameters ourselves works *well enough* when we have a simple model and only a few data points, but can quickly become extremely complex for more detailed models and larger data sets. \n",
    "\n",
    "Instead, we want our machine to *learn* the parameters that fit the model to our data, without needing us to fiddle with the parameters ourselves. In this notebook, we'll talk about the \"learning\" in machine learning.\n",
    "\n",
    "### Motivation: Fitting parameters by hand\n",
    "\n",
    "Let's go back to our example of fitting parameters from notebook 3. Recall that we looked at whether the amount of green in the pictures could distinguish between an apple and a banana, and used a sigmoid function to model our choice of \"apple or banana\" using the amount of green in an image.\n",
    "\n",
    "Intuitively, how did you tweak the sliders so that way the model sends apples to 0 and bananas to 1? Most likely, you did the following:\n",
    "\n",
    "#### Move the sliders a bit, see whether the curve moves in the right direction, and if it did, keep doing it.\n",
    "\n",
    "For a machine, \"learning\" is that same process, translated into math!\n",
    "\n",
    "## \"Learning by nudging\": The process of descent\n",
    "\n",
    "Let's start to formalize this idea. In order to push the curve in the \"right direction\", we need some measurement of \"how right\" and \"how wrong\" the model is. When we translate the idea of a \"right direction\" into math, we end up with a **loss function**, `L(w, b)`, as we saw in notebook 5. We say that the loss function is lowest when the model `σ(x, w, b)` performs the best. \n",
    "\n",
    "Now we want to create a loss function that is the lowest when the apple is at `0` and the banana is at `1`. If the data (the amount of green) for our apple is $x_1$, then our model will output $σ(x_1,w, b)$ for our apple. So, we want the difference $0 - σ(x_1, w, b)$ to be small. Similarly, if our data for our banana (the banana's amount of green) is $x_2$, we want the difference $1 - σ(x_2, w, b)$ to be small.\n",
    "\n",
    "To create our loss function, let's add together the squares of the difference of the model's output from the desired output for the apple and the banana. We get\n",
    "\n",
    "$$ L(w,b) = (0 - σ(x_1, w, b))^2 + (1 - σ(x_2, w, b))^2. $$\n",
    "\n",
    "$L(w, b)$ is lowest when it outputs `0` for the apple and `1` for the banana, and thus the cost is lowest when the model \"is correct\".\n",
    "\n",
    "We can visualize this function by plotting it in 3D with the `surface` function.\n",
    "\n",
    "The blue ball on the 3D plot shows the current parameter choices, plotted as `(w,b)`. Shown below the 3D plot is a 2D plot of the corresponding model with those parameters. Notice that as the blue ball rolls down the hill, the model becomes a better fit. Our loss function gives us a mathematical notion of a \"hill\", and the process of \"learning by nudging\" is simply rolling the ball down that hill.\n",
    "\n",
    "To do this mathematically, we need to know which direction is \"downhill\". Recall from calculus that the derivative of `L` with respect to `b` tells you how `L` changes when `b` changes. Thus to roll downhill, we should go in the direction where the derivative is negative (the function goes down) for each parameter. This direction is the negative of what's called the **gradient**, $\\nabla L$. This means that the \"learn by nudging method\" can be rephrased in mathematical terms as:\n",
    "\n",
    "1. Calculate the gradient\n",
    "2. Move a little bit in the direction of the negative gradient\n",
    "3. Repeat\n",
    "\n",
    "This process of rolling the ball in the direction of the negative gradient is called **gradient descent**; written mathematically, it is\n",
    "\n",
    "$$p_{n+1} = p_n - \\eta \\nabla L(p_n).$$\n",
    "\n",
    "Here, $p_n$ represents the vector of current parameters $(w, b)$; $\\nabla L(p_n)$ is the gradient of the loss function, given those parameters. We start from $p_n$ and change it by $\\eta \\nabla L(p_n)$, where $\\eta$ is a small step size that determines how far we move the parameters in the direction of the negative gradient; notice that if you step too far, you'll overshoot the minimum!. The result is $p_{n+1}$, the new vector of parameters.\n",
    "\n",
    "[Picture of Gradient Descent Vectors]\n",
    "\n",
    "If we repeat this process, then we will end up at parameters where the model correctly labels apples as `0` and bananas as `1`. When this happens, the model has learned from the data and can then read pictures and tell you whether they are apples or bananas!\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Use the following terms to fill in the sentences below. Terms may be used more than once or not at all:\n",
    "> gradient, loss function, derivative, gradient descent, learning.\n",
    "\n",
    "* We can think of a _(A)_ as a 1D version of a _(B)_.\n",
    "* We can visualize a _(C)_ as a hill.\n",
    "* In the explanation above, rolling downhill is called _(D)_ and means traveling along the _(E)_.\n",
    "* To quantify the correctness of a model we use a _(F)_.\n",
    "* When our program can minimize a _(G)_ on its own, we say it is _(H)_.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "A)<br>\n",
    "B)<br>\n",
    "C)<br>\n",
    "D)<br>\n",
    "E)<br>\n",
    "F)<br>\n",
    "G)<br>\n",
    "H)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Plotly\")\n",
    "using Plotly\n",
    "using Plots; gr()\n",
    "using Images; using Interact\n",
    "\n",
    "σ(x,w,b) = 1 / (1 + exp(-w*x+b))\n",
    "\n",
    "apple =  load(\"data/10_100.jpg\")\n",
    "banana = load(\"data/104_100.jpg\")\n",
    "apple_green_amount =  mean(Float64.(green.(apple)))\n",
    "banana_green_amount = mean(Float64.(green.(banana)));\n",
    "\n",
    "@manipulate for w in -10:0.01:30, b in 0:0.1:30\n",
    "    \n",
    "    plot(x->σ(x,w,b), 0, 1, label=\"Model\", legend = :topleft, lw=3)\n",
    "    scatter!([apple_green_amount],  [0.0], label=\"Apple\")\n",
    "    scatter!([banana_green_amount], [1.0], label=\"Banana\")\n",
    "    \n",
    "end\n",
    "\n",
    "plotly()\n",
    "gr()\n",
    "\n",
    "L(w, b) = (0 - σ(apple_green_amount,w,b))^2 + (1 - σ(banana_green_amount,w,b))^2\n",
    "\n",
    "w_range = 10:0.1:13\n",
    "b_range = 0:1:20\n",
    "\n",
    "L_values = [L(w,b) for b in b_range, w in w_range]\n",
    "\n",
    "\n",
    "@manipulate for w in w_range, b in b_range\n",
    "    p1 = surface(w_range, b_range, L_values, xlabel=\"w\", ylabel=\"b\", cam=(70,40), cbar=false, leg=false)\n",
    "    scatter!(p1, [w], [b], [L(w,b)+1e-2], markersize=5, color = :blue)\n",
    "    p2 = plot(x->σ(x,w,b), 0, 1, label=\"Model\", legend = :topleft, lw=3)\n",
    "    scatter!(p2, [apple_green_amount],  [0.0], label=\"Apple\", markersize=10)\n",
    "    scatter!(p2, [banana_green_amount], [1.0], label=\"Banana\", markersize=10, xlim=(0,1), ylim=(0,1))\n",
    "    plot(p1, p2, layout=(2,1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-three",
   "metadata": {},
   "source": [
    "# Minimizing functions - how a computer learns\n",
    "\n",
    "In the previous notebooks, we've seen that by changing **parameters** in a function, we could find a \"best fit\" of that function to some data. We use a **loss function** to quantify the \"goodness\" of a set of parameters and we look for those parameters that **optimize**, in fact **minimize**, the loss function. If we teach a machine how to minimize the loss function on its own, we say that machine is able to **learn** how to model our data.\n",
    "\n",
    "In the last notebook, we threw around terms like **derivative**, **gradient**, and **gradient descent** to give you a rough sense of how we minimize a function on a computer. In this notebook, we will step through these concepts more carefully, with the aim of being able to implement them using Julia.\n",
    "\n",
    "## Minimizing a 1D function using calculus\n",
    "\n",
    "Let's draw the 1D loss function $L_1$ from a previous notebook again:\n",
    "\n",
    "By eye, we can see that the minimum is around $w=0.6$. But how can we get the computer to work this out on its own?\n",
    "\n",
    "In the previous notebook, we thought of this function plot as a hill, viewed from the side. We could find the minimum by making the hill sticky, and letting a ball roll down it. The ball will find and settle in the minimum of the function.  Now let's see how to teach a computer to do this.\n",
    "\n",
    "We need to find the downhill direction along the hill, which is related to its *slope* (how steep it is). Calculus provides us with tools to calculate that slope!\n",
    "\n",
    "Namely, the slope of a curve $L_1(w)$ at $w$ is given by its **derivative** $L_1'(w)$; geometrically, this is the slope of the **tangent line** to the curve at that point, i.e. the straight line which touches the curve at that point.\n",
    "\n",
    "Calculus provides us with some rules to calculate an analytical formula for the derivative, and we will see later how to apply these rules, albeit indirectly, for machine learning. \n",
    "To gain understanding, however, we will see here how to get the computer to help us out by calculating the derivatives numerically instead!\n",
    "\n",
    "## Approximating derivatives\n",
    "\n",
    "Let's recall that the derivative $L_1'(w)$ of a function is defined as\n",
    "\n",
    "$$L_1'(w) \\simeq \\frac{L_1(w+h) - L_1(w)}{h},$$\n",
    "\n",
    "for a small step size $h$. (Strictly speaking, we must take the limit when $h$ tends to $0$ to obtain the exact value of the derivative.)\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Write a function to calculate the derivative of a function at a given point. Note that in Julia, we can easily pass functions as arguments to other functions!\n",
    "The function should take the function, the point $w$ at which to calculate the derivative, and the value of $h$, which should have a default value of 0.001.\n",
    "\n",
    "*Note*: A function input argument can have a *default value* if we set the input argument equal to that default value when we define the function. For example,\n",
    "\n",
    "```julia\n",
    "f(x, a = 3) = a * x^2\n",
    "```\n",
    "\n",
    "The function `f` will square the input we give it and multiply by `a`. However,  if we choose to call `f(x)` *without* passing it an `a` input, it will assume `a` is `3` and return `3*x^2`.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Write an interactive visualization of the tangent line to the graph of $L_1$, so that we can visualize the tangent at any point on $L_1$. Include the current value of the derivative in the title.\n",
    "\n",
    "*Hint*: Recall that the straight line through the point $(x_0, y_0)$ with slope $m$ is given by\n",
    "\n",
    "$$ \\frac{y - y_0}{x - x_0} = m,$$\n",
    "\n",
    "so \n",
    "\n",
    "$$y = y_0 + m*(x - x_0).$$\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "What is the value of the derivative (slope of the tangent line) at a minimum? Can this happen anywhere else?\n",
    "\n",
    "## Minimization by gradient descent\n",
    "\n",
    "The tangent line at a point is a good approximation of the function near that point. Therefore the derivative tells us in which direction, and how fast, a function grows or shrinks when we move a small distance away from that point. \n",
    "\n",
    "As we saw in the previous notebook, we can think of the function as being a hill, and having a ball that we want to move down the hill. Gravity will automatically pull the ball in the direction *down* the hill; we want to emulate this in the computer!\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "When the derivative $L_1'(w)$ is positive, that means that $L_1$ increases from left to right at point $w$.\n",
    "\n",
    "If the derivative $L_1'(w)$ is positive (> 0), in which direction should we move $w$ to *decrease* $L_1$?\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "If the derivative $L_1'(w)$ is negative (< 0), in which direction should we move $w$ to *decrease* $L_1$?\n",
    "\n",
    "We can use this information to tell the computer which way to take a step. This constitutes the numerical algorithm called **gradient descent**; it is called this since we are descending (moving downwards) along the graph of the function by using information about its gradient (slope).\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Implement gradient descent by following this prescription for an **iterative (repetitive) algorithm**:\n",
    "\n",
    "1. Start at an initial guess $w_0$ for $w$. \n",
    "\n",
    "2. At each step, calculate the derivative, $L_1'(w_n)$ at the current value of $w_n$ using the function that you created above.\n",
    "\n",
    "3. Modify the value of $w$ by a small multiple (for example, $\\eta=0.01$) of the value of the derivative you just created, via $w_{n+1} = w_n - \\eta L_1'(w_n)$.\n",
    "\n",
    "For this problem, start with $w_0 = -2.0$. Repeat steps 2 and 3 a total of `2000` times.\n",
    "\n",
    "Package this code into a function called `gradient_descent` that takes two inputs, a function and a range for values of $w$, and returns the final value of $w$ and $L_1(w)$.\n",
    "\n",
    "Using `L1` and `-2:0.01:1.5` as your inputs to `gradient_descent`, for what value of $w$ is $L_1$ at a minimum?\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "Modify your code for gradient descent to return the result once it has found an answer within some *tolerance*, rather than taking a set number of steps. The new prescription for this algorithm is:\n",
    "\n",
    "1. Start at an initial guess $w_0$ for $w$. \n",
    "\n",
    "2. At each step, calculate the derivative, $L_1'(w_n)$ at the current value of $w_n$, using the function that you created above.\n",
    "\n",
    "3. Modify the value of $w$ by a small multiple (for example, $\\eta=0.01$) of the value of the derivative you just created, via $w_{n+1} = w_n - \\eta L_1'(w_n)$.\n",
    "\n",
    "4. Check how different $w_{n+1}$ is from $w_n$. If you're satisfied that $L_1(w_{n+1})$ is minimized, return $w_{n+1}$ and $L_1(w_{n+1})$. Otherwise, go to step (2) and continue.\n",
    "\n",
    "Edit `gradient_descent` so that it takes three inputs: a function, a range for values of $w$, and a tolerance that tells you how close $w_{n+1}$ must be to $w_n$ before you can stop iterating. \n",
    "\n",
    "Using `L1`, `-2:0.01:1.5`, and `.000001` as your inputs to `gradient_descent`, for what value of $w$ is $L_1$ at a minimum?\n",
    "\n",
    "#### Exercise 8\n",
    "\n",
    "Alter the function `gradient_descent` so that it stores the results `(w, L1(w))` at each step of the algorithm as an array and returns this array. How many steps does the algorithm take for input parameters `(L1, -2:0.01:1.5, .000001)` before terminating? You should count your starting $w_0$ as your first step.\n",
    "\n",
    "#### Exercise 9\n",
    "\n",
    "Overlay the steps taken in the last exercise with a plot of $L_1(w)$ vs. $w$.\n",
    "\n",
    "Where does our algorithm take the largest steps? (Where does the ball move fastest down the hill?)\n",
    "\n",
    "A) Between w = -2:-1<br>\n",
    "B) Between w = -1:0<br>\n",
    "C) Between w = 0:.6\n",
    "\n",
    "*Hint*: It may be easier to see what's going on if you only plot, for example, every 15th step.\n",
    "\n",
    "B) The following code generates the desired figure:\n",
    "\n",
    "```julia\n",
    "wsteps = gradient_descent(C1, -2:0.01:1.5, .000001)\n",
    "fig1 = plot(C1, -2, 1.5, xlabel=\"w\", ylabel=\"C1(w)\", leg=false)\n",
    "for step in wsteps[1:15:end]\n",
    "   scatter!([step[1]], [step[2]]) \n",
    "end\n",
    "display(fig1)\n",
    "```\n",
    "\n",
    "## Functions of 2 variables and their derivatives\n",
    "\n",
    "So far, we have been looking at the minimizing a loss function $L_1(w)$ that depends on a single parameter, $w$. Now let's turn to the cost function $L_2(w, b)$ from a previous notebook, that is a function of *two* parameters, $w$ and $b$, and try to minimize it.\n",
    "\n",
    "As we've seen, we get a **surface**, instead of a curve, when we graph $L_2$ as a function of both of its parameters.\n",
    "\n",
    "**Exercise 10** \n",
    "\n",
    "Draw a surface plot of $L_2$, given by\n",
    "\n",
    "$$L_{2}(w, b) = \\sum_i(y_i - g(x_i, w, b))^2$$\n",
    "\n",
    "using the `surface` function from `Plots.jl`. For this plot, use the values of `xs` and `ys` from notebook 5:\n",
    "\n",
    "```julia\n",
    "xs = [2, -3, -1, 1]\n",
    "ys = [0.8, 0.3, 0.4, 0.4]\n",
    "```\n",
    "\n",
    "We can get a nice interactive 3D plot by using the Plotly backend of `Plots.jl` by executing the command \n",
    "\n",
    "    plotly()\n",
    "    \n",
    "### Finding the minimum\n",
    "\n",
    "We can just about see, by rotating the graph, that $L_2$ has a single minimum. We want to find the values of $w$ and $b$ where this minimum value is reached.\n",
    "\n",
    "Following what we did for the function $L_1$ above, we expect that we will need to calculate derivatives of $L_2$. Since the function is more complicated, though, the derivatives are too!\n",
    "\n",
    "It turns out that the right concept is that of the [**gradient**](https://en.wikipedia.org/wiki/Gradient) of $L_2$, denoted $\\nabla L_2(w, b)$. This is a **vector** consisting of $2$ numbers if there are $2$ parameters [or, in general, $n$ numbers if there are $n$ parameters].\n",
    "\n",
    "The numbers that form the gradient $\\nabla L_2(w, b)$ are called the **partial derivatives** of $L_2$ with respect to $w$ and $b$,  written as \n",
    "\n",
    "$$\\frac{\\partial L_2}{\\partial w} \\quad \\text{and} \\quad \\frac{\\partial L_2}{\\partial b}.$$\n",
    "\n",
    "Although this notation might look complicated, all it means is that we calculate derivatives just like we did before, except that we fix the value of the other variable. \n",
    "For example, to calculate $\\frac{\\partial L_2}{\\partial w}$, the partial derivative of $L_2$ with respect to $w$, we fix the value of $b$ and think of the resulting function as a function of the single variable $w$; then we use the formula for derivatives of functions of a single variable.\n",
    "\n",
    "[Note that $\\frac{\\partial L_2}{\\partial w}$ is itself a function of $w$ and $b$; we could write $\\frac{\\partial L_2}{\\partial w}(w, b)$.]\n",
    "\n",
    "#### Exercise 11\n",
    "\n",
    "Write functions that will allow you to calculate the partial derivatives of a function $f$ of two variables.\n",
    "\n",
    "In particular, declare functions called `partial_w` and `partial_b`. Each should take four inputs - a function $f$ of two variables, the first input argument to $f$, the second input argument to $f$, and a step size `h` with default value `0.001`. `partial_w` should return the partial derivative of $f$ with respect to its first input argument and `partial_b` should return the partial derivative of $f$ with respect to its second input argument.\n",
    "\n",
    "#### Exercise 12\n",
    "\n",
    "Use `partial_b` from the last exercise to find the partial derivative of $L_2$ with respect to $w$ at b = 0.3, $\\frac{\\partial L_2}{\\partial w}|_{b = 0.3}$ for `w = -2:0.01:1`\n",
    "\n",
    "#### Exercise 13\n",
    "\n",
    "Plot the cross section of the surface of $L_2(w, b)$ at $b = 0.3$. Make this plot interactive with `@manipulate` to show that the function `partial_w` gives the slope of the tangent to this cross section for any point `w` in the range `-2:0.01:1`.\n",
    "\n",
    "For what value of $w$ in this range is the slope of the cross section closest to -1?\n",
    "\n",
    "## ***Optional**: Functions with $n$ inputs\n",
    "\n",
    "If a function $f$ takes $n$ input arguments, we can write them as $p_1, \\ldots, p_n$, where $p_i$ means the \"$i$th parameter\". In Julia, we can wrap them up into a single **vector**. Now we can calculate the partial derivative $\\frac{\\partial L_2}{\\partial p_i}$ with respect to the $i$th variable.\n",
    "\n",
    "#### Exercise 14\n",
    "\n",
    "For the next exercise, you will need to use the splat command, `...`. You can use this command to \"open up\" a collection and pass all the elements of that collection as inputs to a function.\n",
    "\n",
    "For example, say you have an array, `numbers`,\n",
    "\n",
    "```julia\n",
    "numbers = [4, 3, 2]\n",
    "```\n",
    "\n",
    "and you want to use `numbers` to create a $4\\times3\\times3$ randomly populated array via `rand`. `rand(numbers)` will not do what you want. You could index into `numbers` to grab the values you want and pass them to `rand`, as in\n",
    "\n",
    "```julia\n",
    "rand(numbers[1], numbers[2], numbers[3])\n",
    "```\n",
    "\n",
    "or you could use a splat:\n",
    "\n",
    "```julia\n",
    "rand(numbers...)\n",
    "```\n",
    "\n",
    "Use `...` to pass the contents of `inputs`\n",
    "\n",
    "```julia\n",
    "inputs = [30, 12, \"cats\"]\n",
    "```\n",
    "\n",
    "to the function `dreams`\n",
    "\n",
    "```julia\n",
    "dreams(i, j, perfect_mammal) = \"I wish I had $(i + j) $perfect_mammal.\"\n",
    "```\n",
    "\n",
    "#### Exercise 15:\n",
    "\n",
    "Write a function, `partial`, to calculate the $i$th partial derivative of a function. This function should have four inputs\n",
    "\n",
    "* a function, $f$, for which you want to compute the partial derivative\n",
    "* an array, *p*, specifying the values of all input arguments to $f$ at the point where you want $\\frac{\\partial f}{\\partial p_i}$ computed\n",
    "* the index, $i$, of the variable with respect to which you want to calculate the partial derivative of $f$\n",
    "* a step size with default value 0.001\n",
    "\n",
    "Hint: you will need to `copy` and modify `p` within `partial`.\n",
    "\n",
    "## Gradient descent in 2 dimensions\n",
    "\n",
    "It turns out that the gradient vector of the function $L_2(w, b)$ gives the direction in the plane $(w, b)$ in which the function $L_2$ **increases fastest**. \n",
    "\n",
    "In other words, if you start at the position $(w_0, b_0)$ and take a small step of length $\\eta$ to new values $(w', b')$, the value of the function will change to a new value $L_2(w', b')$. How do we get to the minimum of $L_2(w, b)$ as fast as possible? We want to step in the direction where $L_2$ decreases fastest!\n",
    "\n",
    "In multivariable calculus courses, it is shown that $L_2$ will *increase* fastest if you take a step **in the direction of the gradient $\\nabla L_2(w, b)$**! To decrease $L_2$ the fastest, we should take a step in the *opposite* direction, $-\\nabla L_2(w, b)$.\n",
    "\n",
    "Let's now generalize the gradient descent algorithm that we wrote previously to work our 2-dimensional function.\n",
    "\n",
    "#### Exercise 16\n",
    "\n",
    "Extend your 1D implementation of `gradient_descent` so that it will minimize the function $L_2$.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* Your new method for `gradient_descent` will take four input arguments: the function $f$ for which you seek the minimum, the range of values for $f$'s first input argument that you will consider, the range of values for $f$'s second input argument that you will consider, and a tolerance that will specify the maximum allowable step size, $\\sum_i \\eta \\frac{\\partial f}{\\partial p_i}$\n",
    "\n",
    "* Use $\\eta = .01$. For example, for a function $f(w, b)$, update $w$ such that $w_{n+1} = w_n - 0.01 * \\frac{\\partial f}{\\partial w_n}$\n",
    "\n",
    "* Seed `gradient_descent` with the starting coordinates [-2.0, -2.0], i.e. $w_0 = -2.0$ and $b_0 = -2.0$.\n",
    "\n",
    "* Return all steps (their coordinates) taken during gradient descent and the values of the loss function at these coordinates.\n",
    "\n",
    "Once you have done this, execute\n",
    "\n",
    "```julia\n",
    "gradient_descent(L2, -2:0.02:2, -2:0.02:2, .001)\n",
    "```\n",
    "\n",
    "How many steps were taken by gradient descent? \n",
    "\n",
    "Hint: Do not count your starting coordinates `[-2.0, -2.0]` as a step.\n",
    "\n",
    "#### Exercise 17\n",
    "\n",
    "Use the `surface` and `scatter!` commands to illustrate the path taken by `gradient_descent` from [-2.0, -2.0] to wherever the algorithm terminates. \n",
    "\n",
    "Where do the scattered points representing the steps of gradient descent appear the most dense?\n",
    "\n",
    "A) Near the starting point at [-2.0, -2.0]<br>\n",
    "B) Near the point [-1.8, 0] where $C_2$ appears nearly flat<br>\n",
    "C) Near the minimum of $C_2$\n",
    "\n",
    "## What we have learnt\n",
    "\n",
    "To recap, in this notebook, we have seen that the computer can calculate approximate derivatives, and that we can use those inside a relatively simple algorithm, gradient descent, to minimize functions of 1 and 2 variables!\n",
    "\n",
    "Now that we've used gradient descent to **learn** the best values of `b` and `w` for a cost function, we can imagine how we could do our classification of apples vs. bananas by allowing the machine to choose our parameters! To do this, we need to define a loss function for apple vs. banana classification, using images that have already been categorised as apples or bananas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "σ(x) = 1 / (1 + exp(-x))\n",
    "f(x, w) = σ(w * x)\n",
    "\n",
    "x1 = 2\n",
    "y1 = 0.8\n",
    "\n",
    "L1(w) = (y1 - f(x1, w))^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots; gr()\n",
    "using Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(L1, -2, 1.5, xlabel=\"w\", ylabel=\"L1(w)\", leg=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-granny",
   "metadata": {},
   "source": [
    "## Intro to neurons\n",
    "\n",
    "At this point, we know how to build models and have a computer automatically learn how to match the model to data. This is the core of how any machine learning method works. \n",
    "\n",
    "Now, let's narrow our focus and look at **neural networks**. Neural networks (or \"neural nets\", for short) are a specific choice of a **model**. It's a network made up of **neurons**; this, in turn, leads to the question, \"what is a neuron?\"\n",
    "\n",
    "### Models with multiple inputs\n",
    "\n",
    "So far, we have been using the sigmoid function as our model. One of the forms of the sigmoid function we've used is\n",
    "\n",
    "$$\\sigma_{w, b}(x) = \\frac{1}{1 + \\exp(-wx + b)},$$\n",
    "\n",
    "where `x` and `w` are both single numbers. We have been using this function to model how the amount of the color green in an image (`x`) can be used to determine if an image shows an apple or a banana. \n",
    "\n",
    "But what if we had multiple data features we wanted to fit? \n",
    "\n",
    "We could then extend our model to include multiple features like\n",
    "\n",
    "$$\\sigma_{\\mathbf{w},b}(\\mathbf{x}) = \\frac{1}{1 + \\exp(-w_1 x_1 - w_2 x_2 - \\cdots - w_n x_n + b)}$$\n",
    "\n",
    "Note that now $\\mathbf{x}$ and $\\mathbf{w}$ are vectors with many components, rather than a single number. \n",
    "\n",
    "For example, $x_1$ might be the amount of the color green in an image, $x_2$ could be the height of the object in the picture, $x_3$ could be the width, and so forth. We can add information for as many features as we have! Our model now has more parameters, but the same idea of gradient descent (\"rolling the ball downhill\") will still work to train our model.\n",
    "\n",
    "This version of the sigmoid model that takes multiple inputs is an example of a **neuron**.\n",
    "\n",
    "In the video, we see that one huge class of learning techniques is based around neurons, that is, *artificial neurons*. These are caricatures of real, biological neurons. Both *artificial* and *biological* neurons have several inputs $x_1, \\ldots, x_n$, and a single output, $y$. Schematically they look like this:\n",
    "\n",
    "We should read this as showing how information flows from left to right: \n",
    "- 4 pieces of input information arrive (shown in green on the left);\n",
    "\n",
    "- the neuron (shown in red on the right) receives all the inputs, processes them, and outputs a single number to the right.\n",
    "\n",
    "In other words, a neuron is just a type of function that takes multiple inputs and returns a single output.\n",
    "\n",
    "The simplest interesting case that we will look at in this notebook is when there are just two pieces of input data:\n",
    "\n",
    "Each link between circles above represents a **weight** $w$ that can be modified to allow the neuron to learn, so in this case the neuron has two weights, $w_1$ and $w_2$. \n",
    "\n",
    "The neuron also has a single bias $b$, and an **activation function**, which we will take to be the $\\sigma$ sigmoidal function that we have been using. (Note that other activation functions can be used!)\n",
    "\n",
    "Let's call our neuron $f_{w_1,w_2, b}(x_1, x_2)$, where\n",
    "\n",
    "$$f_{w_1,w_2, b}(x_1, x_2) := \\sigma(w_1 x_1 + w_2 x_2 + b).$$\n",
    "\n",
    "Note that $f_{w_1,w_2, b}(x_1, x_2)$ has 3 parameters: two weights and a bias.\n",
    "\n",
    "To simplify the notation, and to prepare for more complicated scenarios later, we put the two weights into a vector, and the two data values into another vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix};\n",
    "\\qquad\n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We thus have\n",
    "\n",
    "$$f_{\\mathbf{w}, b}(\\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b),$$\n",
    "\n",
    "where the dot product $\\mathbf{w} \\cdot \\mathbf{x}$ is an abbreviated syntax for $w_1 x_1 + w_2 x_2$.\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Declare the function `f(x, w, b)` in Julia. `f` should take vectors `x` and `w` as vectors and `b` as a scalar. Furthermore `f` should call\n",
    "\n",
    "```julia\n",
    "σ(x) = 1 / (1 + exp(-x))\n",
    "```\n",
    "\n",
    "What output do you get for\n",
    "\n",
    "```julia\n",
    "f(3, 4, 5)\n",
    "```\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"draw_neural_net.jl\")\n",
    "\n",
    "number_inputs, number_neurons = 4, 1\n",
    "\n",
    "draw_network([number_inputs, number_neurons])\n",
    "\n",
    "draw_network([2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-collect",
   "metadata": {},
   "source": [
    "## Learning with a single neuron\n",
    "\n",
    "In this notebook, we'll build a neuron that classifies an image as an apple or as a banana using multiple features from the image. We'll **train** our neuron using data from many images that have already been correctly categorised; our neuron will thereby **learn** what parameters to use, by minimizing a loss function using gradient descent.\n",
    "\n",
    "We'll do this with almost the simplest possible neuron, namely one that takes just two inputs:\n",
    "\n",
    "To do this, we need to work with and *clean* some real data. Let's get started!\n",
    "\n",
    "## Loading in some data\n",
    "\n",
    "Let's load in some real data! We'll use data that we have prepared from photos of apples and bananas; it turns out to be stored on disk in data files as \"tab-separated values\". We can read this data in with the `CSV.jl` package, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"draw_neural_net.jl\")\n",
    "number_inputs, number_neurons = 2, 1\n",
    "draw_network([number_inputs, number_neurons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    ";head data/Apple_Golden_1.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    ";head data/bananas.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Interact\")\n",
    "Pkg.add(\"PyPlot\")\n",
    "Pkg.add(\"TextParse\")\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Interact\n",
    "using PyPlot\n",
    "using TextParse\n",
    "\n",
    "applecols, applecolnames = TextParse.csvread(\"data/Apple_Golden_1.dat\", '\\t')\n",
    "bananacols, bananacolnames = TextParse.csvread(\"data/bananas.dat\", '\\t');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "apples =  DataFrame(Dict(strip(name) => col for (name, col) in zip(applecolnames, applecols)))\n",
    "bananas = DataFrame(Dict(strip(name) => col for (name, col) in zip(bananacolnames, bananacols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-printer",
   "metadata": {},
   "source": [
    "Next, we want to use `DataFrames` to store the information from our CSV files.\n",
    "\n",
    "One way we can create a `DataFrame` is to pass a dictionary that contain arrays as values and descriptive names for those arrays as keys to the `DataFrame` function:\n",
    "\n",
    "Above, we used a \"dictionary comprehension\" to create each `DataFrame`. Here is some code to create a dictionary, `appledict`, via a dictionary comprehension:\n",
    "\n",
    "```julia\n",
    "appledict = Dict(strip(name)=>col for (name, col) in zip(applecolnames, applecols))\n",
    "```\n",
    "For now, don't worry about the exact syntax used here. Just note that the dictionary created associates names with arrays, and that we've used a dictionary like this to create a `DataFrame` with named columns!\n",
    "\n",
    "So for now, each of the two data sets is stored in a `DataFrame` (from the `DataFrames.jl` package).\n",
    "\n",
    "### Roadmap\n",
    "\n",
    "To use a neuron with two inputs, we will use just two of the data (numbers) for each image, say columns 3 and 4, the mean amount of red and the mean amount of green; each data point will then be a 2-dimensional vector, and the data points lie on a two-dimensional plane. We will have many input data, labelled with an index $i$. We will denote the $i$th data point as  $\\mathbf{x}^{(i)}$.\n",
    "\n",
    "The goal is that our neuron will take a single point on the two-dimensional plane as its input, and should return a single output that **classifies** it as an apple ($0$) or a banana ($1$). \n",
    "\n",
    "To do so, it must \"**learn**\" the correct values of its parameters $\\mathbf{w}$ and $b$. For this learning to take place, we'll need **labels** for each of the input data, which identify them as either an apple (0) or as a banana (1). \n",
    "\n",
    "These labels will, in turn, allow us to create a loss function, which will allow our algorithm to learn to determine if a given choice of parameters does a good or a poor job of classifying our fruit images. \n",
    "\n",
    "**So what do we have left to do?**\n",
    "\n",
    "The above might sound complex, but luckily we can break it down into a series of actionable steps:\n",
    "\n",
    "1. Clean our input data (amounts of red and green) to get it into a usable format;\n",
    "2. Create a sequence of labels that we can use to identify correct and incorrect classifications;\n",
    "3. Define a loss function that contains parameters;\n",
    "4. Implement an algorithm to pick parameters for our neuron by minimizing the loss function with respect to the parameters;\n",
    "5. Use all of the above to train our neuron how to classify images as apples or bananas!\n",
    "\n",
    "#### Note: \n",
    "\n",
    "Note that *in general we cannot expect that a single neuron will be adequate for classification.* \n",
    "\n",
    "If a single neuron struggles to classify our images, we may need to use a more complicated neural network structure (which corresponds to using a more complicated function).\n",
    "\n",
    "## Cleaning the data\n",
    "\n",
    "Usually it will be necessary to \"clean\" the data in some way, i.e. pre-process it, before it can be used for whichever task you are interested in.\n",
    "\n",
    "Our next *meta*-exercise will be to collect all the data from columns 3 and 4 into a *single* Julia vector `x` (of which each entry is itself a vector), and the labels into a single vector `y`. Let's do this in a series of steps!\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "First, let's practice grabbing a single column of a `DataFrame`. To grab a column, you can index into the `DataFrame` with the name of the column you want, passed as a symbol. In Julia, symbols are names preceded by a `:`. For example, we could grab the \"height\" column from `apples` by indexing into `apples` with the symbol, `:height`:\n",
    "\n",
    "```julia\n",
    "apples[:height]\n",
    "```\n",
    "\n",
    "Index into `apples` to get the \"red\" column. What is the type of the object returned? How many entries does it have?\n",
    "\n",
    "A) Array, 5 <br>\n",
    "B) DataArray, 5 <br>\n",
    "C) Array, 64 <br>\n",
    "D) DataArray, 64 <br>\n",
    "E) Array, 492 <br>\n",
    "F) DataArray, 492\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "We can grab an individual entry of a `DataFrame` by specifying the row index of the entry and the column symbol. For example, to access the height of the 4th image of an apple, we would execute\n",
    "\n",
    "```julia\n",
    "apples[4, :height]\n",
    "```\n",
    "\n",
    "How much red is there in the 63rd image of a banana?\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "We want to reorganize data from the 3rd and 4th columns of `apples` and `bananas` to put that data in a single array. Let's start by organizing the data from the 3rd and 4th columns of `apples` into a single array, `x_apples`. Create `x_apples` such that there is a single element in `x_apples` for each image in `apples`. The $i^\\mathrm{th}$ element in `x_apples` should be a `Vector`, i.e. a 1D `Array`, with two elements - the amount of red and the amount of blue in the $i^\\mathrm{th}$ image from `apples`.\n",
    "\n",
    "Similarly create the `Array` `x_bananas` using data from `bananas`.\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "Next we want to combine the elements of `x_apples` and `x_bananas` into a single array, `xs`. `xs` should contain, first, all the elements of `x_apples`, and then all the elements of `x_bananas`. Use the `vcat` function to create `xs`.\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "If you've gotten this far, our data is in the format we want for learning. Now we need labels! We want to store a label (either a `0` or a `1` for every apple or banana image in our data set in an array called `ys`. Recall that \"0\" refers to an apple and \"1\" refers to a banana.\n",
    "\n",
    "Create an array `ys` where the $i^\\mathrm{th}$ element of `ys` is a `0` if the $i^\\mathrm{th}$ element of `xs` is an apple, and where the $i^\\mathrm{th}$ element of `ys` is a `1` if the $i^\\mathrm{th}$ element of `xs` is a banana.\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Add data points for all apple and banana images in our data sets to a plot using `scatter`. Plot data points for apples in one color and use a different color for banana data points.\n",
    "\n",
    "Hint: You may want to use the `first` and `last` functions.\n",
    "\n",
    "## \"Learning\" by hand\n",
    "\n",
    "Intuitively, looking at the plot of the data, we see that it should be \"easy\" to find a function that separates the data into bananas on one side and apples on the other: we just need to draw a straight line that divides the two clouds of data. We can do this \"by hand\", as follows.\n",
    "\n",
    "In the code below, the neuron will learn a function of the form $\\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)$. Since $\\sigma$ looks like a smooth version of a step function, we can think of $\\sigma$ classifying based on whether the value of its output argument is less than `0.5` or greater than `0.5`. \n",
    "\n",
    "**Game**: Use the interactive visualization to find suitable values of $\\mathbf{w}$ and $b$ such that the hyperplane $\\sigma(w_1 x_1 + w_2 x_2 + b) = 0.5$ divides the data. This is the same as the hyperplane for which $w_1 x_1 + w_2 x_2 + b = 0$ ! (Note that there are many such values!)\n",
    "\n",
    "We can solve for $x_2$ via \n",
    "\n",
    "$$x_2 = -(w_1 x_1 + b) / w_2,$$\n",
    "\n",
    "and use this to draw the corresponding hyperplane.\n",
    "\n",
    "## How can the neuron *learn* to classify the data?\n",
    "\n",
    "We are now ready for our first experience of **machine learning**: we will let the neuron learn automatically by processing data and tuning model parameters accordingly (the process we call \"learning\")!\n",
    "\n",
    "For given values of the parameters $w_1$, $w_2$ and $b$, the function $f_{\\mathbf{w}, b}$ maps a vector of length $2$ to a number between $0$ and $1$ (due to the definition of $\\sigma$). Now we want to have a neuron *learn* suitable values of these parameters. \n",
    "\n",
    "We want to discover (learn!) the parameters such that $f$ models the relationship between the data we explored above about the fruit images, and outputs a classification of the fruit: $0$ if it corresponds to an apple, and $1$ if it is a banana.\n",
    "\n",
    "So the neuron's input will be a vector of 2 pieces of information about an image; let's call the data about the $i$th image $\\mathbf{x}^{(i)}$.\n",
    "We also are given the label that says which type of fruit it is: $0$ for an apple, and $1$ for a banana; let's call this *desired* output number $y^{(i)}$. When we feed in the $i$th data, $\\mathbf{x}^{(i)}$, we want the neuron to give an output that is *as close as possible* to the desired output $y^{(i)}$; i.e. it should **minimize** the mean-squared distance\n",
    "\n",
    "$$L_i = [f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)} ]^2.$$\n",
    "\n",
    "However, now we see a key difference from what we did previously: the neuron should vary its parameters in such a way that it manages to minimize this distance for *all* of the input data, simultaneously!\n",
    "\n",
    "How can we express this mathematically? We once again define a loss function, $L(\\mathbf{w}, b)$, which tells us \"how wrong\" we are when the parameters take on the given values, and then **minimize** this loss function with respect to all of its parameters.\n",
    "\n",
    "One way to take account of all the data at once is to use the \"mean-squared error\" loss function, which is the mean (squared) over all the differences between the output of the network, $f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)})$ on the $i$th data, and the desired output $y^{(i)}$:\n",
    "\n",
    "$$L_\\mathrm{total}(\\mathbf{w}, b) = \\frac{1}{N} \\sum_i L_i = \\frac{1}{N} \\sum_i [f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)} ]^2,$$\n",
    "\n",
    "where $N$ is the total number of data in the training set.\n",
    "\n",
    "Why do we choose this particular loss function? Because the minimum possible value of this loss function is $0$ (since it is a sum of squares), and this is reached only when the neural network perfectly predicts the output. If we can find a way to minimize this loss function, we will get as close as possible to this perfect prediction. (In general, though, we won't be able to get an exact prediction.)\n",
    "\n",
    "## Minimizing the loss function: *stochastic* gradient descent\n",
    "\n",
    "We already know how to minimise loss functions on a computer: we just calculate the gradient, and do gradient descent! But here we hit a problem: the function $L_\\mathrm{total}$ usually has a *lot* of terms, and so calculating the gradient of that function will be very time-consuming.\n",
    "\n",
    "Instead, we will use a variant of gradient descent, called *stochastic* gradient descent. Here, the idea is that we will not use the complete loss function; instead, at each step we will choose a random data point, number $i$, and do a step of gradient descent for the partial loss function $L_i$ *corresponding to only that data point*.\n",
    "\n",
    "**Exercise 7:** \n",
    "\n",
    "Write functions for the partial loss function `L(w, b, x, y)`.\n",
    "\n",
    "To do this, recall\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix};\n",
    "\\qquad\n",
    "\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix};\n",
    "\\qquad\n",
    "f_{\\mathbf{w}, b}(\\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b),$$\n",
    "\n",
    "and declare `f(x, w, b)` as in notebook 8.\n",
    "\n",
    "#### Exercise 8\n",
    "\n",
    "Write a function for the gradient of `L`, i.e. `∇L(w, b, x, y)`, with respect to the parameters $(w_1, w_2, b)$, using finite differences. $∇L$ will be a vector with one component per parameter:\n",
    "\n",
    "$$∇L = \\left( \\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, \\frac{\\partial L}{\\partial b} \\right).$$\n",
    "\n",
    "#### Exercise 9\n",
    "\n",
    "Implement stochastic gradient descent in the function `stochastic_gradient_descent(L, w, b, xs, ys, N = 1000)`. Use this function to minimize the function $L_\\mathrm{total}$.\n",
    "\n",
    "The algorithm: For each of `N` steps, randomly select an index $i$ into the vector `xs` storing your image data. Calculate the gradient of the cost function, $L_i$, for this image and update each of the parameters, $p_j$, of $L_i$ according to\n",
    "\n",
    "$$p_j = p_j - 0.01 * ∇L_j$$\n",
    "\n",
    "(Here, $j$ signifies the $j^{th}$ parameter of $L$ and similarly the $j^{th}$ component of $∇L$.)\n",
    "\n",
    "`stochastic_gradient_descent` should return the updated values for vector $\\mathbf{w}$ and scalar $b$.\n",
    "\n",
    "Optional: Keep track of the value of $L_\\mathrm{total}$ over time if you want to visualize the learning process.\n",
    "\n",
    "#### Exercise 10\n",
    "\n",
    "Use the values of `w` and `b` from the last exercise to see how `f` is classifying a couple of the images in out dataset.\n",
    "\n",
    "In particular, calculate `f` using the 1st and the 900th image in `xs`. For which image is the output of `f` closer to the value of its label?\n",
    "\n",
    "A) The output of `f` for the 1st image in `xs` is closer to its label\n",
    "\n",
    "B) The output of `f` for the 900th image in `xs` is closer to its label.\n",
    "\n",
    "#### Exercise 11\n",
    "\n",
    "Use the `maximum` function to determine the maximum squared distance of the prediction from the true value. (For each image, this formula is $y_i - f_{w, b}(x_i)$.)\n",
    "\n",
    "#### Exercise 12\n",
    "\n",
    "Use `w` and `b` from stochastic gradient descent to draw the function that the network has learned, as before, as the hyperplane $w_1 x + w_2 y + b = 0$. Overlay this with the data.\n",
    "\n",
    "\n",
    "Does this hyperplane correctly separate the data? (I.e., is the data for all apples on one side of the line, and is the data for all bananas on the other side of the line?)\n",
    "\n",
    "A) Yes\n",
    "B) No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "@manipulate for w1 in -2:0.01:3, w2 in -2:0.01:3, b in -2:0.01:3\n",
    "    \n",
    "    PyPlot.scatter(first.(x_apples), last.(x_apples), m=:cross, label=\"apples\")\n",
    "    PyPlot.scatter!(first.(x_bananas), last.(x_bananas), label=\"bananas\")\n",
    "    \n",
    "    ylims!(0.3, 0.66)\n",
    "    xlims!(0.45, 0.75)\n",
    "    \n",
    "    plot!(x -> -(w1*x + b) / w2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-condition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0-beta1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
