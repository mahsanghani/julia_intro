{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "female-offense",
   "metadata": {},
   "source": [
    "## Data\n",
    "Being able to easily load and process data is a crucial task that can make any data science more pleasant. In this notebook, we will cover most common types often encountered in data science tasks, and we will be using this data throughout the rest of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"DelimitedFiles\")\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"XLSX\")\n",
    "using BenchmarkTools\n",
    "using DataFrames\n",
    "using DelimitedFiles\n",
    "using CSV\n",
    "using XLSX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-glance",
   "metadata": {},
   "source": [
    "# üóÉÔ∏è Get some data\n",
    "In Julia, it's pretty easy to dowload a file from the web using the `download` function. But also, you can use your favorite command line commad to download files by easily switching from Julia via the `;` key. Let's try both.\n",
    "\n",
    "Note: `download` depends on external tools such as curl, wget or fetch. So you must have one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "?download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = download(\"https://raw.githubusercontent.com/nassarhuda/easy_data/master/programming_languages.csv\",\n",
    "    \"programminglanguages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-password",
   "metadata": {},
   "outputs": [],
   "source": [
    ";wget \"https://github.com/nassarhuda/easy_data/blob/master/programminglanguages.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-maintenance",
   "metadata": {},
   "source": [
    "# üìÇ Read your data from text files.\n",
    "The key question here is to load data from files such as `csv` files, `xlsx` files, or just raw text files. We will go over some Julia packages that will allow us to read such files very easily.\n",
    "\n",
    "Let's start with the package `DelimitedFiles` which is in the standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    ";head programminglanguages.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "readdlm(source, \n",
    "    delim::AbstractChar, \n",
    "    T::Type, \n",
    "    eol::AbstractChar; \n",
    "    header=false, \n",
    "    skipstart=0, \n",
    "    skipblanks=true, \n",
    "    use_mmap, \n",
    "    quotes=true, \n",
    "    dims, \n",
    "    comments=false, \n",
    "    comment_char='#')\n",
    "=#\n",
    "P,H = readdlm(\"programming_languages.csv\",',';header=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write to a text file, you can:\n",
    "writedlm(\"programminglanguages_dlm.txt\", P, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "C = CSV.read(\"programming_languages.csv\");|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show typeof(C)\n",
    "C[1:10,:]\n",
    "# C.year #[!,:year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show typeof(P)\n",
    "P[1:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "names(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "names(C)\n",
    "C.year\n",
    "C.language\n",
    "describe(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime P,H = readdlm(\"programming_languages.csv\",',';header=true);\n",
    "@btime C = CSV.read(\"programming_languages.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write to a *.csv file using the CSV package\n",
    "CSV.write(\"programminglanguages_CSV.csv\",DataFrame(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = XLSX.readdata(\"data/zillow_data_download_april2020.xlsx\", #file name\n",
    "    \"Sale_counts_city\", #sheet name\n",
    "    \"A1:F9\" #cell range\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = XLSX.readtable(\"data/zillow_data_download_april2020.xlsx\",\"Sale_counts_city\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "G[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "G[1][1][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "G[2][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = DataFrame(G...) # equivalent to DataFrame(G[1],G[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = [\"apple\", \"cucumber\", \"tomato\", \"banana\"]\n",
    "calories = [105,47,22,105]\n",
    "prices = [0.85,1.6,0.8,0.6,]\n",
    "dataframe_calories = DataFrame(item=foods,calories=calories)\n",
    "dataframe_prices = DataFrame(item=foods,price=prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = innerjoin(dataframe_calories,dataframe_prices,on=:item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use the DataFrame constructor on a Matrix\n",
    "DataFrame(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already have a dataframe: \n",
    "# XLSX.writetable(\"filename.xlsx\", collect(DataFrames.eachcol(df)), DataFrames.names(df))\n",
    "XLSX.writetable(\"writefile_using_XLSX.xlsx\",G[1],G[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-victorian",
   "metadata": {},
   "source": [
    "## ‚¨áÔ∏è Importing your data\n",
    "\n",
    "Often, the data you want to import is not stored in plain text, and you might want to import different kinds of types. Here we will go over importing `jld`, `npz`, `rda`, and `mat` files. Hopefully, these four will capture the types from four common programming languages used in Data Science (Julia, Python, R, Matlab).\n",
    "\n",
    "We will use a toy example here of a very small matrix. But the same syntax will hold for bigger files.\n",
    "\n",
    "```\n",
    "4√ó5 Array{Int64,2}:\n",
    " 2  1446  1705  1795  1890\n",
    " 3  2926  3121  3220  3405\n",
    " 4  2910  3022  2937  3224\n",
    " 5  1479  1529  1582  1761\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD\n",
    "jld_data = JLD.load(\"data/mytempdata.jld\")\n",
    "save(\"mywrite.jld\", \"A\", jld_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "using NPZ\n",
    "npz_data = npzread(\"data/mytempdata.npz\")\n",
    "npzwrite(\"mywrite.npz\", npz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "using RData\n",
    "R_data = RData.load(\"data/mytempdata.rda\")\n",
    "# We'll need RCall to save here. https://github.com/JuliaData/RData.jl/issues/56\n",
    "using RCall\n",
    "@rput R_data\n",
    "R\"save(R_data, file=\\\"mywrite.rda\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MAT\n",
    "Matlab_data = matread(\"data/mytempdata.mat\")\n",
    "matwrite(\"mywrite.mat\",Matlab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show typeof(jld_data)\n",
    "@show typeof(npz_data)\n",
    "@show typeof(R_data)\n",
    "@show typeof(Matlab_data)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matlab_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-accessory",
   "metadata": {},
   "source": [
    "# üî¢ Time to process the data from Julia\n",
    "We will mainly cover `Matrix` (or `Vector`), `DataFrame`s, and `dict`s (or dictionaries). Let's bring back our programming languages dataset and start playing it the matrix it's stored in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-seeking",
   "metadata": {},
   "source": [
    "Here are some quick questions we might want to ask about this simple data.\n",
    "- Which year was was a given language invented?\n",
    "- How many languages were created in a given year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Which year was was a given language invented?\n",
    "function year_created(P,language::String)\n",
    "    loc = findfirst(P[:,2] .== language)\n",
    "    return P[loc,1]\n",
    "end\n",
    "year_created(P,\"Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_created(P,\"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "function year_created_handle_error(P,language::String)\n",
    "    loc = findfirst(P[:,2] .== language)\n",
    "    !isnothing(loc) && return P[loc,1]\n",
    "    error(\"Error: Language not found.\")\n",
    "end\n",
    "year_created_handle_error(P,\"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: How many languages were created in a given year?\n",
    "function how_many_per_year(P,year::Int64)\n",
    "    year_count = length(findall(P[:,1].==year))\n",
    "    return year_count\n",
    "end\n",
    "how_many_per_year(P,2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_df = C #DataFrame(year = P[:,1], language = P[:,2]) # or DataFrame(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even better, since we know the types of each column, we can create the DataFrame as follows:\n",
    "# P_df = DataFrame(year = Int.(P[:,1]), language = string.(P[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Which year was was a given language invented?\n",
    "# it's a little more intuitive and you don't need to remember the column ids\n",
    "function year_created(P_df,language::String)\n",
    "    loc = findfirst(P_df.language .== language)\n",
    "    return P_df.year[loc]\n",
    "end\n",
    "year_created(P_df,\"Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_created(P_df,\"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "function year_created_handle_error(P_df,language::String)\n",
    "    loc = findfirst(P_df.language .== language)\n",
    "    !isnothing(loc) && return P_df.year[loc]\n",
    "    error(\"Error: Language not found.\")\n",
    "end\n",
    "year_created_handle_error(P_df,\"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: How many languages were created in a given year?\n",
    "function how_many_per_year(P_df,year::Int64)\n",
    "    year_count = length(findall(P_df.year.==year))\n",
    "    return year_count\n",
    "end\n",
    "how_many_per_year(P_df,2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick example to show how to build a dictionary\n",
    "Dict([(\"A\", 1), (\"B\", 2),(1,[1,2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_dictionary = Dict{Integer,Vector{String}}()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_dictionary[67] = [\"julia\",\"programming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not gonna work.\n",
    "P_dictionary[\"julia\"] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-kidney",
   "metadata": {},
   "source": [
    "Now, let's populate the dictionary with years as keys and vectors that hold all the programming languages created in each year as their values. Even though this looks like more work, we often need to do it just once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = Dict{Integer,Vector{String}}()\n",
    "for i = 1:size(P,1)\n",
    "    year,lang = P[i,:]\n",
    "    if year in keys(dict)\n",
    "        dict[year] = push!(dict[year],lang) \n",
    "        # note that push! is not our favorite thing to do in Julia, \n",
    "        # but we're focusing on correctness rather than speed here\n",
    "    else\n",
    "        dict[year] = [lang]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though a smarter way to do this is:\n",
    "curyear = P_df.year[1]\n",
    "P_dictionary[curyear] = [P_df.language[1]]\n",
    "for (i,nextyear) in enumerate(P_df.year[2:end])\n",
    "    if nextyear == curyear\n",
    "        #same key\n",
    "        P_dictionary[curyear] = push!(P_dictionary[curyear],P_df.language[i+1])\n",
    "        # note that push! is not our favorite thing to do in Julia, \n",
    "        # but we're focusing on correctness rather than speed here\n",
    "    else\n",
    "        curyear = nextyear\n",
    "        P_dictionary[curyear] = [P_df.language[i+1]]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "length(keys(P_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "length(unique(P[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Which year was was a given language invented?\n",
    "# now instead of looking in one long vector, we will look in many small vectors\n",
    "function year_created(P_dictionary,language::String)\n",
    "    keys_vec = collect(keys(P_dictionary))\n",
    "    lookup = map(keyid -> findfirst(P_dictionary[keyid].==language),keys_vec)\n",
    "    # now the lookup vector has `nothing` or a numeric value. We want to find the index of the numeric value.\n",
    "    return keys_vec[findfirst((!isnothing).(lookup))]\n",
    "end\n",
    "year_created(P_dictionary,\"Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: How many languages were created in a given year?\n",
    "how_many_per_year(P_dictionary,year::Int64) = length(P_dictionary[year])\n",
    "how_many_per_year(P_dictionary,2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-lease",
   "metadata": {},
   "source": [
    "# üìù A note about missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume there were missing values in our dataframe\n",
    "P[1,1] = missing\n",
    "P_df = DataFrame(year = P[:,1], language = P[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropmissing(P_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-catalog",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] dowload a data file from the web given a url\n",
    "- [ ] load data from a file from a text file via DelimitedFiles or CSV\n",
    "- [ ] write your data to a text file or csv file\n",
    "- [ ] load data from file types xlsx, jld, npz, mat, rda\n",
    "- [ ] write your data to an xlsx file, jld, npz, mat, rda\n",
    "- [ ] store data in a 2D array (`Matrix`), or `DataFrame` or `Dict`\n",
    "- [ ] write functions to perform basic lookups on `Matrix`, `DataFrame`, and `Dict` types\n",
    "- [ ] use some of the basic functions on `DataFrame`s such as: `dropmissing`, `describe`, `by`, and `join`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-intake",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "Julia was created in 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-initial",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "A lot of the Data Science methods we will see in this tutorial require some understanding of linear algebra, and in this notebook we will focus on how Julia handles matrices, the types that exist, and how to call basic linear algebra tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some packages we will use\n",
    "import Pkg;\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"SparseArrays\")\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"MAT\")\n",
    "using LinearAlgebra\n",
    "using SparseArrays\n",
    "using Images\n",
    "using MAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-academy",
   "metadata": {},
   "source": [
    "![title](data/matrix_storage.png)\n",
    "### üü¢Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(10,10); # created a random matrix of size 10-by-10\n",
    "Atranspose = A' # matrix transpose\n",
    "A = A*Atranspose; # matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show A[11] == A[1,2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = rand(10); #created a random vector of size 10\n",
    "x = A\\b; #x is the solutions to the linear system Ax=b\n",
    "@show norm(A*x-b)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-farming",
   "metadata": {},
   "source": [
    "A few things that are noteworthy: \n",
    "- `A` is a `Matrix` type, and `b` is a `Vector` type.\n",
    "- The transpose function creates a matrix of type `Adjoint`.\n",
    "- `\\` is always the recommended way to solve a linear system. You almost never want to call the `inv` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show typeof(A)\n",
    "@show typeof(b)\n",
    "@show typeof(rand(1,10))\n",
    "@show typeof(Atranspose)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix{Float64} == Array{Float64,2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector{Float64} == Array{Float64,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "Atranspose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-salon",
   "metadata": {},
   "source": [
    "`adjoint` in julia is a lazy adjoint -- often, we can easily perform Linear Algebra operations such as `A*A'` without actually transposing the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "?adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "Atranspose.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To actually copy the matrix\n",
    "B = copy(Atranspose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "?\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-trail",
   "metadata": {},
   "source": [
    "### üü¢Factorizations\n",
    "A common tool used in Linear Algebra is matrix factorizations. These factorizations are often used to solve linear systems like `Ax=b`, and as we will see later in this tutorial... `Ax=b` comes up in a lot of Data Science problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-checkout",
   "metadata": {},
   "source": [
    "#### LU factorization\n",
    "L\\*U = P\\*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "luA = lu(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(luA.L*luA.U - luA.P*A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-rocket",
   "metadata": {},
   "source": [
    "#### QR factorization\n",
    "Q\\*R = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrA = qr(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(qrA.Q*qrA.R - A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-banking",
   "metadata": {},
   "source": [
    "#### Cholesky factorization, note that A needs to be symmetric positive definite\n",
    "L\\*L' = A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "isposdef(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "cholA = cholesky(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(cholA.L*cholA.U - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "cholA.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "cholA.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorize(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "?factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "?diagm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert(Diagonal{Int64,Array{Int64,1}},diagm([1,2,3]))\n",
    "Diagonal([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "I(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-petite",
   "metadata": {},
   "source": [
    "### üü¢Sparse Linear Algebra\n",
    "Sparse matrices are stored in Compressed Sparse Column (CSC) form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays\n",
    "S = sprand(5,5,2/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "S.rowval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "S.colptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "S.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-alias",
   "metadata": {},
   "source": [
    "### üü¢Images as matrices\n",
    "Let's get to the more \"data science-y\" side. We will do so by working with images (which can be viewed as matrices), and we will use the `SVD` decomposition.\n",
    "\n",
    "First let's load an image. I chose this image as it has a lot of details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = load(\"data/khiam-small.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show typeof(X1)\n",
    "X1[1,1] # this is pixel [1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgray = Gray.(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = map(i->X1[i].r,1:length(X1))\n",
    "R = Float64.(reshape(R,size(X1)...))\n",
    "\n",
    "G = map(i->X1[i].g,1:length(X1))\n",
    "G = Float64.(reshape(G,size(X1)...))\n",
    "\n",
    "B = map(i->X1[i].b,1:length(X1))\n",
    "B = Float64.(reshape(B,size(X1)...))\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = zeros(size(R)...) \n",
    "# just a matrix of all zeros of equal size as the image\n",
    "RGB.(Z,G,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgrayvalues = Float64.(Xgray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_V = svd(Xgrayvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(SVD_V.U*diagm(SVD_V.S)*SVD_V.V' - Xgrayvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the top 4 singular vectors/values to form a new image\n",
    "u1 = SVD_V.U[:,1]\n",
    "v1 = SVD_V.V[:,1]\n",
    "img1 = SVD_V.S[1]*u1*v1'\n",
    "\n",
    "i = 2\n",
    "u1 = SVD_V.U[:,i]\n",
    "v1 = SVD_V.V[:,i]\n",
    "img1 += SVD_V.S[i]*u1*v1'\n",
    "\n",
    "i = 3\n",
    "u1 = SVD_V.U[:,i]\n",
    "v1 = SVD_V.V[:,i]\n",
    "img1 += SVD_V.S[i]*u1*v1'\n",
    "\n",
    "i = 4\n",
    "u1 = SVD_V.U[:,i]\n",
    "v1 = SVD_V.V[:,i]\n",
    "img1 += SVD_V.S[i]*u1*v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gray.(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1:100\n",
    "u1 = SVD_V.U[:,i]\n",
    "v1 = SVD_V.V[:,i]\n",
    "img1 = u1*spdiagm(0=>SVD_V.S[i])*v1'\n",
    "Gray.(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(Xgrayvalues-img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = matread(\"data/face_recog_qr.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = reshape(M[\"V2\"][:,1],192,168)\n",
    "Gray.(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = q[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = M[\"V2\"][:,2:end]\n",
    "x = A\\b #Ax=b\n",
    "Gray.(reshape(A*x,192,168))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(A*x-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "qv = q+rand(size(q,1),size(q,2))*0.5\n",
    "qv = qv./maximum(qv)\n",
    "Gray.(qv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = qv[:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = A\\b\n",
    "norm(A*x-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gray.(reshape(A*x,192,168))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-confirmation",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] reshape and vectorize a matrix\n",
    "- [ ] apply basic linear algebra operations such as transpose, matrix-matrix product, and solve a linear systerm\n",
    "- [ ] call a linear algebra factorization on your matrix\n",
    "- [ ] use SVD to created a compressed version of an image\n",
    "- [ ] solve the face recognition problem via a least square approach\n",
    "- [ ] create a sparse matrix, and call the components of the Compressed Sparse Column storage\n",
    "- [ ] list a few types of matrices Julia uses (diagonal, upper triangular,...)\n",
    "- [ ] (unrelated to linear algebra): load an image, convert it to grayscale, and extract the RGB layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-giant",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "We can solve a simple form of the face recognition problem even when a face image has been distorted with wrong pixels. Example, one of our inputs was this image: <img src=\"data/0201.png\" width=\"100\">\n",
    "\n",
    "And we were able to detect this face to be closest to the input image: <img src=\"data/0202.png\" width=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-bronze",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "Having a solid understanding of statistics in data science allows us to understand our data better, and allows us to create a quantifiable evaluation of any future conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"StatsBase\")\n",
    "Pkg.add(\"RDatasets\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"StatsPlots\")\n",
    "Pkg.add(\"KernelDensity\")\n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"HypothesisTests\")\n",
    "Pkg.add(\"PyCall\")\n",
    "Pkg.add(\"MLBase\")\n",
    "using Statistics\n",
    "using StatsBase\n",
    "using RDatasets\n",
    "using Plots\n",
    "using StatsPlots\n",
    "using KernelDensity\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using HypothesisTests\n",
    "using PyCall\n",
    "using MLBase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-hartford",
   "metadata": {},
   "source": [
    "In this notebook, we will use eruption data on the faithful geyser. The data will contain wait times between every consecutive times the geyser goes off and the length of the eruptions.\n",
    "<img src=\"data/faithful.JPG\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = dataset(\"datasets\",\"faithful\")\n",
    "@show names(D)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "eruptions = D[!,:Eruptions]\n",
    "scatter(eruptions,label=\"eruptions\")\n",
    "waittime = D[!,:Waiting]\n",
    "scatter!(waittime,label=\"wait time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-member",
   "metadata": {},
   "source": [
    "### üîµStatistics plots\n",
    "As you can see, this doesn't tell us much about the data... Let's try some statistical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot([\"eruption length\"],transpose(eruptions),legend=false,size=(200,400),whisker_width=1,ylabel=\"time in minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-integer",
   "metadata": {},
   "source": [
    "Statistical plots such as a box plot (and a violin plot as we will see in notebook `12. Visualization`), can provide a much better understanding of the data. Here, we immediately see that the median time of each eruption is about 4 minutes.\n",
    "\n",
    "The next plot we will see is a histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(eruptions,label=\"eruptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "?histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(eruptions,bins=:sqrt,label=\"eruptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=kde(eruptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-arbitration",
   "metadata": {},
   "source": [
    "If we want the histogram and the kernel density graph to be aligned we need to remember that the \"density contribution\" of every point added to one of these histograms is `1/(nb of elements)*bin width`. Read more about kernel density estimates on its wikipedia page https://en.wikipedia.org/wiki/Kernel_density_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(eruptions,label=\"eruptions\")\n",
    "plot!(p.x,p.density .* length(eruptions), linewidth=3,color=2,label=\"kde fit\") # nb of elements*bin width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(eruptions,bins=:sqrt,label=\"eruptions\")\n",
    "plot!(p.x,p.density .* length(eruptions) .*0.2, linewidth=3,color=2,label=\"kde fit\") # nb of elements*bin width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "myrandomvector = randn(100_000)\n",
    "histogram(myrandomvector)\n",
    "p=kde(myrandomvector)\n",
    "plot!(p.x,p.density .* length(myrandomvector) .*0.1, linewidth=3,color=2,label=\"kde fit\") # nb of elements*bin width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-aspect",
   "metadata": {},
   "source": [
    "### üîµProbability distributions\n",
    "Another way to generate the same plot is via using the `Distributions` package and choosing the probability distribution you want, and then drawing random numbers from it. As an example, we will use `d = Normal()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Normal()\n",
    "myrandomvector = rand(d,100000)\n",
    "histogram(myrandomvector)\n",
    "p=kde(myrandomvector)\n",
    "plot!(p.x,p.density .* length(myrandomvector) .*0.1, linewidth=3,color=2,label=\"kde fit\") # nb of elements*bin width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Binomial(40) \n",
    "myrandomvector = rand(b,1000000)\n",
    "histogram(myrandomvector)\n",
    "p=kde(myrandomvector)\n",
    "plot!(p.x,p.density .* length(myrandomvector) .*0.5,color=2,label=\"kde fit\") # nb of elements*bin width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(1000)\n",
    "d = fit(Normal, x)\n",
    "myrandomvector = rand(d,1000)\n",
    "histogram(myrandomvector,nbins=20,fillalpha=0.3,label=\"fit\")\n",
    "histogram!(x,nbins=20,linecolor = :red,fillalpha=0.3,label=\"myvector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = eruptions\n",
    "d = fit(Normal, x)\n",
    "myrandomvector = rand(d,1000)\n",
    "histogram(myrandomvector,nbins=20,fillalpha=0.3)\n",
    "histogram!(x,nbins=20,linecolor = :red,fillalpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-strand",
   "metadata": {},
   "source": [
    "### üîµHypothesis testing\n",
    "Next, we will perform hypothesis testing using the `HypothesisTests.jl` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "?OneSampleTTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "myrandomvector = randn(1000)\n",
    "OneSampleTTest(myrandomvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneSampleTTest(eruptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-checkout",
   "metadata": {},
   "source": [
    "A note about p-values: Currently using the pvalue of spearman and pearson correlation from Python. But you can follow the formula here to implement your own.\n",
    "https://stackoverflow.com/questions/53345724/how-to-use-julia-to-compute-the-pearson-correlation-coefficient-with-p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy_stats = pyimport(\"scipy.stats\")\n",
    "@show scipy_stats.spearmanr(eruptions,waittime)\n",
    "@show scipy_stats.pearsonr(eruptions,waittime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy_stats.pearsonr(eruptions,waittime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "corspearman(eruptions,waittime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor(eruptions,waittime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(eruptions,waittime,xlabel=\"eruption length\",\n",
    "    ylabel=\"wait time between eruptions\",legend=false,grid=false,size=(400,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-passenger",
   "metadata": {},
   "source": [
    "Interesting! This means that the next time you visit Yellowstone National part ot see the faithful geysser and you have to wait for too long for it to go off, you will likely get a longer eruption! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-relation",
   "metadata": {},
   "source": [
    "### üîµAUC and Confusion matrix\n",
    "Finally, we will cover basic tools you will need such as AUC scores or confusion matrix. We use the `MLBase` package for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = [1, 1, 1, 1, 1, 1, 1, 2]\n",
    "pred = [1, 1, 2, 2, 1, 1, 1, 1]\n",
    "C = confusmat(2, gt, pred)   # compute confusion matrix\n",
    "C ./ sum(C, dims=2)   # normalize per class\n",
    "sum(diag(C)) / length(gt)  # compute correct rate from confusion matrix\n",
    "correctrate(gt, pred)\n",
    "C = confusmat(2, gt, pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = [1, 1, 1, 1, 1, 1, 1, 0];\n",
    "pred = [1, 1, 0, 0, 1, 1, 1, 1]\n",
    "ROC = MLBase.roc(gt,pred)\n",
    "recall(ROC)\n",
    "precision(ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-headset",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] generate statistics plots such as box plot, histogram, and kernel densities\n",
    "- [ ] generate distributions in Julia, and draw random numbers accordingly\n",
    "- [ ] fit a given set of numbers to a distribution\n",
    "- [ ] compute basic evaluation metrics such as AUC and confusion matrix\n",
    "- [ ] run hypothesis testing\n",
    "- [ ] compute correlations and p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-welsh",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "<img src=\"data/faithful.JPG\" width=\"300\">\n",
    "\n",
    "If you go Yellowstone national park and you find out that the old faithful geyser is taking too long to erupt, then the wait might be worth it because you are likely to experience a longer eruption (i.e. there seems to be a high correlation between wait time and eruption time).\n",
    "\n",
    "<img src=\"data/0301.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-currency",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "As the name says, dimensionality reduction is the idea of reducing your feature set to a much smaller number. Dimensionality reduction is often used in visualization of datasets to try and detect samples that are similar. We will cover three dimensionality reduction techniques here: \n",
    "1. t-SNE\n",
    "2. PCA\n",
    "3. umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages we will use throughout this notebook\n",
    "import Pkg;\n",
    "Pkg.add(\"UMAP\")\n",
    "Pkg.add(\"Makie\")\n",
    "Pkg.add(\"XLSX\")\n",
    "Pkg.add(\"VegaDatasets\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"MultivariateStats\")\n",
    "Pkg.add(\"RDatasets\")\n",
    "Pkg.add(\"StatsBase\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"ScikitLearn\")\n",
    "Pkg.add(\"MLBase\")\n",
    "Pkg.add(\"Distances\")\n",
    "\n",
    "using UMAP\n",
    "using Makie\n",
    "using XLSX\n",
    "using VegaDatasets\n",
    "using DataFrames\n",
    "using MultivariateStats\n",
    "using RDatasets\n",
    "using StatsBase\n",
    "using Statistics\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "using ScikitLearn\n",
    "using MLBase\n",
    "using Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = DataFrame(VegaDatasets.dataset(\"cars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropmissing!(C)\n",
    "M = Matrix(C[:,2:7])\n",
    "names(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_origin = C[:,:Origin]\n",
    "carmap = labelmap(car_origin) #from MLBase\n",
    "uniqueids = labelencode(carmap,car_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-personal",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ PCA \n",
    "We will first center the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and normalize the data\n",
    "data = M\n",
    "data = (data .- mean(data,dims = 1))./ std(data,dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each car is now a column, PCA takes features - by - samples matrix\n",
    "data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = fit(PCA,data',maxoutdim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = projection(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "P'*(data[1,:]-mean(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yte = MultivariateStats.transform(p, data') #notice that Yte[:,1] is the same as P'*(data[1,:]-mean(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct testing observations (approximately)\n",
    "Xr = reconstruct(p, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(Xr-data') # this won't be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.scatter(Yte[1,:],Yte[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.scatter(Yte[1,car_origin.==\"USA\"],Yte[2,car_origin.==\"USA\"],color=1,label=\"USA\")\n",
    "Plots.xlabel!(\"pca component1\")\n",
    "Plots.ylabel!(\"pca component2\")\n",
    "Plots.scatter!(Yte[1,car_origin.==\"Japan\"],Yte[2,car_origin.==\"Japan\"],color=2,label=\"Japan\")\n",
    "Plots.scatter!(Yte[1,car_origin.==\"Europe\"],Yte[2,car_origin.==\"Europe\"],color=3,label=\"Europe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = fit(PCA,data',maxoutdim=3)\n",
    "Yte = MultivariateStats.transform(p, data')\n",
    "scatter3d(Yte[1,:],Yte[2,:],Yte[3,:],color=uniqueids,legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = Makie.scatter(Yte[1,:],Yte[2,:],Yte[3,:],color=uniqueids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-auckland",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ t-SNE\n",
    "The next method we will use for dimensionality reduction is t-SNE. There are multiple ways you can call t-SNE from julia. Check out this notebook: https://github.com/nassarhuda/JuliaTutorials/blob/master/TSNE/TSNE.ipynb. But we will take this opportunity to try out something new... Call a function from the Scikit learn python package. This makes use of the package `ScikitLearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import manifold : TSNE\n",
    "tfn = TSNE(n_components=2) #,perplexity=20.0,early_exaggeration=50)\n",
    "Y2 = tfn.fit_transform(data);\n",
    "Plots.scatter(Y2[:,1],Y2[:,2],color=uniqueids,legend=false,size=(400,300),markersize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-horizontal",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Next, UMAP\n",
    "This will be our final dimensionality reduction method and we will use the package `UMAP` for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = cor(data,data,dims=2)\n",
    "embedding = umap(L, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.scatter(embedding[1,:],embedding[2,:],color=uniqueids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-couple",
   "metadata": {},
   "source": [
    "For UMAP, we can create distances between every pair of observations differently, if we choose to. But even with both choices, we will see that UMAP generates a very similar pattern to what we have observed with t-SNE and PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pairwise(Euclidean(), data, data,dims=1) \n",
    "embedding = umap(-L, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.scatter(embedding[1,:],embedding[2,:],color=uniqueids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-heritage",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] apply tsne on your data\n",
    "- [ ] apply umap on your data\n",
    "- [ ] apply pca on your data\n",
    "- [ ] generate a 3d plot\n",
    "- [ ] call a function from Python's ScikitLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-angel",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "All dimensionality reduction techniques we used seemed to agree on that European and Japanese cars seem to be similar in specifications where as American cars seem to form their own two clusters based on their specifications.\n",
    "\n",
    "Blue are American cars. Green and orange are Japanese and European.\n",
    "\n",
    "<img src=\"data/0401.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-advisory",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Put simply, the task of clustering is to place observations that seem similar within the same cluster. Clustering is commonly used in two dimensional data where the goal is to create clusters based on coordinates. Here, we will use something similar. We will cluster houses based on their latitude-longitude locations using several different clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages we will use throughout this notebook\n",
    "import Pkg;\n",
    "Pkg.add(\"Clustering\")\n",
    "Pkg.add(\"VegaLite\")\n",
    "Pkg.add(\"VegaDatasets\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"JSON\")\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"Distances\")\n",
    "\n",
    "using Clustering\n",
    "using VegaLite\n",
    "using VegaDatasets\n",
    "using DataFrames\n",
    "using Statistics\n",
    "using JSON\n",
    "using CSV\n",
    "using Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\",\"newhouses.csv\")\n",
    "houses = CSV.read(\"newhouses.csv\",DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "names(houses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-retention",
   "metadata": {},
   "source": [
    "We will use the `VegaLite` package here for plotting. This package makes it very easy to plot information on a map. All you need is a JSON file of the map you intend to draw. Here, we will use the California counties JSON file and plot each house on the map and color code it via a heatmap of the price. This is done by this line `color=\"median_house_value:q\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_shape = JSON.parsefile(\"data/california-counties.json\")\n",
    "VV = VegaDatasets.VegaJSONDataset(cali_shape,\"data/california-counties.json\")\n",
    "\n",
    "@vlplot(width=500, height=300) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=:black,\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=VV,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:cb_2015_california_county_20m\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ")+\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=houses,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size={value=12},\n",
    "    color=\"median_house_value:q\"\n",
    "                    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-joining",
   "metadata": {},
   "source": [
    "One thing we will try and explore in this notebook is if clustering the houses has any direct relationship with their prices, so we will bucket the houses into intervals of $50000 and re perform the color codes based on each bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketprice = Int.(div.(houses[!,:median_house_value],50000))\n",
    "insertcols!(houses,3,:cprice=bucketprice)\n",
    "\n",
    "@vlplot(width=500, height=300) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=:black,\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=VV,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:cb_2015_california_county_20m\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ")+\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=houses,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size={value=12},\n",
    "    color=\"cprice:n\"\n",
    "                    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-secretariat",
   "metadata": {},
   "source": [
    "### üü§K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = houses[!, [:latitude,:longitude]]\n",
    "C = kmeans(Matrix(X)', 10) \n",
    "insertcols!(houses,3,:cluster10=>C.assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(width=500, height=300) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=:black,\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=VV,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:cb_2015_california_county_20m\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ")+\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=houses,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size={value=12},\n",
    "    color=\"cluster10:n\"\n",
    "                    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-damages",
   "metadata": {},
   "source": [
    "Yes, location affects price of the house but this means location as in proximity to water, prosimity to downtown, promisity to a bus stop and so on\n",
    "\n",
    "lets' see if this remains true for the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-valentine",
   "metadata": {},
   "source": [
    "### üü§K-medoids clustering\n",
    "For this type of clustering, we need to build a distance matrix. We will use the `Distances` package for this purpose and compute the pairwise Euclidean distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmatrix = Matrix(X)'\n",
    "D = pairwise(Euclidean(), xmatrix, xmatrix,dims=2) \n",
    "\n",
    "K = kmedoids(D,10)\n",
    "insertcols!(houses,3,:medoids_clusters=>K.assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(width=500, height=300) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=:black,\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=VV,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:cb_2015_california_county_20m\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ")+\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=houses,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size={value=12},\n",
    "    color=\"medoids_clusters:n\"\n",
    "                    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-german",
   "metadata": {},
   "source": [
    "### üü§Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = hclust(D)\n",
    "L = cutree(K;k=10)\n",
    "insertcols!(houses,3,:hclust_clusters=>L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(width=500, height=300) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=:black,\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=VV,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:cb_2015_california_county_20m\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ")+\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=houses,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size={value=12},\n",
    "    color=\"hclust_clusters:n\"\n",
    "                    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-architect",
   "metadata": {},
   "source": [
    "### üü§DBscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "?dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distances\n",
    "dclara = pairwise(SqEuclidean(), Matrix(X)',dims=2)\n",
    "L = dbscan(dclara, 0.05, 10)\n",
    "@show length(unique(L.assignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertcols!(houses,3,:dbscanclusters3=>L.assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(width=500, height=300) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "    \n",
    "        fill=:black,\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=VV,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:cb_2015_california_county_20m\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ")+\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=houses,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size={value=12},\n",
    "    color=\"dbscanclusters3:n\"\n",
    "                    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-overhead",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] run kmeans clustering on your data\n",
    "- [ ] run kmedoids clustering on your data\n",
    "- [ ] run hierarchial clustering on your data\n",
    "- [ ] run DBscan clustering on your data\n",
    "- [ ] modify a dataframe and add a new named column\n",
    "- [ ] generate good looking plots of maps using the VegaLite package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-pittsburgh",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "Prices in California do not seem to have an exact mapping with geographical locations. In specifc, performing a clustering algorithm on the houses dataset we had did not reveal a mapping with the price ranges. This indicate that prices relationship to geographical location is not necessairly based on neighborhood but probably other factors like closeness to the water or closeness to a downtown. Here is a figure with a heat map of prices \n",
    "<img src=\"data/0501.png\" width=\"400\">\n",
    "And here is a and k-means clustering of the same houses based on their location\n",
    "<img src=\"data/0502.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-analysis",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Put simply, classification is the task of predicting a label for a given observation. For example: you are given certain physical descriptions of an animal, and your taks is to classify them as either a dog or a cat. Here, we will classify iris flowers.\n",
    "\n",
    "As we will see later, we will use different classifiers and at the end of this notebook, we will compare them. We will define our accuracy function right now to get it out of the way. We will use a simple accuracy function that returns the ratio of the number of correctly classified observations to the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "findaccuracy(predictedvals,groundtruthvals) = sum(predictedvals.==groundtruthvals)/length(groundtruthvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages we will use throughout this notebook\n",
    "import Pkg;\n",
    "Pkg.add(\"GLMNet\")\n",
    "Pkg.add(\"RDatasets\")\n",
    "Pkg.add(\"MLBase\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"DecisionTree\")\n",
    "Pkg.add(\"Distances\")\n",
    "Pkg.add(\"NearestNeighbors\")\n",
    "Pkg.add(\"Random\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"DataStructures\")\n",
    "Pkg.add(\"LIBSVM\")\n",
    "\n",
    "using GLMNet\n",
    "using RDatasets\n",
    "using MLBase\n",
    "using Plots\n",
    "using DecisionTree\n",
    "using Distances\n",
    "using NearestNeighbors\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using DataStructures\n",
    "using LIBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = dataset(\"datasets\", \"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Matrix(iris[:,1:4])\n",
    "irislabels = iris[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "irislabelsmap = labelmap(irislabels)\n",
    "y = labelencode(irislabelsmap, irislabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-founder",
   "metadata": {},
   "source": [
    "In classification, we often want to use some of the data to fit a model, and the rest of the data to validate (commonly known as `training` and `testing` data). We will get this data ready now so that we can easily use it in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "function perclass_splits(y,at)\n",
    "    uids = unique(y)\n",
    "    keepids = []\n",
    "    for ui in uids\n",
    "        curids = findall(y.==ui)\n",
    "        rowids = randsubseq(curids, at) \n",
    "        push!(keepids,rowids...)\n",
    "    end\n",
    "    return keepids\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "?randsubseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainids = perclass_splits(y,0.7)\n",
    "testids = setdiff(1:length(y),trainids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-salvation",
   "metadata": {},
   "source": [
    "We will need one more function, and that is the function that will assign classes based on the predicted values when the predicted values are continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_class(predictedvalue) = argmin(abs.(predictedvalue .- [1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-mercury",
   "metadata": {},
   "source": [
    "### üü£ Method 1: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = glmnet(X[trainids,:], y[trainids])\n",
    "cv = glmnetcv(X[trainids,:], y[trainids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best lambda to predict with.\n",
    "path = glmnet(X[trainids,:], y[trainids])\n",
    "cv = glmnetcv(X[trainids,:], y[trainids])\n",
    "mylambda = path.lambda[argmin(cv.meanloss)]\n",
    "\n",
    "path = glmnet(X[trainids,:], y[trainids],lambda=[mylambda]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = X[testids,:];\n",
    "predictions_lasso = GLMNet.predict(path,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lasso = assign_class.(predictions_lasso)\n",
    "findaccuracy(predictions_lasso,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-shaft",
   "metadata": {},
   "source": [
    "### üü£ Method 2: Ridge\n",
    "We will use the same function but set alpha to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best lambda to predict with.\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0);\n",
    "cv = glmnetcv(X[trainids,:], y[trainids],alpha=0)\n",
    "mylambda = path.lambda[argmin(cv.meanloss)]\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0,lambda=[mylambda]);\n",
    "q = X[testids,:];\n",
    "predictions_ridge = GLMNet.predict(path,q)\n",
    "predictions_ridge = assign_class.(predictions_ridge)\n",
    "findaccuracy(predictions_ridge,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-lancaster",
   "metadata": {},
   "source": [
    "### üü£ Method 3: Elastic Net\n",
    "We will use the same function but set alpha to 0.5 (it's the combination of lasso and ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best lambda to predict with.\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0.5);\n",
    "cv = glmnetcv(X[trainids,:], y[trainids],alpha=0.5)\n",
    "mylambda = path.lambda[argmin(cv.meanloss)]\n",
    "path = glmnet(X[trainids,:], y[trainids],alpha=0.5,lambda=[mylambda]);\n",
    "q = X[testids,:];\n",
    "predictions_EN = GLMNet.predict(path,q)\n",
    "predictions_EN = assign_class.(predictions_EN)\n",
    "findaccuracy(predictions_EN,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-insertion",
   "metadata": {},
   "source": [
    "### üü£ Method 4: Decision Trees\n",
    "We will use the package `DecisionTree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "DecisionTree.fit!(model, X[trainids,:], y[trainids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = X[testids,:];\n",
    "predictions_DT = DecisionTree.predict(model, q)\n",
    "findaccuracy(predictions_DT,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-camera",
   "metadata": {},
   "source": [
    "### üü£ Method 5: Random Forests\n",
    "The `RandomForestClassifier` is available through the `DecisionTree` package as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_trees=20)\n",
    "DecisionTree.fit!(model, X[trainids,:], y[trainids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = X[testids,:];\n",
    "predictions_RF = DecisionTree.predict(model, q)\n",
    "findaccuracy(predictions_RF,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-laptop",
   "metadata": {},
   "source": [
    "### üü£ Method 6: Using a Nearest Neighbor method\n",
    "We will use the `NearestNeighbors` package here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[trainids,:]\n",
    "ytrain = y[trainids]\n",
    "kdtree = KDTree(Xtrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = X[testids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, dists = knn(kdtree, queries', 5, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ytrain[hcat(idxs...)]\n",
    "possible_labels = map(i->counter(c[:,i]),1:size(c,2))\n",
    "predictions_NN = map(i->parse(Int,string(argmax(DataFrame(possible_labels[i])[1,:]))),1:size(c,2))\n",
    "findaccuracy(predictions_NN,y[testids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-probability",
   "metadata": {},
   "source": [
    "### üü£ Method 7: Support Vector Machines\n",
    "We will use the `LIBSVM` package here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[trainids,:]\n",
    "ytrain = y[trainids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svmtrain(Xtrain', ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_SVM, decision_values = svmpredict(model, X[testids,:]')\n",
    "findaccuracy(predictions_SVM,y[testids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accuracies = zeros(7)\n",
    "methods = [\"lasso\",\"ridge\",\"EN\", \"DT\", \"RF\",\"kNN\", \"SVM\"]\n",
    "ytest = y[testids]\n",
    "overall_accuracies[1] = findaccuracy(predictions_lasso,ytest)\n",
    "overall_accuracies[2] = findaccuracy(predictions_ridge,ytest)\n",
    "overall_accuracies[3] = findaccuracy(predictions_EN,ytest)\n",
    "overall_accuracies[4] = findaccuracy(predictions_DT,ytest)\n",
    "overall_accuracies[5] = findaccuracy(predictions_RF,ytest)\n",
    "overall_accuracies[6] = findaccuracy(predictions_NN,ytest)\n",
    "overall_accuracies[7] = findaccuracy(predictions_SVM,ytest)\n",
    "hcat(methods, overall_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-dinner",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] split your data into training and testing data to test the effectiveness of a certain method\n",
    "- [ ] apply a simple accuracy function to test the effectiveness of a certain method\n",
    "- [ ] run multiple classification algorithms:\n",
    "    - [ ] LASSO\n",
    "    - [ ] Ridge\n",
    "    - [ ] ElasticNet\n",
    "    - [ ] Decision Tree\n",
    "    - [ ] Random Forest\n",
    "    - [ ] Nearest Neighbors\n",
    "    - [ ] Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-capital",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "We used multiple methods to run classification on the `iris` dataset which is a dataset of flowers and there are three types of iris flowers in it. We split the data into training and testing and ran our methods. Here is the scoreboard:\n",
    "\n",
    "| method | accuracy score |\n",
    "|---|---|\n",
    "| lasso  |1.0|\n",
    "| ridge  |1.0|\n",
    "| EN     |1.0|\n",
    "| DT     |0.960784|\n",
    "| RF     |0.980392|\n",
    "| kNN    |1.0|\n",
    "| SVM    |1.0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages we will use throughout this notebook\n",
    "import Pkg;\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"StatsBase\")\n",
    "Pkg.add(\"PyCall\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"GLM\")\n",
    "Pkg.add(\"Tables\")\n",
    "Pkg.add(\"XLSX\")\n",
    "Pkg.add(\"MLBase\")\n",
    "Pkg.add(\"RDatasets\")\n",
    "Pkg.add(\"LsqFit\")\n",
    "\n",
    "using Plots\n",
    "using Statistics\n",
    "using StatsBase\n",
    "using PyCall\n",
    "using DataFrames\n",
    "using GLM\n",
    "using Tables\n",
    "using XLSX\n",
    "using MLBase\n",
    "using RDatasets\n",
    "using LsqFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = repeat(1:0.5:10, inner=2)\n",
    "yvals = 3 .+ xvals .+ 2 .* rand(length(xvals)) .-1\n",
    "scatter(xvals, yvals, color=:black, leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "function find_best_fit(xvals,yvals)\n",
    "    meanx = mean(xvals)\n",
    "    meany = mean(yvals)\n",
    "    stdx = std(xvals)\n",
    "    stdy = std(yvals)\n",
    "    r = cor(xvals,yvals)\n",
    "    a = r*stdy/stdx\n",
    "    b = meany - a*meanx\n",
    "    return a,b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = find_best_fit(xvals,yvals)\n",
    "ynew = a .* xvals .+ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "np = pyimport(\"numpy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = xvals\n",
    "ydata = yvals\n",
    "@time myfit = np.polyfit(xdata, ydata, 1);\n",
    "ynew2 = collect(xdata) .* myfit[1] .+ myfit[2];\n",
    "scatter(xvals,yvals)\n",
    "plot!(xvals,ynew)\n",
    "plot!(xvals,ynew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(X=xdata, Y=ydata)\n",
    "ols = lm(@formula(Y ~ X), data)\n",
    "plot!(xdata,predict(ols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-russia",
   "metadata": {},
   "source": [
    "Now let's get some real data. We will use housing information from zillow, check out the file `zillow_data_download_april2020.xlsx` for a quick look of what the data looks like. Our goal will be to build a linear regression model between the number of houses listed vs the number of houses sold in a few states. Fitting these models can serve as a key real estate indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play around with data for a bit\n",
    "R = XLSX.readxlsx(\"data/zillow_data_download_april2020.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_counts = R[\"Sale_counts_city\"][:]\n",
    "df_sale_counts = DataFrame(sale_counts[2:end,:],Symbol.(sale_counts[1,:]))\n",
    "\n",
    "monthly_listings = R[\"MonthlyListings_City\"][:]\n",
    "df_monthly_listings = DataFrame(monthly_listings[2:end,:],Symbol.(monthly_listings[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_listings_2020_02 = df_monthly_listings[!,[1,2,3,4,5,end]]\n",
    "rename!(monthly_listings_2020_02, Symbol(\"2020-02\") .=> Symbol(\"listings\"))\n",
    "\n",
    "sale_counts_2020_02 = df_sale_counts[!,[1,end]]\n",
    "rename!(sale_counts_2020_02, Symbol(\"2020-02\") .=> Symbol(\"sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feb2020data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feb2020data = innerjoin(monthly_listings_2020_02,sale_counts_2020_02,on=:RegionID) #, type=\"outer\")\n",
    "dropmissing!(Feb2020data)\n",
    "sales = Feb2020data[!,:sales]\n",
    "# prices = Feb2020data[!,:price]\n",
    "counts = Feb2020data[!,:listings]\n",
    "using DataStructures\n",
    "states = Feb2020data[!,:StateName]\n",
    "C = counter(states)\n",
    "C.map\n",
    "countvals = values(C.map)\n",
    "topstates = sortperm(collect(countvals),rev=true)[1:10]\n",
    "states_of_interest = collect(keys(C.map))[topstates]\n",
    "all_plots = Array{Plots.Plot}(undef,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feb2020data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plots = Array{Plots.Plot}(undef,10)\n",
    "for (i,si) in enumerate(states_of_interest)\n",
    "    curids = findall(Feb2020data[!,:StateName].==si)\n",
    "    data = DataFrame(X=float.(counts[curids]), Y=float.(sales[curids]))\n",
    "    ols = GLM.lm(@formula(Y ~ 0 + X), data)    \n",
    "    all_plots[i] = scatter(counts[curids],sales[curids],markersize=2,\n",
    "        xlim=(0,500),ylim=(0,500),color=i,aspect_ratio=:equal,\n",
    "        legend=false,title=si)\n",
    "    @show si,coef(ols)\n",
    "    plot!(counts[curids],predict(ols),color=:black)\n",
    "end\n",
    "plot(all_plots...,layout=(2,5),size=(900,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plots = Array{Plots.Plot}(undef,10)\n",
    "for (i,si) in enumerate(states_of_interest)\n",
    "    curids = findall(Feb2020data[!,:StateName].==si)\n",
    "    data = DataFrame(X=float.(counts[curids]), Y=float.(sales[curids]))\n",
    "    ols = GLM.lm(@formula(Y ~ X), data)    \n",
    "    all_plots[i] = scatter(counts[curids],sales[curids],markersize=2,\n",
    "        xlim=(0,500),ylim=(0,500),color=i,aspect_ratio=:equal,\n",
    "        legend=false,title=si)\n",
    "    @show si,coef(ols)\n",
    "    plot!(counts[curids],predict(ols),color=:black)\n",
    "end\n",
    "plot(all_plots...,layout=(2,5),size=(900,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()\n",
    "for (i,si) in enumerate(states_of_interest)\n",
    "    curids = findall(Feb2020data[!,:StateName].==si)\n",
    "    data = DataFrame(X=float.(counts[curids]), Y=float.(sales[curids]))\n",
    "    ols = GLM.lm(@formula(Y ~ 0 + X), data)    \n",
    "    scatter!(counts[curids],sales[curids],markersize=2,\n",
    "        xlim=(0,500),ylim=(0,500),color=i,aspect_ratio=:equal,\n",
    "        legend=false,marker=(3,3,stroke(0)),alpha=0.2)\n",
    "        if si == \"NC\" || si == \"CA\" || si == \"FL\"\n",
    "            annotate!([(500-20,10+coef(ols)[1]*500,text(si,10))])\n",
    "        end\n",
    "    @show si,coef(ols)\n",
    "    plot!(counts[curids],predict(ols),color=i,linewidth=2)\n",
    "end\n",
    "# plot(all_plots...,layout=(2,5),size=(900,300))\n",
    "xlabel!(\"listings\")\n",
    "ylabel!(\"sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-vietnam",
   "metadata": {},
   "source": [
    "---- \n",
    "### üü† Logistic regression\n",
    "So far, we have shown several ways to solve the linear regression problem in Julia. Here, we will first start with a motivating example of when you would want to use logistic regression. Let's assume that our predictor vector is binary (`0` or `1`), let's fit a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(X=[1,2,3,4,5,6,7], Y=[1,0,1,1,1,1,1])\n",
    "linear_reg = lm(@formula(Y ~ X), data)\n",
    "scatter(data[!,:X],data[!,:Y],legend=false,size=(300,200))\n",
    "plot!(1:7,predict(linear_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-value",
   "metadata": {},
   "source": [
    "What this plot quickly shows is that linear regression may end up predicting values outside the `[0,1]` interval. For an example like this, we will use logistic regression. Interestingly, a generalized linear model (https://en.wikipedia.org/wiki/Generalized_linear_model) unifies concepts like linear regression and logistic regression, and the `GLM` package allows you to apply either of these regressions easily by specifying the `distribution family` and the `link` function. \n",
    "\n",
    "To apply logistic regression via the `GLM` package, you can readily use the `Binomial()` family and the `LogitLink()` link function. \n",
    "\n",
    "Let's load some data and take a look at one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will load this data from RDatasets\n",
    "cats = dataset(\"MASS\", \"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmap = labelmap(cats[!,:Sex])\n",
    "ci = labelencode(lmap, cats[!,:Sex])\n",
    "scatter(cats[!,:BWt],cats[!,:HWt],color=ci,legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(X=cats[!,:HWt], Y=ci.-1)\n",
    "probit = glm(@formula(Y ~ X), data, Binomial(), LogitLink())\n",
    "scatter(data[!,:X],data[!,:Y],label=\"ground truth gender\",color=6)\n",
    "scatter!(data[!,:X],predict(probit),label=\"predicted gender\",color=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-memorial",
   "metadata": {},
   "source": [
    "-----\n",
    "### üü† Non linear regression\n",
    "Finally, sometimes you may have a set of points and the goal is to fit a non-linear function (maybe a quadratic function, a cubic function, an exponential function...). The way we would solve such a problem is by minimizing the least square error between the fitted function and the observations we have. We will use the package `LsqFit` for this task. Note that this problem is usually modeled as a numerical optimizaiton problem.\n",
    "\n",
    "We will first set up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = 0:0.05:10\n",
    "yvals = 1*exp.(-xvals*2) + 2*sin.(0.8*pi*xvals) + 0.15 * randn(length(xvals));\n",
    "scatter(xvals,yvals,legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "@. model(x, p) = p[1]*exp(-x*p[2]) + p[3]*sin(0.8*pi*x)\n",
    "p0 = [0.5, 0.5, 0.5]\n",
    "myfit = curve_fit(model, xvals, yvals, p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = myfit.param\n",
    "findyvals = p[1]*exp.(-xvals*p[2]) + p[3]*sin.(0.8*pi*xvals)\n",
    "scatter(xvals,yvals,legend=false)\n",
    "plot!(xvals,findyvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "@. model(x, p) = p[1]*x\n",
    "myfit = curve_fit(model, xvals, yvals, [0.5])\n",
    "p = myfit.param\n",
    "findyvals = p[1]*xvals\n",
    "scatter(xvals,yvals,legend=false)\n",
    "plot!(xvals,findyvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-computer",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] run a linear regression model\n",
    "- [ ] use the GLM package to pass functions and probability distributions to solve your special regression problem\n",
    "- [ ] use GLM to solve a logistic regression problem\n",
    "- [ ] fit a nonlinear regression to your data using the LsqFit package\n",
    "- [ ] use the LsqFit package to fit a linear function too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-behavior",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "One metric in real estate is to find the number of houses being sold out of the number of houses on the market. We collect multiple data points from multiple states and fit a linear model to these states. It turns out that North Carolina has the highest sold/listed ratios. Florida is one of the least, and California is somewhere in between.\n",
    "\n",
    "<img src=\"data/0701.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages we will use throughout this notebook\n",
    "import Pkg;\n",
    "Pkg.add(\"LightGraphs\")\n",
    "Pkg.add(\"MatrixNetworks\")\n",
    "Pkg.add(\"VegaDatasets\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"SparseArrays\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"VegaLite\")\n",
    "Pkg.add(\"MatrixNetworks\")\n",
    "using MatrixNetworks\n",
    "using LightGraphs\n",
    "using MatrixNetworks\n",
    "using VegaDatasets\n",
    "using DataFrames\n",
    "using SparseArrays\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "using VegaLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = VegaDatasets.dataset(\"cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = VegaDatasets.dataset(\"airports\")\n",
    "flightsairport = VegaDatasets.dataset(\"flights-airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsairport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsairportdf = DataFrame(flightsairport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "allairports = vcat(flightsairportdf[!,:origin],flightsairportdf[!,:destination])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "uairports = unique(allairports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an airports data frame that has a subset of airports that are only included in the routes dataset\n",
    "airportsdf = DataFrame(airports)\n",
    "subsetairports = map(i->findfirst(airportsdf[!, :iata].==uairports[i]),1:length(uairports))\n",
    "airportsdf_subset = airportsdf[subsetairports,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "using VegaLite, VegaDatasets\n",
    "p1 = @vlplot(data=VegaDatasets.dataset(\"cars\"),\n",
    "    mark={type=\"point\", tooltip=true, size=75},\n",
    "    x=:Horsepower,\n",
    "    y=:Miles_per_Gallon,\n",
    "    color={condition={selection=\"org\", field=\"Cylinders\", type=\"ordinal\"}, value=\"\"},\n",
    "    width=800,\n",
    "    height=600,\n",
    "    selection={org={type=\"single\", fields=[\"Origin\"], bind={input=\"select\", options=[nothing, \"Europe\", \"Japan\", \"USA\"]}}},\n",
    "    hover=:Miles_per_Gallon)\n",
    "\n",
    "save(\"cars_dropdown.html\", p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the adjacency matrix\n",
    "ei_ids = findfirst.(isequal.(flightsairportdf[!,:origin]), [uairports])\n",
    "ej_ids = findfirst.(isequal.(flightsairportdf[!,:destination]), [uairports])\n",
    "edgeweights = flightsairportdf[!,:count];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sparse(ei_ids,ej_ids,1,length(uairports),length(uairports))\n",
    "A = max.(A,A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsPlots.spy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearAlgebra.issymmetric(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = SimpleGraph(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=SimpleGraph(10) #SimpleGraph(nnodes,nedges) \n",
    "add_edge!(G,7,5)#modifies graph in place.\n",
    "add_edge!(G,3,5)\n",
    "add_edge!(G,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = scomponents(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = sum(A,dims=2)[:]\n",
    "p1 = Plots.plot(sort(degrees,rev=true),ylabel=\"log degree\",legend=false,yaxis=:log)\n",
    "p2 = Plots.plot(sort(degrees,rev=true),ylabel=\"degree\",legend=false)\n",
    "Plots.plot(p1,p2,size=(600,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-cathedral",
   "metadata": {},
   "source": [
    "This is actually very interesting because it looks like that the airline transportation network seems to fit a powerlaw degree distribution. Knowing that your graph fits a well known model for degree distribution can be very helpful for further studying it. (For instance, there is a lot of literature on graphs that fit power law degree distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdegreeid = argmax(degrees)\n",
    "uairports[maxdegreeid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "us10m = VegaDatasets.dataset(\"us-10m\")\n",
    "@vlplot(width=500, height=300) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=:lightgray,\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=us10m,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:states\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ") +\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=airportsdf_subset,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size={value=10},\n",
    "    color={value=:steelblue}\n",
    ")+\n",
    "@vlplot(\n",
    "    :rule,\n",
    "    data=flightsairport,\n",
    "    transform=[\n",
    "        {filter={field=:origin,equal=:ATL}},\n",
    "        {\n",
    "            lookup=:origin,\n",
    "            from={\n",
    "                data=airportsdf_subset,\n",
    "                key=:iata,\n",
    "                fields=[\"latitude\", \"longitude\"]\n",
    "            },\n",
    "            as=[\"origin_latitude\", \"origin_longitude\"]\n",
    "        },\n",
    "        {\n",
    "            lookup=:destination,\n",
    "            from={\n",
    "                data=airportsdf_subset,\n",
    "                key=:iata,\n",
    "                fields=[\"latitude\", \"longitude\"]\n",
    "            },\n",
    "            as=[\"dest_latitude\", \"dest_longitude\"]\n",
    "        }\n",
    "    ],\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"origin_longitude:q\",\n",
    "    latitude=\"origin_latitude:q\",\n",
    "    longitude2=\"dest_longitude:q\",\n",
    "    latitude2=\"dest_latitude:q\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-elephant",
   "metadata": {},
   "source": [
    "### üü°Shortest path problem\n",
    "Finding the shortest path between two nodes. We will use Dijkstra's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATL_paths = dijkstra(A,maxdegreeid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATL_paths[1][maxdegreeid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(ATL_paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show stop1 = argmax(ATL_paths[1])\n",
    "@show uairports[stop1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show stop2 = ATL_paths[2][stop1]\n",
    "@show uairports[stop2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show stop3 = ATL_paths[2][stop2]\n",
    "@show uairports[stop3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show stop4 = ATL_paths[2][stop3]\n",
    "@show uairports[stop4];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "using VegaLite, VegaDatasets\n",
    "\n",
    "us10m = VegaDatasets.dataset(\"us-10m\")\n",
    "airports = VegaDatasets.dataset(\"airports\")\n",
    "\n",
    "@vlplot(width=800, height=500) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=\"#eee\",\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=us10m,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:states\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ") +\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=airportsdf_subset,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size={value=5},\n",
    "    color={value=:gray}\n",
    ") +\n",
    "@vlplot(\n",
    "    :line,\n",
    "    data={\n",
    "        values=[\n",
    "            {airport=:ATL,order=1},\n",
    "            {airport=:SEA,order=2},\n",
    "            {airport=:JNU,order=3},\n",
    "            {airport=:GST,order=4}\n",
    "        ]\n",
    "    },\n",
    "    transform=[{\n",
    "        lookup=:airport,\n",
    "        from={\n",
    "            data=airports,\n",
    "            key=:iata,\n",
    "            fields=[\"latitude\",\"longitude\"]\n",
    "        }\n",
    "    }],\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    order={field=:order,type=:ordinal}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeid = argmin(degrees)\n",
    "@show uairports[nodeid]\n",
    "d = dijkstra(A,nodeid)\n",
    "argmax(d[1]),uairports[argmax(d[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "function find_path(d,id)\n",
    "    shortestpath = zeros(Int,1+Int.(d[1][id]))\n",
    "    shortestpath[1] = id\n",
    "    for i = 2:length(shortestpath)\n",
    "        shortestpath[i] = d[2][shortestpath[i-1]]\n",
    "    end\n",
    "    return shortestpath\n",
    "end\n",
    "p = find_path(d,123)\n",
    "uairports[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-friendly",
   "metadata": {},
   "source": [
    "### üü°Minimum Spanning Tree (MST)\n",
    "The next problem is forming a minimum spanning tree on the graph. The idea of a minimum spanning tree is to connect all nodes in the graph with as little edges as possible. We will use Prim's algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "?mst_prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "ti,tj,tv,nverts = mst_prim(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = DataFrame(:ei=>uairports[ti],:ej=>uairports[tj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(width=800, height=500) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=\"#eee\",\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=us10m,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:states\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ") +\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=airportsdf_subset,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size={value=20},\n",
    "    color={value=:gray}\n",
    ") +\n",
    "@vlplot(\n",
    "    :rule,\n",
    "    data=df_edges, #data=flightsairport,\n",
    "    transform=[\n",
    "        {\n",
    "            lookup=:ei,\n",
    "            from={\n",
    "                data=airportsdf_subset,\n",
    "                key=:iata,\n",
    "                fields=[\"latitude\", \"longitude\"]\n",
    "            },\n",
    "            as=[\"originx\", \"originy\"]\n",
    "        },\n",
    "        {\n",
    "            lookup=:ej,\n",
    "            from={\n",
    "                data=airportsdf_subset,\n",
    "                key=:iata,\n",
    "                fields=[\"latitude\", \"longitude\"]\n",
    "            },\n",
    "            as=[\"destx\", \"desty\"]\n",
    "        }\n",
    "    ],\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"originy:q\",\n",
    "    latitude=\"originx:q\",\n",
    "    longitude2=\"desty:q\",\n",
    "    latitude2=\"destx:q\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-preservation",
   "metadata": {},
   "source": [
    "### üü°PageRank\n",
    "PageRank is the algorithm that got Google started. The idea is: given an network of connections between multiple nodes (web pages in the cae of Google), is there a way to return a list of ranked nodes? PageRank provides this ranking. Obviously, nodes can be ranked in several different ways but PageRank remains to be one of the most popular methods in network analysis. Let's check out the documentation of `pagerank` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "?MatrixNetworks.pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = MatrixNetworks.pagerank(A,0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertcols!(airportsdf_subset,7,:pagerank_value=>v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(width=500, height=300) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=\"#eee\",\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=us10m,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:states\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ") +\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=airportsdf_subset,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size=\"pagerank_value:q\",\n",
    "    color={value=:steelblue}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-withdrawal",
   "metadata": {},
   "source": [
    "### üü°Clustering Coefficients\n",
    "From Wikipedia: The local clustering coefficient of a vertex (node) in a graph quantifies how close its neighbours are to being a clique (complete graph).\n",
    "\n",
    "This means that if for example, a node is connected to two nodes that are also connected to each other, that node's clustering coeefficient is 1. This can be a good metric to find out which nodes tend to have tight clusters around them. Let's look at the documentation of `clustercoeffs` from MatrixNetworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "?clustercoeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = clustercoeffs(A)\n",
    "cc[findall(cc.<=eps())] .= 0\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertcols!(airportsdf_subset,7,:ccvalues=>cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(width=500, height=300) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=\"#eee\",\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=us10m,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:states\n",
    "        }\n",
    "    },\n",
    "    projection={type=:albersUsa},\n",
    ") +\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=airportsdf_subset,\n",
    "    projection={type=:albersUsa},\n",
    "    longitude=\"longitude:q\",\n",
    "    latitude=\"latitude:q\",\n",
    "    size=\"ccvalues:q\",\n",
    "    color={value=:gray}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-jenny",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] take a list of connections between nodes and form an adjacency matrix out of them\n",
    "- [ ] use the LightGraphs package to create a graph\n",
    "- [ ] detect if a graph is connected or not\n",
    "- [ ] solve the shortest path problem on a graph and a given node\n",
    "- [ ] solve the minimum spanning tree problem on a graph\n",
    "- [ ] solve the PageRank problem on a graph\n",
    "- [ ] find the clustering coefficients of nodes in a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-calendar",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "We ran PageRank (which is a common algorithm to rank nodes in a network), and visualized the PageRank values on the US airports. The results agreed with the hypothesis that known hubs have higher PageRank value. Look at Atlanta, it's actually the biggest.\n",
    "\n",
    "<img src=\"data/0801.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages we will use throughout this notebook\n",
    "import Pkg;\n",
    "Pkg.add(\"Convex\")\n",
    "Pkg.add(\"SCS\")\n",
    "Pkg.add(\"XLSX\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"DelimitedFiles\")\n",
    "using Pkg\n",
    "using Convex\n",
    "using SCS\n",
    "using XLSX\n",
    "using DataFrames\n",
    "using Plots\n",
    "using CSV\n",
    "using Statistics\n",
    "using Images\n",
    "using DelimitedFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-details",
   "metadata": {},
   "source": [
    "### üìà Problem 1 Portfolio investment.\n",
    "Our first problem will be an investment problem. We will look at stock prices from three companies and decide how to spend an amount of $1000 on these three companies. Let's first load some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = DataFrame(XLSX.readtable(\"data/stock_prices.xlsx\",\"Sheet2\")...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-barcelona",
   "metadata": {},
   "source": [
    "`T` is a `DataFrame` that has weekly stock prices value of three companies (Microsoft, Facebook, Apple) from the period of January 2019 - March 2019. We will first take a quick look at these prices in a quick plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(T[!,:MSFT],label=\"Microsoft\")\n",
    "plot!(T[!,:AAPL],label=\"Apple\")\n",
    "plot!(T[!,:FB],label=\"FB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the prices to a Matrix to be used later in the optimization problem\n",
    "prices_matrix = Matrix(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = prices_matrix[1:end-1,:]\n",
    "M2 = prices_matrix[2:end,:]\n",
    "R = (M2.-M1)./M1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-causing",
   "metadata": {},
   "source": [
    "Now let's assume that the vector `x = [x1 x2 x3]` will contain the total number of dollars we will invest in these companies, i.e. `x1` is how much we will invest in the first company (MSFT), `x2` is how much we will invest in FB, and `x3` is how much we will invest in `AAPL`. The return on the investment will be `dot(r,x)`, where `r = [r1 r2 r3]` is the return from each of the companies.\n",
    "\n",
    "Here, `r` is a random variable and we will have to model it in terms of expected values. And the expected value `E(dot(r,x))` will be `E[dot(mean(R,dims=2),x)`. If we want a return of `10%` or more, then we need `dot(r,x) >= 0.1`.\n",
    "\n",
    "Next, we will model the risk matrix. We will skip the derivation of the risk matrix here, but you can read about it here: https://www.kdnuggets.com/2019/06/optimization-python-money-risk.html. The risk matrix will be the covariance matrix of the computed return prices (`R`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_matrix = cov(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the risk matrix is positive definite\n",
    "isposdef(risk_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = mean(R,dims=1)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-recommendation",
   "metadata": {},
   "source": [
    "Now let's solve the following problem: Someone gives you $1000 and tells you to spend them in the form of investment on these three compnaies such that you get a return of 2\\% on what you spent.\n",
    "\n",
    "The goal will be to minimize the risk, that is x'\\*risk_matrix\\*x.\n",
    "The constraints will be \n",
    "- `sum(x) = 1`, we will compute the percentage of investment rather than the exact amount\n",
    "- `dot(r,x) >= 0.02`\n",
    "- `x[i] >= 0`\n",
    "\n",
    "This problem is a convext problem, and we will use `Convex.jl` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(length(r))\n",
    "problem = minimize(x'*risk_matrix*x,[sum(x)==1;r'*x>=0.02;x.>=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-building",
   "metadata": {},
   "source": [
    "Note the `Convex.NotDcp` in the answer above and the warning. `Convex.jl` requires that we pass Dcp compliant problem (Disciplined convex programming). Learn more about the DCP ruleset here: http://cvxr.com/cvx/doc/dcp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the problem DCP compliant\n",
    "problem = minimize(Convex.quadform(x,risk_matrix),[sum(x)==1;r'*x>=0.02;x.>=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve!(problem, SCS.Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(x.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return\n",
    "r'*x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.value .* 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "The conclusion is that we should invest **67.9USD in Microsoft**, **122.3USD in Facebook**, and **809.7USD in Apple**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-benjamin",
   "metadata": {},
   "source": [
    "---\n",
    "### üñºÔ∏è Problem 2 Image recovery.\n",
    "In this problem, we are given an image where some of the pixels have been altered. The goal is to recover the unknonwn pixels by solving an optimization problem. Let's first load the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kref = load(\"data/khiam-small.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = copy(Kref)\n",
    "p = prod(size(K))\n",
    "missingids = rand(1:p,400)\n",
    "K[missingids] .= RGBX{N0f8}(0.0,0.0,0.0)\n",
    "K\n",
    "Gray.(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Float64.(Gray.(K));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-attendance",
   "metadata": {},
   "source": [
    "Given this image, the goal is now to complete the matrix. We will use a common technique for this problem developed by Candes and Tao. The goal will be to create a new matrix `X` where we minimize the nuclear norm of `X` (i.e. the sum of the singular values of `X`), and such that the entries that are already known in `Y` remain the same in `X`. We will again use `Convex.jl` to solve this problem. Let's write it down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctids = findall(Y[:].!=0)\n",
    "X = Convex.Variable(size(Y))\n",
    "problem = minimize(nuclearnorm(X))\n",
    "problem.constraints += X[correctids]==Y[correctids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve!(problem, SCS.Optimizer(eps=1e-3, alpha=1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show norm(float.(Gray.(Kref))-X.value)\n",
    "@show norm(-X.value)\n",
    "colorview(Gray, X.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show norm(float.(Gray.(Kref))-X.value)\n",
    "@show norm(-X.value)\n",
    "colorview(Gray, X.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-chart",
   "metadata": {},
   "source": [
    "---\n",
    "### ü•í Problem 3 Diet optimization problem.\n",
    "This is a common problem in Numerical Optimization, and you can find multiple references about it online. Here, we will use one of the examples in the JuMP package. Refer to this page for details: https://github.com/JuliaOpt/JuMP.jl/blob/master/examples/diet.jl.\n",
    "\n",
    "In this porblem we are given constraints on the number of (minimum, maximum) number of calories, protein, fat, and sodium to consume. We will first build a JuMP container to store this information and pass it as constraints later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"JuMP\")\n",
    "Pkg.add(\"GLPK\")\n",
    "using JuMP\n",
    "using GLPK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_data = JuMP.Containers.DenseAxisArray(\n",
    "    [1800 2200;\n",
    "     91   Inf;\n",
    "     0    65;\n",
    "     0    1779], \n",
    "    [\"calories\", \"protein\", \"fat\", \"sodium\"], \n",
    "    [\"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-portland",
   "metadata": {},
   "source": [
    "You can think of this matrix as indexed by rows via the vector `[\"calories\", \"protein\", \"fat\", \"sodium\"]`, and indexed by columns via the vector `[\"min\", \"max\"]`. In fact, we can now checkout the values: `category_data[\"calories\",\"max\"]` or `category_data[\"fat\",\"min\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show category_data[\"calories\",\"max\"] \n",
    "@show category_data[\"fat\",\"min\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = [\"hamburger\", \"chicken\", \"hot dog\", \"fries\", \"macaroni\", \"pizza\",\"salad\", \"milk\", \"ice cream\"]\n",
    "\n",
    "# we will use the same concept we used above to create an array indexed \n",
    "# by foods this time to record the cost of each of these items\n",
    "cost = JuMP.Containers.DenseAxisArray(\n",
    "    [2.49, 2.89, 1.50, 1.89, 2.09, 1.99, 2.49, 0.89, 1.59],\n",
    "    foods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-riding",
   "metadata": {},
   "source": [
    "Next we will create a new matrix to encode the calories,protein, fat, and sodium present in each of these foods. This will be a matrix encoded by foods by rows, and `[\"calories\", \"protein\", \"fat\", \"sodium\"]` by columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_data = JuMP.Containers.DenseAxisArray(\n",
    "    [410 24 26 730;\n",
    "     420 32 10 1190;\n",
    "     560 20 32 1800;\n",
    "     380  4 19 270;\n",
    "     320 12 10 930;\n",
    "     320 15 12 820;\n",
    "     320 31 12 1230;\n",
    "     100  8 2.5 125;\n",
    "     330  8 10 180], \n",
    "    foods, \n",
    "    [\"calories\", \"protein\", \"fat\", \"sodium\"])\n",
    "\n",
    "@show food_data[\"chicken\", \"fat\"]\n",
    "@show food_data[\"milk\", \"sodium\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "model = Model(GLPK.Optimizer)\n",
    "\n",
    "categories = [\"calories\", \"protein\", \"fat\", \"sodium\"]\n",
    "\n",
    "# add the variables\n",
    "@variables(model, begin\n",
    "    # Variables for nutrition info\n",
    "    category_data[c, \"min\"] <= nutrition[c = categories] <= category_data[c, \"max\"]\n",
    "    # Variables for which foods to buy\n",
    "    buy[foods] >= 0\n",
    "end)\n",
    "\n",
    "# Objective - minimize cost\n",
    "@objective(model, Min, sum(cost[f] * buy[f] for f in foods))\n",
    "\n",
    "# Nutrition constraints\n",
    "@constraint(model, [c in categories],\n",
    "    sum(food_data[f, c] * buy[f] for f in foods) == nutrition[c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "JuMP.optimize!(model)\n",
    "term_status = JuMP.termination_status(model)\n",
    "is_optimal = term_status == MOI.OPTIMAL\n",
    "@show JuMP.primal_status(model) == MOI.FEASIBLE_POINT\n",
    "@show JuMP.objective_value(model) ‚âà 11.8288 atol = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "hcat(buy.data,JuMP.value.(buy.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-directory",
   "metadata": {},
   "source": [
    "----\n",
    "### üó∫Ô∏è How many passports do you need to travel the world without obtaining a visa in advance?\n",
    "This problem is the same problem shown in the JuliaCon 2018 JuMP workshop, with updated code and data. The original post can be found here: https://github.com/juan-pablo-vielma/JuliaCon2018_JuMP_Workshop/blob/master/Introduction_Slides.ipynb.\n",
    "\n",
    "We will first get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    ";git clone https://github.com/ilyankou/passport-index-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-effect",
   "metadata": {},
   "source": [
    "The file we need is `passport-index-dataset/passport-index-matrix.csv`, and we will use the `DelimitedFiles` package to read it -- this is mainly because what we are loading is a matrix and we will have to extract the matrix out of the DataFrame if we use the `CSV` package. Both are viable options, this will just be quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "passportdata = readdlm(joinpath(\"passport-index-dataset\",\"passport-index-matrix.csv\"),',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr = passportdata[2:end,1]\n",
    "vf = (x ->  typeof(x)==Int64 || x == \"VF\" || x == \"VOA\" ? 1 : 0).(passportdata[2:end,2:end]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(GLPK.Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "@variable(model, pass[1:length(cntr)], Bin)\n",
    "@constraint(model, [j=1:length(cntr)], sum( vf[i,j]*pass[i] for i in 1:length(cntr)) >= 1)\n",
    "@objective(model, Min, sum(pass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "JuMP.optimize!(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(JuMP.objective_value(model),\" passports: \",join(cntr[findall(JuMP.value.(pass) .== 1)],\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-surface",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] solve optimization problems via the Convex.jl package\n",
    "- [ ] solve optimization problems via the JuMP.jl package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-documentary",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "We found out that you need at least 21 passports to tour the world visa free. Here is one solutions we found:\n",
    "\n",
    "`Afghanistan, Austria, Comoros, Equatorial Guinea, Eritrea, Gambia, Georgia, Hong Kong, India, Iraq, Kenya, Madagascar, Maldives, North Korea, Papua New Guinea, Seychelles, Singapore, Somalia, Tunisia, United Arab Emirates, Zimbabwe`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-robertson",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "In this notebook, we will walk through one main neural nets example. And that is, classifying the infamous MNIST dataset. **If you have no experience with neural nets prior to this notebook, I recommend doing a quick search for an \"intro to neural nets\"**, there are multiple tutorials/blog posts out there and you can choose the one that works for you.\n",
    "\n",
    "Here, we will use the `Flux` package, but if you want to look at other packages I encourage you to look at `Knet.jl` and `TensorFlow.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; \n",
    "Pkg.add(\"Flux\")\n",
    "Pkg.add(\"Images\")\n",
    "using Flux, Flux.Data.MNIST\n",
    "using Flux: onehotbatch, argmax, crossentropy, throttle\n",
    "using Base.Iterators: repeated\n",
    "using Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = MNIST.images()\n",
    "colorview(Gray, imgs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(imgs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "myFloat32(X) = Float32.(X)\n",
    "fpt_imgs = myFloat32.(imgs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(fpt_imgs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize(x) = x[:]\n",
    "vectorized_imgs = vectorize.(fpt_imgs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(vectorized_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hcat(vectorized_imgs...)\n",
    "size(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "onefigure = X[:,3]\n",
    "t1 = reshape(onefigure,28,28)\n",
    "colorview(Gray,t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MNIST.labels()\n",
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = onehotbatch(labels, 0:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "  Dense(28^2, 32, relu),\n",
    "  Dense(32, 10),\n",
    "  softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "m(onefigure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x, y) = Flux.crossentropy(m(x), y)\n",
    "accuracy(x, y) = mean(argmax(m(x)) .== argmax(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetx = repeated((X, Y), 200)\n",
    "C = collect(datasetx);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalcb = () -> @show(loss(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = Flux.params(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "?Flux.train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ADAM()\n",
    "Flux.train!(loss, ps, datasetx, opt, cb = throttle(evalcb, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = hcat(float.(reshape.(MNIST.images(:test), :))...);\n",
    "test_image = m(tX[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax(test_image) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = reshape(tX[:,1],28,28)\n",
    "colorview(Gray, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "onefigure = X[:,2]\n",
    "m(onefigure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-alignment",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] prepare data to fit the format to create a neural network using Flux.jl\n",
    "- [ ] create a neural network with Flux.jl\n",
    "- [ ] creating an accuracy function and loss function to be passed to train the neural network\n",
    "- [ ] train the neural network\n",
    "- [ ] describe a few tips that can help make your nerual network faster or more accurate (such as using Float32 as opposed to Float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-strand",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "We ran a trained a neural network on a dataset of of handwritten digits (called the MNIST dataset). At the end, we were able to pass this figure to the neural network and the return result was:\n",
    "\n",
    "<img src=\"data/1001.png\" width=\"40\">\n",
    "\n",
    "```\n",
    "10-element Array{Float32,1}:\n",
    " 0.00029263002\n",
    " 1.5993925f-5\n",
    " 0.0002862561\n",
    " 0.0035434738\n",
    " 1.388653f-5\n",
    " 2.4878627f-5\n",
    " 6.433018f-7\n",
    " 0.99414164 ### <= this is the highest number!\n",
    " 0.000118321994\n",
    " 0.0015623316\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-dispatch",
   "metadata": {},
   "source": [
    "## Using other languages\n",
    "Often, I hear that the biggest challenge of moving from another language to Julia is giving up all the codes you have written in other languages or your favorite packages from other languages. **This notebook is not about data science, but it's about your next data science project** (if you're working on a data science project in Julia and you want to use functionality from other langages). Here, we will specifically cover Python, R, and C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-vegetable",
   "metadata": {},
   "source": [
    "### ‚ö´Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"PyCall\")\n",
    "using PyCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "math = pyimport(\"math\")\n",
    "math.sin(math.pi / 4)\n",
    "# returns ‚âà 1/‚àö2 = 0.70710678..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_networkx = pyimport(\"networkx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "py\"\"\"\n",
    "import numpy\n",
    "def find_best_fit_python(xvals,yvals):\n",
    "    meanx = numpy.mean(xvals)\n",
    "    meany = numpy.mean(yvals)\n",
    "    stdx = numpy.std(xvals)\n",
    "    stdy = numpy.std(yvals)\n",
    "    r = numpy.corrcoef(xvals,yvals)[0][1]\n",
    "    a = r*stdy/stdx\n",
    "    b = meany - a*meanx\n",
    "    return a,b\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = repeat(1:0.5:10, inner=2)\n",
    "yvals = 3 .+ xvals .+ 2 .* rand(length(xvals)) .-1\n",
    "find_best_fit_python = py\"find_best_fit_python\"\n",
    "a,b = find_best_fit_python(xvals,yvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-quantity",
   "metadata": {},
   "source": [
    "If the above python code was in a file called `fit_linear.py`, you can call it as follows:\n",
    "```\n",
    "python_linear_fit = pyimport(\"fit_linear\") \n",
    "python_linear_fit.find_best_fit_python(xvals,yvals)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-procurement",
   "metadata": {},
   "source": [
    "### ‚ö´R code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"RCall\")\n",
    "using RCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the rcall function\n",
    "r = rcall(:sum, Float64[1.0, 4.0, 6.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 1\n",
    "@rput z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = R\"z+z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rimport base as rbase\n",
    "rbase.sum([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rlibrary boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "using HypothesisTests\n",
    "OneSampleTTest(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-neighborhood",
   "metadata": {},
   "source": [
    "### ‚ö´C code\n",
    "Calling standard libraries is easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ccall(:clock, Int32, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-notion",
   "metadata": {},
   "source": [
    "Can look at Python and C/C++ examples here: https://github.com/xorJane/Excelling-at-Julia-Basics-and-Beyond/blob/master/JuliaCon2019_Huda/Julia%20Wrappers.ipynb\n",
    "```\n",
    "ccall((:hello_world_repeated,\"hello_world_lib.dylib\"),\n",
    "    Int64,\n",
    "    (Int64,),\n",
    "    10)\n",
    "    ```\n",
    "    \n",
    "**Finally**, I would say that this is the only off-topic notebook in this course, and it's a topic that can be covered on its own in a standalone tutorial... Nevertheless, the goal of this notebook is to tell you that porting your code from Python, R, and C should be easy and straight forward in Julia. \n",
    "\n",
    "# ü•≥ One cool finding\n",
    "\n",
    "You can easily call Python, R, C, and Cpp code from Julia!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-adapter",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "In this notebook, we will cover five visualizations and the hope is that each of them will reveal tips for you for your next plot. There is a wide range of visually pleasing plots you can generate with Julia and I strongly recommend envisioning what you want to plot beforehand and then figuring out how to accomplish it (most likely you will find all what you need in at least one julia plotting package). Here, we will be using the `Plots` package with a `gr()` backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"GKS_ENCODING\"] = \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateabbreviations = Dict(\"Alabama\" => \"AL\",\n",
    "    \"Alaska\" => \"AK\",\n",
    "    \"Arizona\" => \"AZ\",\n",
    "    \"Arkansas\" => \"AR\",\n",
    "    \"California\" => \"CA\",\n",
    "    \"Colorado\" => \"CO\",\n",
    "    \"Connecticut\" => \"CT\",\n",
    "    \"Delaware\" => \"DE\",\n",
    "    \"Florida\" => \"FL\",\n",
    "    \"Georgia\" => \"GA\",\n",
    "    \"Hawaii\" => \"HI\",\n",
    "    \"Idaho\" => \"ID\",\n",
    "    \"Illinois\" => \"IL\",\n",
    "    \"Indiana\" => \"IN\",\n",
    "    \"Iowa\" => \"IA\",\n",
    "    \"Kansas\" => \"KS\",\n",
    "    \"Kentucky\" => \"KY\",\n",
    "    \"Louisiana\" => \"LA\",\n",
    "    \"Maine\" => \"ME\",\n",
    "    \"Maryland\" => \"MD\",\n",
    "    \"Massachusetts\" => \"MA\",\n",
    "    \"Michigan\" => \"MI\",\n",
    "    \"Minnesota\" => \"MN\",\n",
    "    \"Mississippi\" => \"MS\",\n",
    "    \"Missouri\" => \"MO\",\n",
    "    \"Montana\" => \"MT\",\n",
    "    \"Nebraska\" => \"NE\",\n",
    "    \"Nevada\" => \"NV\",\n",
    "    \"New Hampshire\" => \"NH\",\n",
    "    \"New Jersey\" => \"NJ\",\n",
    "    \"New Mexico\" => \"NM\",\n",
    "    \"New York\" => \"NY\",\n",
    "    \"North Carolina\" => \"NC\",\n",
    "    \"North Dakota\" => \"ND\",\n",
    "    \"Ohio\" => \"OH\",\n",
    "    \"Oklahoma\" => \"OK\",\n",
    "    \"Oregon\" => \"OR\",\n",
    "    \"Pennsylvania\" => \"PA\",\n",
    "    \"Rhode Island\" => \"RI\",\n",
    "    \"South Carolina\" => \"SC\",\n",
    "    \"South Dakota\" => \"SD\",\n",
    "    \"Tennessee\" => \"TN\",\n",
    "    \"Texas\" => \"TX\",\n",
    "    \"Utah\" => \"UT\",\n",
    "    \"Vermont\" => \"VT\",\n",
    "    \"Virginia\" => \"VA\",\n",
    "    \"Washington\" => \"WA\",\n",
    "    \"West Virginia\" => \"WV\",\n",
    "    \"Wisconsin\" => \"WI\",\n",
    "    \"Wyoming\" => \"WY\", \n",
    "    \"District of Columbia\"=>\"DC\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using StatsPlots # this package provides stats specific plotting functions\n",
    "gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using StatsBase\n",
    "using MLBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtickslabels = [\"one\",\"five\",\"six\",\"fourteen\"]\n",
    "p = plot(rand(15),xticks = ([1,5,6,14],xtickslabels),xrotation=90,xtickfont=font(13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "function pad_empty_plot(p)\n",
    "    ep = plot(grid=false,legend=false,axis=false,framestyle = :box)#empty plot\n",
    "    newplot = plot(p,ep,layout=@layout([a{0.99h};b{0.001h}]))\n",
    "    return newplot\n",
    "end\n",
    "pad_empty_plot(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "using XLSX\n",
    "using DataFrames\n",
    "D = DataFrame(XLSX.readtable(\"data/zillow_data_download_april2020.xlsx\", \"Sales_median_price_city\")...);\n",
    "dropmissing!(D)\n",
    "states = D[:,:StateName];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "NYids = findall(states.==\"New York\")\n",
    "NYframe = dropmissing(D[NYids,:])\n",
    "CAids = findall(states.==\"California\")\n",
    "CAframe = dropmissing(D[CAids,:])\n",
    "FLids = findall(states.==\"Florida\")\n",
    "FLframe = dropmissing(D[FLids,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-simpson",
   "metadata": {},
   "source": [
    "Just a quick note about plotting with xlabels that are long and rotated. Currently, there seems to be an issue with using xticks labels that are rotated and long, like the plot I show next. As per this issue https://github.com/JuliaPlots/Plots.jl/issues/2107, this hasn't been fixed yet. But here, I create a quick function that will act as a \"hack\" to avoid this problem.\n",
    "\n",
    "One concept I learned from reading one of Edward Tufte's books is the idea of avoiding symmetry. Here, as you can see, each violin plot is symmetric. We can probably fit more information there by making use of each side of the violin plot. And indeed, we will now compare housing prices in these states from February 2020 with housing prices from 10 years before that (February 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-restriction",
   "metadata": {},
   "source": [
    "### üî¥Plot 1: Symmetric violin plots and annotations\n",
    "We will get started by just picking the most recent data we have about these states and plot their violin plots to see the distribution of house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a year: 2020-02\n",
    "ca = CAframe[!,Symbol(\"2020-02\")]\n",
    "ny = NYframe[!,Symbol(\"2020-02\")]\n",
    "fl = FLframe[!,Symbol(\"2020-02\")]\n",
    "\n",
    "violin(repeat([\"New York\"], outer=size(ny)), ny,alpha=0.8)\n",
    "violin!(repeat([\"California\"], outer=size(ca)), ca,alpha=0.8)\n",
    "violin!(repeat([\"Florida\"], outer=size(fl)),fl,alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020 data\n",
    "ca = CAframe[!,Symbol(\"2020-02\")]\n",
    "ny = NYframe[!,Symbol(\"2020-02\")]\n",
    "fl = FLframe[!,Symbol(\"2020-02\")]\n",
    "violin(repeat([\"New York\"], outer=size(ny)), ny,legend=false,alpha=0.8,side=:right)\n",
    "violin!(repeat([\"California\"], outer=size(ca)), ca,alpha=0.8,side=:right)\n",
    "violin!(repeat([\"Florida\"], outer=size(fl)),fl,alpha=0.8,side=:right)\n",
    "\n",
    "### get the February 2010 data\n",
    "ca10 = CAframe[!,Symbol(\"2010-02\")]\n",
    "ny10 = NYframe[!,Symbol(\"2010-02\")]\n",
    "fl10 = FLframe[!,Symbol(\"2010-02\")]\n",
    "\n",
    "violin(repeat([\"New York\"], outer=size(ny)), ny10,legend=false,alpha=0.8,side=:left)\n",
    "violin!(repeat([\"California\"], outer=size(ca)), ca10,alpha=0.8,side=:left)\n",
    "violin!(repeat([\"Florida\"], outer=size(fl)),fl10,alpha=0.8,side=:left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need for using many colors, let's just use one color for 2010, and one color for 2020\n",
    "\n",
    "# pick a year: 2019-02\n",
    "ca = CAframe[!,Symbol(\"2010-02\")]\n",
    "ny = NYframe[!,Symbol(\"2010-02\")]\n",
    "fl = FLframe[!,Symbol(\"2010-02\")]\n",
    "violin(repeat([\"New York\"], outer=size(ny)), ny,alpha=0.8,side=:left,color=6,label=\"2010-02\")\n",
    "violin!(repeat([\"California\"], outer=size(ca)), ca,alpha=0.8,side=:left,color=6,label=\"\")\n",
    "violin!(repeat([\"Florida\"], outer=size(fl)),fl,alpha=0.8,side=:left,color=6,label=\"\")\n",
    "\n",
    "# pick a year: 2020-02\n",
    "ca = CAframe[!,Symbol(\"2020-02\")]\n",
    "ny = NYframe[!,Symbol(\"2020-02\")]\n",
    "fl = FLframe[!,Symbol(\"2020-02\")]\n",
    "violin(repeat([\"New York\"], outer=size(ny)), ny,alpha=0.8,side=:right,color=7,label=\"2020-02\")\n",
    "violin!(repeat([\"California\"], outer=size(ca)), ca,alpha=0.8,side=:right,color=7,label=\"\")\n",
    "violin!(repeat([\"Florida\"], outer=size(fl)),fl,alpha=0.8,side=:right,color=7,label=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a year: 2019-02\n",
    "ca = CAframe[!,Symbol(\"2010-02\")]\n",
    "ny = NYframe[!,Symbol(\"2010-02\")]\n",
    "fl = FLframe[!,Symbol(\"2010-02\")]\n",
    "violin(repeat([\"New York\"], outer=size(ny)), ny,alpha=0.8,side=:left,color=6,label=\"2010-02\")\n",
    "violin!(repeat([\"California\"], outer=size(ca)), ca,alpha=0.8,side=:left,color=6,label=\"\")\n",
    "violin!(repeat([\"Florida\"], outer=size(fl)),fl,alpha=0.8,side=:left,color=6,label=\"\")\n",
    "\n",
    "# pick a year: 2020-02\n",
    "ca = CAframe[!,Symbol(\"2020-02\")]\n",
    "ny = NYframe[!,Symbol(\"2020-02\")]\n",
    "fl = FLframe[!,Symbol(\"2020-02\")]\n",
    "violin(repeat([\"New York\"], outer=size(ny)), ny,alpha=0.8,side=:right,color=7,label=\"2020-02\")\n",
    "violin!(repeat([\"California\"], outer=size(ca)), ca,alpha=0.8,side=:right,color=7,label=\"\")\n",
    "violin!(repeat([\"Florida\"], outer=size(fl)),fl,alpha=0.8,side=:right,color=7,label=\"\")\n",
    "\n",
    "\n",
    "m = median(ny)\n",
    "ep = 0.1\n",
    "annotate!([(0.5+ep,m+0.05,text(m/1000,10,:left))])\n",
    "\n",
    "m = median(ca)\n",
    "ep = 0.1\n",
    "annotate!([(1.5+ep,m+0.05,text(m/1000,10,:left))])\n",
    "\n",
    "m = median(fl)\n",
    "ep = 0.1\n",
    "annotate!([(2.5+ep,m+0.05,text(m/1000,10,:left))])\n",
    "\n",
    "plot!(xtickfont=font(10),size=(500,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it together.\n",
    "\n",
    "ep = 0.05 # will later be used in padding for annotations\n",
    "\n",
    "# set up the plot\n",
    "plot(xtickfont=font(10))\n",
    "\n",
    "states_of_interest = [\"New York\", \"California\", \"Florida\", \"Ohio\",\"Idaho\"]\n",
    "years_of_interst = [Symbol(\"2010-02\"),Symbol(\"2020-02\")]\n",
    "\n",
    "# year 1\n",
    "xstart = 0.5\n",
    "yi = years_of_interst[1]\n",
    "for si in states_of_interest\n",
    "    curids = findall(states.==si)\n",
    "    curFrame = D[curids,:]\n",
    "    curprices = curFrame[!,yi]\n",
    "    m = median(curprices)\n",
    "    annotate!([(xstart-ep,m+0.05,text(m/1000,8,:right))])\n",
    "    xstart += 1\n",
    "    violin!(repeat([si], outer=size(curprices)), curprices,alpha=0.8,side=:left,color=6,label=\"\")\n",
    "end\n",
    "plot!(Shape([],[]),color=6,label=yi)\n",
    "\n",
    "# year 2\n",
    "xstart = 0.5\n",
    "yi = years_of_interst[2]\n",
    "for si in states_of_interest\n",
    "    curids = findall(states.==si)\n",
    "    curFrame = D[curids,:]\n",
    "    curprices = curFrame[!,yi]\n",
    "    m = median(curprices)\n",
    "    annotate!([(xstart+ep,m+0.05,text(m/1000,8,:left))])\n",
    "    xstart += 1\n",
    "    violin!(repeat([si], outer=size(curprices)), curprices,alpha=0.8,side=:right,color=7,label=\"\")\n",
    "end\n",
    "plot!(Shape([],[]),color=7,label=yi)\n",
    "ylabel!(\"housing prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-tongue",
   "metadata": {},
   "source": [
    "### üî¥Plot 2: Bar charts, histograms, and insets\n",
    "Now let's compare states based on the number of location entries they have in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapstates = labelmap(states)\n",
    "stateids = labelencode(mapstates, states)\n",
    "histogram(stateids,nbins=length(mapstates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-sperm",
   "metadata": {},
   "source": [
    "There are a few problems with this histogram. First, unsorted histograms are often harder to read so the first thing we will do is rearrange this histogram. Next, we will add annotations to be able to map each bar to a state quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll start with sorting\n",
    "h = fit(Histogram, stateids,nbins=length(mapstates))\n",
    "sortedids = sortperm(h.weights,rev=true)\n",
    "bar(h.weights[sortedids],legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(h.weights[sortedids],legend=false,orientation = :horizontal,yflip=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just an example of annotations\n",
    "bar(h.weights[sortedids],legend=false,orientation = :horizontal,yflip=true,size=(400,500))\n",
    "stateannotations = mapstates.vs[sortedids]\n",
    "for i = 1:3\n",
    "    annotate!([(h.weights[sortedids][i]-5,i,text(stateannotations[i],10,:left))])\n",
    "end\n",
    "plot!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(h.weights[sortedids],legend=false,orientation = :horizontal,yflip=true,linewidth=0,width=0,size=(400,500))\n",
    "stateannotations = mapstates.vs[sortedids]\n",
    "for i = 1:length(stateannotations)\n",
    "    annotate!([(h.weights[sortedids][i]-5,i,text(stateabbreviations[stateannotations[i]],5,:left))])\n",
    "end\n",
    "plot!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(h.weights[sortedids],legend=false,orientation = :horizontal,\n",
    "        yflip=true,linewidth=0,width=0,color=:gray,alpha=0.8)\n",
    "stateannotations = mapstates.vs[sortedids]\n",
    "for i = 20:20:200\n",
    "    plot!([i,i],[50,0],color=:white)\n",
    "end\n",
    "for i = 1:length(stateannotations)\n",
    "    annotate!([(h.weights[sortedids][i]-5,i,text(stateabbreviations[stateannotations[i]],6,:left))])\n",
    "end\n",
    "plot!(grid=false,yaxis=false,xlim=(0,maximum(h.weights)),xticks = 0:20:200)\n",
    "xlabel!(\"number of listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(h.weights[sortedids],legend=false,orientation = :horizontal,\n",
    "        yflip=true,linewidth=0,color=:gray,alpha=0.8,size=(300,500))\n",
    "stateannotations = mapstates.vs[sortedids]\n",
    "ht = length(h.weights)\n",
    "for i = 20:20:200\n",
    "    plot!([i,i],[ht,0],color=:white)\n",
    "end\n",
    "for i = 1:length(stateannotations)\n",
    "    annotate!([(h.weights[sortedids][i]+2,i,text(stateabbreviations[stateannotations[i]],6,:left))])\n",
    "end\n",
    "plot!(grid=false,yaxis=false,xlim=(0,maximum(h.weights)+5),xticks = 0:20:200)\n",
    "xlabel!(\"number of listings\")\n",
    "\n",
    "f = Plots.plot!(inset = bbox(0.7,0.15,0.25,0.6,:top,:left))\n",
    "bar!(f[2],h.weights[sortedids][21:end],legend=false,orientation = :horizontal,\n",
    "        yflip=true,linewidth=0,width=0,color=:gray,alpha=0.8)\n",
    "for i = 21:length(stateannotations)\n",
    "    annotate!(f[2],[(h.weights[sortedids][i]+1,i-20,text(stateabbreviations[stateannotations[i]],6,:left))])\n",
    "end\n",
    "plot!(f[2],[10,10],[20,0],color=:white,xticks=0:10:20,yaxis=false,grid=false,xlim=(0,20))\n",
    "plot!()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-portsmouth",
   "metadata": {},
   "source": [
    "### üî¥Plot 3: Plots with error bars\n",
    "Next, we will compar state prices over the years and see how they have changed. we will use error bars too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Matrix(NYframe[:,5:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtickslabels = string.(names(NYframe[!,5:end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()\n",
    "for i = 1:size(M,1)\n",
    "    plot!(M[i,:],legend=false)\n",
    "end\n",
    "plot!()\n",
    "p = plot!(xticks = (1:4:length(xtickslabels),xtickslabels[1:4:end]),xrotation=90,xtickfont=font(8),grid=false)\n",
    "pad_empty_plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-landscape",
   "metadata": {},
   "source": [
    "A plot like this isn't indicative of how the price trend is going overall for New York. What we will do next is for each time point, we will find the median value as well as the 80th and 20th percentile and plot these values. Let's write the precentile functin first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "function find_percentile(M, pct)\n",
    "    r = zeros(size(M,2))\n",
    "    for i = 1:size(M,2)\n",
    "        v = M[:,i]\n",
    "        len = length(v)\n",
    "        ind = floor(Int64,pct*len)\n",
    "        newarr = sort(v);\n",
    "        r[i] = newarr[ind];\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "md = find_percentile(M,0.5)\n",
    "mx = find_percentile(M,0.8)\n",
    "mn = find_percentile(M,0.2)\n",
    "plot(md,ribbon =(md.-mn,mx.-md),color = :blue,label=\"NY\",grid=false)\n",
    "p = plot!(xticks = (1:4:length(xtickslabels),xtickslabels[1:4:end]),xrotation=90,xtickfont=font(8))\n",
    "pad_empty_plot(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_individual_state!(plotid,statevalue,colorid)\n",
    "    curids = findall(states.==statevalue)\n",
    "    curFrame = D[curids,:]\n",
    "    M = Matrix(curFrame[:,5:end])\n",
    "    md = find_percentile(M,0.5)\n",
    "    mx = find_percentile(M,0.8)\n",
    "    mn = find_percentile(M,0.2)\n",
    "    plot!(plotid,md,ribbon =(md.-mn,mx.-md),color = colorid,label=stateabbreviations[statevalue],grid=false)\n",
    "    plot!(plotid,xticks = (1:4:length(xtickslabels),xtickslabels[1:4:end]),xrotation=90,xtickfont=font(8))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotid = plot()\n",
    "plot_individual_state!(plotid,\"Indiana\",1)\n",
    "plot_individual_state!(plotid,\"Ohio\",2)\n",
    "plot_individual_state!(plotid,\"Idaho\",3)\n",
    "# plot_individual_state!(plotid,\"California\",4)\n",
    "ylabel!(\"prices\")\n",
    "pad_empty_plot(plotid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-stick",
   "metadata": {},
   "source": [
    "### üî¥Plot 4: Plots with double axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = rand(10)\n",
    "vector2 = rand(10)*100\n",
    "plot(vector1,label = \"b\",size=(300,200))\n",
    "plot!(twinx(), vector2,color=2,axis=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtickslabels = NYframe[!,:RegionName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = NYframe[!,:SizeRank]\n",
    "pc = NYframe[!,end]\n",
    "M = Matrix(NYframe[:,5:end])\n",
    "M = copy(M')\n",
    "md = find_percentile(M,0.9)\n",
    "\n",
    "md = find_percentile(M,0.5)\n",
    "mx = find_percentile(M,0.9)\n",
    "mn = find_percentile(M,0.1)\n",
    "vector1 = sz\n",
    "\n",
    "plot()\n",
    "plot!(md,ribbon =(md.-mn,mx.-md),color = 1,grid=false,label=\"\")\n",
    "\n",
    "plot!(xticks = (1:length(xtickslabels),xtickslabels),xrotation=90,xtickfont=font(10))\n",
    "plot!(twinx(), vector1,color=2,label=\"\",ylabel=\"rank\",grid=false,xticks=[],linewidth=2)\n",
    "plot!(Shape([],[]),color=1,label=\"Prices (left)\")\n",
    "p = plot!([],[],color=2,label=\"Rank (right)\")\n",
    "ep = plot(grid=false,legend=false,axis=false,framestyle = :box)#empty plot\n",
    "plot(p,ep,layout=@layout([a{0.85h};b{0.001h}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-steal",
   "metadata": {},
   "source": [
    "### üî¥Plot 5: High-dimensional data in a 2D plot\n",
    "We've seen a 3D plot in the Clustering notebook previously. I personally prefer 2D plots because they are often easier to read and is viewable in a print version more clearly. Here, we will explore how we can use color as a third dimension. Note that you can also use sizes as third dimension.\n",
    "\n",
    "We will use the California data, and plot the prices from 2010-02 on the x-axis and 2020-02 on the y-axis. We will then color code each data point by its current rank.\n",
    "\n",
    "Let's generate a quick scatter plot first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA202002 = CAframe[!,Symbol(\"2020-02\")]\n",
    "CA201002 = CAframe[!,Symbol(\"2010-02\")]\n",
    "scatter(CA201002,CA202002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA202002 = CAframe[!,Symbol(\"2020-02\")]\n",
    "CA201002 = CAframe[!,Symbol(\"2010-02\")]\n",
    "CAranks = CAframe[!,:SizeRank]\n",
    "scatter(CA201002,CA202002,legend=false,markerstrokewidth=0,markersize=3,alpha=0.6,grid=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"ColorSchemes\")\n",
    "using ColorSchemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the ranks to be between 0 and 1\n",
    "continuousranks = CAranks./maximum(CAranks)\n",
    "\n",
    "# create a placeholder vector that will store the color of each value\n",
    "colorsvec = Vector{RGB{Float64}}(undef,length(continuousranks))\n",
    "\n",
    "# and finally map the colors according to ColorSchemes.autumn1, there are many other schemes you can choose from\n",
    "map(i->colorsvec[i]=get(ColorSchemes.autumn1,continuousranks[i]),1:length(colorsvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuousdates = CAranks./maximum(CAranks)\n",
    "colorsvec = Vector{RGB{Float64}}(undef,length(continuousdates))\n",
    "map(i->colorsvec[i]=get(ColorSchemes.autumn1,continuousdates[i]),1:length(colorsvec))\n",
    "scatter(CA201002,CA202002,color=colorsvec,\n",
    "    legend=false,markerstrokewidth=0,markersize=3,grid=false)\n",
    "xlabel!(\"2010-02 prices\",xguidefontsize=10)\n",
    "ylabel!(\"2020-02 prices\",yguidefontsize=10)\n",
    "p1 = plot!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the plot canvas\n",
    "xvals = 0:100\n",
    "s = Shape([0,1,1,0],[0,0,1,1])\n",
    "plot(s,color=ColorSchemes.autumn1[1],grid=false,axis=false,\n",
    "    legend=false,linewidth=0,linecolor=nothing)\n",
    "\n",
    "for i = 2:101\n",
    "    s = Shape([xvals[i],xvals[i]+1,xvals[i]+1,xvals[i]],[0,0,1,1])\n",
    "    plot!(s,color=ColorSchemes.autumn1[i],grid=false,axis=false,\n",
    "    legend=false,linewidth=0,linecolor=nothing)\n",
    "end\n",
    "\n",
    "mynormalizer = maximum(CAranks)\n",
    "xtickslabels = 0:div(mynormalizer,10):mynormalizer\n",
    "continuousdates = xtickslabels./mynormalizer\n",
    "xticksloc = round.(Int,continuousdates.*101)\n",
    "\n",
    "# annotate using the ranks\n",
    "rotatedfont = font(10, \"Helvetica\",rotation=90)\n",
    "for i = 1:length(xtickslabels)\n",
    "    annotate!(xticksloc[i],0.5,text(xtickslabels[i], rotatedfont))\n",
    "end\n",
    "p2 = plot!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylayout = @layout([a{0.89h};b{0.1h}])\n",
    "plot(p1,p2,layout=mylayout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-leone",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- [ ] create violin plots in julia\n",
    "- [ ] create bar charts\n",
    "- [ ] add annotations to your plots\n",
    "- [ ] create an inset figure for your plot\n",
    "- [ ] create plots with error margin\n",
    "- [ ] create plots with double axes\n",
    "- [ ] create a new color mapping to a given set of values\n",
    "- [ ] create two dimensional plots and use color to indicate a third dimension\n",
    "- [ ] pad multiple plots together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-ceiling",
   "metadata": {},
   "source": [
    "# ü•≥ One cool finding\n",
    "\n",
    "Many interesting cool things here! The most interesting I found was that Idaho is following California's trend in housing prices, and Idaho's prices are growing faster than places like Indiana and Ohio.\n",
    "\n",
    "<img src=\"data/1201.png\" width=\"500\">\n",
    "<img src=\"data/1202.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-importance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0-beta1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
