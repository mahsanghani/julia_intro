{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reliable-momentum",
   "metadata": {},
   "source": [
    "# Intro to Flux.jl\n",
    "\n",
    "We have learned how machine learning allows us to classify data as apples or bananas with a single neuron. However, some of those details are pretty fiddly! Fortunately, Julia has a powerful package that does much of the heavy lifting for us, called [`Flux.jl`](https://fluxml.github.io/).\n",
    "\n",
    "*Using `Flux` will make classifying data and images much easier!*\n",
    "\n",
    "## Using `Flux.jl`\n",
    "\n",
    "In the next notebook, we are going to see how Flux allows us to redo the calculations from the previous notebook in a simpler way. We can get started with `Flux.jl` via:\n",
    "\n",
    "#### Helpful built-in functions\n",
    "\n",
    "When working we'll `Flux`, we'll make use of built-in functionality that we've had to create for ourselves in previous notebooks.\n",
    "\n",
    "For example, the sigmoid function, σ, that we have been using already lives within `Flux`:\n",
    "\n",
    "Importantly, `Flux` allows us to *automatically create neurons* with the **`Dense`** function. For example, in the last notebook, we were looking at a neuron with 2 inputs and 1 output:\n",
    " \n",
    " <img src=\"data/single-neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    " \n",
    "We could create a neuron with two inputs and one output via\n",
    " \n",
    "This `model` object comes with places to store weights and biases:\n",
    "\n",
    "Unlike in previous notebooks, note that `W` is no longer a `Vector` (1D `Array`) and `b` is no longer a number! Both are now stored in so-called `TrackedArray`s and `W` is effectively being treated as a matrix with a single row. We'll see why in the next notebook.\n",
    "\n",
    "Other helpful built-in functionality includes ways to automatically calculate gradients and also the cost function that we've used in previous notebooks - \n",
    "\n",
    "$$L(w, b) = \\sum_i \\left[y_i - f(x_i, w, b) \\right]^2$$\n",
    "\n",
    "This is the \"mean square error\" function, which in `Flux` is named **`Flux.mse`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-content",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Flux\")\n",
    "Pkg.add(\"TextParse\")\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Flux\n",
    "using TextParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-trouble",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "?σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-monaco",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Dense(2, 1, σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-somalia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-necklace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-asbestos",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "typeof(model.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-concord",
   "metadata": {},
   "source": [
    "# Learning with a single neuron using Flux.jl\n",
    "\n",
    "In this notebook, we'll use `Flux` to create a single neuron and teach it to learn, as we did by hand in notebook 10!\n",
    "\n",
    "### Read in data and process it\n",
    "\n",
    "Let's start by reading in our data\n",
    "\n",
    "and processing it to extract information about the red and green coloring in our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-mauritius",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "applecols, applecolnames = TextParse.csvread(\"data/Apple_Golden_1.dat\", '\\t')\n",
    "bananacols, bananacolnames = TextParse.csvread(\"data/bananas.dat\", '\\t')\n",
    "\n",
    "apples = DataFrame(Dict(strip(name)=>col for (name, col) in zip(applecolnames, applecols)))\n",
    "bananas = DataFrame(Dict(strip(name)=>col for (name, col) in zip(bananacolnames, bananacols)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-reducing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col1 = :red\n",
    "col2 = :green\n",
    "\n",
    "x_apples  = [ [apples[i, col1], apples[i, col2]] for i in 1:size(apples)[1] ]\n",
    "x_bananas = [ [bananas[i, col1], bananas[i, col2]] for i in 1:size(bananas)[1] ]\n",
    "\n",
    "xs = vcat(x_apples, x_bananas)\n",
    "\n",
    "ys = vcat( zeros(size(x_apples)[1]), ones(size(x_bananas)[1]) );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-financing",
   "metadata": {},
   "source": [
    "The input data is in `xs` and the labels (true classifications as bananas or apples) in `ys`.\n",
    "\n",
    "### Using `Flux.jl`\n",
    "\n",
    "Now we can load `Flux` to really get going!\n",
    "\n",
    "We saw in the last notebook that σ is a built-in function in `Flux`.\n",
    "\n",
    "Another function that is used a lot in neural networks is called `ReLU`; in Julia, the function is called `relu`.\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Use the docs to discover what `ReLU` is all about.\n",
    "\n",
    "`relu.([-3, 3])` returns\n",
    "\n",
    "A) [-3, 3] <br>\n",
    "B) [0, 3] <br>\n",
    "C) [0, 0] <br>\n",
    "D) [3, 3] <br>\n",
    "\n",
    "### Making a single neuron in Flux\n",
    "\n",
    "Let's use `Flux` to build our neuron with 2 inputs and 1 output:\n",
    "\n",
    " <img src=\"data/single-neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    " \n",
    "We previously put the two weights in a vector, $\\mathbf{w}$. Flux instead puts weights in a $1 \\times 2$ matrix (i.e. a matrix with 1 *row* and 2 *columns*). \n",
    "\n",
    "Previously, to compute the dot product of $\\mathbf{w}$ and $\\mathbf{x}$ we had to use either the `dot` function, or we had to transpose the vector $\\mathbf{w}$:\n",
    "\n",
    "```julia\n",
    "# transpose w\n",
    "b = w' * x\n",
    "# or use dot!\n",
    "b = dot(w, x)\n",
    "```\n",
    "If the weights are instead stored in a $1 \\times 2$ matrix, `W`, then we can simply multiply `W` and `x` together instead!\n",
    "\n",
    "We start off with random values for our parameters and data:\n",
    "\n",
    "Note that the product of `W` and `x` will now be an array (vector) with a single element, rather than a single number:\n",
    "\n",
    "This means that our bias `b` is treated as an array when we're using `Flux`:\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Write a function `mypredict` that will take a single input, array `x` and use `W`, `b`, and built-in `σ` to generate an output prediction (stored as an array). This function defines our neural network!\n",
    "\n",
    "Hint: This function will look very similar to $f_{\\mathbf{w},\\mathbf{b}}$ from the last notebook but has changed since our data structures to store our parameters have changed!\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Define a loss function called `loss`.\n",
    "\n",
    "`loss` should take two inputs: a vector storing data, `x`, and a vector storing the correct \"labels\" for that data. `loss` should return the sum of the squares of differences between the predictions and the correct labels.\n",
    "\n",
    "## Calculating gradients using Flux: backpropagation\n",
    "\n",
    "For learning, we know that what we need is a way to calculate derivatives of the `loss` function with respect to the parameters `W` and `b`. So far, we have been doing that using finite differences. \n",
    "\n",
    "`Flux.jl` instead implements a numerical method called **backpropagation** that calculates gradients (essentially) exactly, in an automatic way, by indirectly applying the rules of calculus.\n",
    "To do so, it provides a new type of object called \"tracked\" arrays. These are arrays that store not only their current value, but also information about gradients, which is used by the backpropagation method.\n",
    "\n",
    "[If you want to understand the maths behind backpropagation, we recommend e.g. [this lecture](https://www.youtube.com/watch?v=i94OvYb6noo).]\n",
    "\n",
    "To do so, `Flux` provides a function `param` to define such objects that will contain the information for a *param*eter.\n",
    "\n",
    "Let's start, as usual, by setting up some random initial values for the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-approach",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = rand(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-negative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-hazard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_data = rand(1, 2)  \n",
    "b_data = rand(1)\n",
    "\n",
    "W_data, b_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-berkeley",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = Flux.params(W_data)\n",
    "b = Flux.params(b_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-pierre",
   "metadata": {},
   "source": [
    "Here, `params` is a function that `Flux` provides to create an object that represents a parameter of a machine learning model, i.e. an object which has both a value and derivative information, and such that other objects know how to *keep track* of when it is used in an expression.\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "What type does `W` have?\n",
    "\n",
    "A) Array (1D) <br>\n",
    "B) Array (2D) <br>\n",
    "C) TrackedArray (1D) <br>\n",
    "D) TrackedArray (2D) <br>\n",
    "E) Parameter (1D) <br>\n",
    "F) Parameter (2D) <br>\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "`W` stores not only its current value, but also has space to store gradient information. You can access the values and gradient of the weights as follows:\n",
    "\n",
    "```julia\n",
    "W.data\n",
    "W.grad\n",
    "```\n",
    "\n",
    "At this point, are the values of the weights or the gradient of the weights larger?\n",
    "\n",
    "A) the values of the weights <br>\n",
    "B) the gradient of the weights\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "For data `x` and `y` where\n",
    "\n",
    "```julia\n",
    "x, y = [0.413759, 0.692204], [0.845677]\n",
    "```\n",
    "apply the loss function to `x` and `y` to give a new variable `l`. What is the type of `l`? (How many dimensions does it have?)\n",
    "\n",
    "A) Array (0D) <br>\n",
    "B) Array (1D) <br>\n",
    "C) TrackedArray (0D) <br>\n",
    "D) TrackedArray (1D)<br> \n",
    "E) Float64<br>\n",
    "F) Int64<br>\n",
    "\n",
    "### Stochastic gradient descent\n",
    "\n",
    "We can now use these features to reimplement stochastic gradient descent, following the method we used in the previous notebook, but now using backpropagation!\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "Modify the code from the previous notebook for stochastic gradient descent to use Flux instead.\n",
    "\n",
    "### Investigating stochastic gradient descent\n",
    "\n",
    "Let's look at the values stored in `b` before we run stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-resort",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-mining",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = [0.413759, 0.692204], [0.845677]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-smile",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_final, b_final = Descent(loss, W, b, xs, ys, 100000)\n",
    "\n",
    "W_final\n",
    "b_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-infrastructure",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "\n",
    "Plot the data and the learned function.\n",
    "    \n",
    "#### Exercise 9\n",
    "\n",
    "Do this plot every so often as the learning process is proceeding in order to have an animation of the process.\n",
    "\n",
    "### Automation with Flux.jl\n",
    "\n",
    "We will need to repeat the above process for a lot of different systems.\n",
    "Fortunately, `Flux.jl` provides us with tools to automate this!\n",
    "\n",
    "Flux allows to create a neuron in a simple way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-sucking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "model = Dense(2,1,σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-cloud",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "typeof(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-clarity",
   "metadata": {},
   "source": [
    "The `2` and `1` refer to the number of inputs and outputs, and the neuron is defined using the $\\sigma$ function.\n",
    "\n",
    "We have made an object of type `Dense`, defined by `Flux`, with the name `model`. This represents a \"dense neural network layer\" (see later for more on neural network layers).\n",
    "The parameters that will be modified during the learning process live *inside* the `model` object.\n",
    "\n",
    "#### Exercise 10\n",
    "\n",
    "Investigate which variables live inside the `model` object and what type they are. How does that compare to the call to create the `Dense` object that we started with?\n",
    "\n",
    "### Model object as a function\n",
    "\n",
    "We can apply the `model` object to data just as if it were a standard function:\n",
    "\n",
    "#### Exercise 11\n",
    "\n",
    "Prove to yourself that you understand what is going on when we call `model`. Create two arrays `W` and `b` with the same elements as `model.W` and `model.b`. Use `W` and `b` to generate the same answer that you get when we call `model([.5, .5])`.\n",
    "\n",
    "### Using Flux\n",
    "\n",
    "We now need to provide Flux with three pieces of information: \n",
    "\n",
    "1. A loss function\n",
    "2. Some training data\n",
    "3. An optimization method\n",
    "\n",
    "### Loss functions\n",
    "\n",
    "Flux has various loss functions built in, for example the mean-squared error (`mse`) that we have been using:\n",
    "\n",
    "Another common one is the cross entropy, `Flux.crossentropy`.\n",
    "\n",
    "### Data\n",
    "\n",
    "The data can take a couple of different forms. \n",
    "One form is a single **iterator**, consisting of pairs $(x, y)$ of data and labels. We can achieve this with `zip`.\n",
    "\n",
    "#### Exercise 12\n",
    "\n",
    "Use `zip` to \"zip together\" `xs` and `ys`. Then use the `collect` function to check what `zip` actually does.\n",
    "\n",
    "### Optimization routine\n",
    "\n",
    "Now we need to tell Flux what kind of optimization routine to use. It has several built in; the standard stochastic gradient descent algorithm that we have been using is called `SGD`. We must pass it two things: a list of parameter objects which will be modified by the optimization routine, and a step size:\n",
    "\n",
    "The gradient calculations and parameter updates will be carried out by this optimizer function; we do not see those details, but if you are curious, you can, of course, look at the `Flux.jl` source code!\n",
    "\n",
    "### Training\n",
    "\n",
    "We now have all the pieces in place to actually **train** our model (a single neuron) on the data. \n",
    "\"Training\" refers to using pre-labeled data to learn the function that relates the input data to the desired output data given by the labels.\n",
    "\n",
    "`Flux` provides the function `train!`, which performs a single pass through the data and does a single step of optimization using the partial cost function for each data point:\n",
    "\n",
    "We can then just repeat this several times to train the network more and coax it towards the minimum of the cost function:\n",
    "\n",
    "Now let's look at the parameters after training:\n",
    "\n",
    "Instead of writing out a list of parameters to modify, `Flux` provides the function `params`, which extracts all available parameters from a model:\n",
    "\n",
    "## Adding more features\n",
    "\n",
    "#### Exercise 13\n",
    "\n",
    "So far we have just used two features, red and green. \n",
    "\n",
    "(i) Add a third feature, blue. Plot the new data.\n",
    "\n",
    "(ii) Train a neuron with 3 inputs and 1 output on the data.\n",
    "\n",
    "(iii) Can you find a good way to visualize the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-palestinian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(rand(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-contact",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss(x,y) = Flux.mse(model(x),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-beijing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = Flux.Descent([model.W, model.b], 0.01)\n",
    "# give a list of the parameters that will be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-reproduction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Flux.train!(loss,data,opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-martial",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in 1:100\n",
    "    Flux.train!(loss, data, opt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-workplace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-pregnancy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-appreciation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = Descent(params(model), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-scoop",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-weight",
   "metadata": {},
   "source": [
    "## Adding more features\n",
    "\n",
    "#### Exercise 13\n",
    "\n",
    "So far we have just used two features, red and green. \n",
    "\n",
    "(i) Add a third feature, blue. Plot the new data.\n",
    "\n",
    "(ii) Train a neuron with 3 inputs and 1 output on the data.\n",
    "\n",
    "(iii) Can you find a good way to visualize the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-toddler",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "Now that we know what neurons are, we are ready for the final step: the neural network!. A neural network is literally made out of a network of neurons that are connected together. \n",
    "\n",
    "So far, we have just looked at single neurons, that only have a single output.\n",
    "What if we want multiple outputs?\n",
    "\n",
    "\n",
    "### Multiple output models\n",
    "\n",
    "What if we wanted to distinguish between apples, bananas, *and* grapes? We could use *vectors* of `0` or `1` values to symbolize each output.\n",
    "\n",
    "<img src=\"data/fruit-salad.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "The idea of using vectors is that different directions in the space of outputs encode information about different types of inputs.\n",
    "\n",
    "Now we extend our previous model to give multiple outputs by repeating it with different weights. For the first element of the array we'd use:\n",
    "\n",
    "$$\\sigma(x;w^{(1)},b^{(1)}) := \\frac{1}{1 + \\exp(-w^{(1)} \\cdot x + b^{(1)})};$$\n",
    "\n",
    "then for the second we'd use\n",
    "\n",
    "$$\\sigma(x;w^{(2)},b^{(2)}) := \\frac{1}{1 + \\exp(-w^{(2)} \\cdot x + b^{(2)})};$$\n",
    "\n",
    "and if you wanted $n$ outputs, you'd have for each one\n",
    "\n",
    "$$\\sigma(x;w^{(i)},b^{(i)}) := \\frac{1}{1 + \\exp(-w^{(i)} \\cdot x + b^{(i)})}.$$\n",
    "\n",
    "Notice that these equations are all the same, except for the parameters, so we can write this model more succinctly, as follows. Let's write $b$ in an array:\n",
    "\n",
    "$$b=\\left[\\begin{array}{c}\n",
    "b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "b_{n}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "and put our array of weights as a matrix:\n",
    "\n",
    "$$ \\mathsf{W}=\\left[\\begin{array}{c}\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\end{array}\\begin{array}{cccc}\n",
    "w_{1}^{(1)} & w_{2}^{(1)} & \\ldots & w_{n}^{(1)}\\\\\n",
    "w_{1}^{(2)} & w_{2}^{(2)} & \\ldots & w_{n}^{(2)}\\\\\n",
    "\\vdots & \\vdots &  & \\vdots\\\\\n",
    "w_{1}^{(n)} & w_{2}^{(n)} & \\ldots & w_{n}^{(n)}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "We can write this all in one line as:\n",
    "\n",
    "$$\\sigma(x;w,b)= \\left[\\begin{array}{c}\n",
    "\\sigma^{(1)}\\\\\n",
    "\\sigma^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "\\sigma^{(n)}\n",
    "\\end{array}\\right] = \\frac{1}{1 + \\exp(-\\mathsf{W} x + b)}$$\n",
    "\n",
    "$\\mathsf{W} x$ is the operation called \"matrix multiplication\"\n",
    "\n",
    "[Show small matrix multiplication]\n",
    "\n",
    "It takes each column of weights and does the dot product against $x$ (remember, that's how $\\sigma^{(i)}$ was defined) and spits out a vector from doing that with each column. The result is a vector, which makes this version of the function give a vector of outputs which we can use to encode larger set of choices. \n",
    "\n",
    "Matrix multiplication is also interesting since **GPUs (Graphics Processing Units, i.e. graphics cards) are basically just matrix multiplication machines**, which means that by writing the equation this way, the result can be calculated really fast.\n",
    "\n",
    "This \"multiple input and multiple output\" version of the sigmoid function is known as a *layer of neurons*.\n",
    "\n",
    "Previously we worked with a single neuron, which we visualized as\n",
    "\n",
    "<img src=\"data/single-neuron.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "where we have two pieces of data (green) coming into a single neuron (pink) that returned a single output. We could use this single output to do binary classification - to identify an image of a fruit as `1`, meaning banana or as `0`, meaning not a banana (or an apple).\n",
    "\n",
    "To do non-binary classification, we can use a layer of neurons, which we can visualize as\n",
    "\n",
    "<img src=\"data/single-layer.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "We now have stacked a bunch of neurons on top of each other to hopefully work together and train to output results of more complicated features. \n",
    "\n",
    "We still have two input pieces of data, but now have several neurons, each of which produces an output for a given binary classification: \n",
    "* neuron 1: \"is it an apple?\"\n",
    "* neuron 2: \"is it a banana?\"\n",
    "* neuron 3: \"is it a grape?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-routine",
   "metadata": {},
   "source": [
    "# Building a single neural network layer using `Flux.jl`\n",
    "\n",
    "In this notebook, we'll move beyond binary classification. We'll try to distinguish between three fruits now, instead of two. We'll do this using **multiple** neurons arranged in a **single layer**.\n",
    "\n",
    "## Read in and process data\n",
    "\n",
    "We can start by loading the necessary packages and getting our data into working order with similar code we used at the beginning of the previous notebooks, except that now we will combine three different apple data sets, and will add in some grapes to the fruit salad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load apple data in with `readdlm` for each file\n",
    "apples1, applecolnames1 = readdlm(\"data/Apple_Golden_1.dat\", '\\t', header = true)\n",
    "apples2, applecolnames2 = readdlm(\"data/Apple_Golden_2.dat\", '\\t', header = true)\n",
    "apples3, applecolnames3 = readdlm(\"data/Apple_Golden_3.dat\", '\\t', header = true)\n",
    "\n",
    "# Check that the column names are the same for each apple file\n",
    "println( applecolnames1 == applecolnames2 == applecolnames3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-station",
   "metadata": {},
   "source": [
    "Since each apple file has columns with the same headers, we know we can concatenate these columns from the different files together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "apples = vcat(apples1, apples2, apples3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-fairy",
   "metadata": {},
   "source": [
    "And now let's build an array called `x_apples` that stores data from the `red` and `blue` columns of `apples`. From `applecolnames1`, we can see that these are the 3rd and 5th columns of `apples`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "applecolnames1[3], applecolnames1[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "length(apples[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_apples  = [ [apples[i, 3], apples[i, 5]] for i in 1:length(apples[:, 3]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly, let's create arrays called x_bananas and x_grapes\n",
    "# Load data from *.dat files\n",
    "bananas, bananacolnames = readdlm(\"data/Banana.dat\", '\\t', header = true)\n",
    "grapes1, grapecolnames1 = readdlm(\"data/Grape_White.dat\", '\\t', header = true)\n",
    "grapes2, grapecolnames2 = readdlm(\"data/Grape_White_2.dat\", '\\t', header = true)\n",
    "\n",
    "# Concatenate data from two grape files together\n",
    "grapes = vcat(grapes1, grapes2)\n",
    "\n",
    "# Check that column 3 and column 5 refer to the \"red\" and \"blue\" columns from each file\n",
    "println(\"All column headers are the same: \", bananacolnames == grapecolnames1 == grapecolnames2 == applecolnames1)\n",
    "\n",
    "# Build x_bananas and x_grapes from bananas and grapes\n",
    "x_bananas  = [ [bananas[i, 3], bananas[i, 5]] for i in 1:length(bananas[:, 3]) ]\n",
    "x_grapes = [ [grapes[i, 3], grapes[i, 5]] for i in 1:length(grapes[:, 3]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-eagle",
   "metadata": {},
   "source": [
    "## One-hot vectors\n",
    "\n",
    "Now we wish to classify *three* different types of fruit. It is not clear how to encode these three types using a single output variable; indeed, in general this is not possible.\n",
    "\n",
    "Instead, we have the idea of encoding $n$ output types from the classification into *vectors of length $n$*, called \"one-hot vectors\":\n",
    "\n",
    "$$\n",
    "\\textrm{apple} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix};\n",
    "\\quad\n",
    "\\textrm{banana} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix};\n",
    "\\quad\n",
    "\\textrm{grape} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The term \"one-hot\" refers to the fact that each vector has a single $1$, and is $0$ otherwise.\n",
    "\n",
    "Effectively, the first neuron will learn whether or not (1 or 0) the data corresponds to an apple, the second whether or not (1 or 0) it corresponds to a banana, etc.\n",
    "\n",
    "`Flux.jl` provides an efficient representation for one-hot vectors, using advanced features of Julia so that it does not actually store these vectors, which would be a waste of memory; instead `Flux` just records in which position the non-zero element is. To us, however, it looks like all the information is being stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: onehot\n",
    "\n",
    "onehot(1, 1:3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-minute",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Make an array `labels` that gives the labels (1, 2 or 3) of each data point. Then use `onehot` to encode the information about the labels as a vector of `OneHotVector`s.\n",
    "\n",
    "## Single layer in Flux\n",
    "\n",
    "Let's suppose that there are two pieces of input data, as in the previous single neuron notebook. Then the network has 2 inputs and 3 outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"draw_neural_net.jl\")\n",
    "draw_network([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(2,3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-reflection",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Now what do the weights inside `model` look like? How does this compare to the diagram of the network layer above?\n",
    "\n",
    "## Training the model\n",
    "\n",
    "Despite the fact that the model is now more complicated than the single neuron from the previous notebook, the beauty of `Flux.jl` is that the rest of the training process **looks exactly the same**!\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Implement training for this model.\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "Visualize the result of the learning for each neuron. Since each neuron is sigmoidal, we can get a good idea of the function by just plotting a single contour level where the function takes the value 0.5, using the `contour` function with keyword argument `levels=[0.5, 0.501]`.\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Interpret the results by checking which fruit each neuron was supposed to learn and what it managed to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()\n",
    "\n",
    "contour!(0:0.01:1, 0:0.01:1, (x,y)->model([x,y]).data[1], levels=[0.5, 0.501], color = cgrad([:blue, :blue]))\n",
    "contour!(0:0.01:1, 0:0.01:1, (x,y)->model([x,y]).data[2], levels=[0.5,0.501], color = cgrad([:green, :green]))\n",
    "contour!(0:0.01:1, 0:0.01:1, (x,y)->model([x,y]).data[3], levels=[0.5,0.501], color = cgrad([:red, :red]))\n",
    "\n",
    "scatter!(first.(x_apples), last.(x_apples), m=:cross, label=\"apples\")\n",
    "scatter!(first.(x_bananas), last.(x_bananas), m=:circle, label=\"bananas\")\n",
    "scatter!(first.(x_grapes), last.(x_grapes), m=:square, label=\"grapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-poison",
   "metadata": {},
   "source": [
    "## Going deep: Deep neural networks\n",
    "\n",
    "So far, we've learned that if we want to classify more than two fruits, we'll need to go beyond using a single neuron and use *multiple* neurons to get multiple outputs. We can think of stacking these multiple neurons together in a single neural layer.\n",
    "\n",
    "Even so, we found that using a single neural layer was not enough to fully distinguish between bananas, grapes, **and** apples. To do this properly, we'll need to add more complexity to our model. We need not just a neural network, but a *deep neural network*. \n",
    "\n",
    "There is one step remaining to build a deep neural network. We have been saying that a neural network takes in data and then spits out `0` or `1` predictions that together declare what kind of fruit the picture is. However, what if we instead put the output of one neural network layer into another neural network layer?\n",
    "\n",
    "This gets pictured like this below:\n",
    "\n",
    "<img src=\"data/deep-neural-net.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "On the left we have 3 data points in blue. Those 3 data points each get fed into 4 neurons in purple. Each of those 4 neurons produces a single output, but those output are each fed into three neurons (the second layer of purple). Each of those 3 neurons spits out a single value, and those values are fed as inputs into the last layer of 6 neurons. The 6 values that those final neurons produce are the output of the neural network. This is a deep neural network.\n",
    "\n",
    "### Why would a deep neural network be better?\n",
    "\n",
    "This is a little perplexing when you first see it. We used neurons to train the model before: why would sticking the output from neurons into other neurons help us fit the data better? The answer can be understood by drawing pictures. Geometrically, the matrix multiplication inside of a layer of neurons is streching and rotating the axis that we can vary:\n",
    "\n",
    "[Show linear transformation of axis, with data]\n",
    "\n",
    "A nonlinear transformation, such as the sigmoid function, then adds a bump to the line:\n",
    "\n",
    "[Show the linear transformed axis with data, and then a bumped version that fits the data better]\n",
    "\n",
    "Now let's repeat this process. When we send the data through another layer of neurons, we get another rotation and another bump:\n",
    "\n",
    "[Show another rotation, then another bump]\n",
    "\n",
    "Visually, we see that if we keep doing this process we can make the axis line up with any data. What this means is that **if we have enough layers, then our neural network can approximate any model**. \n",
    "\n",
    "The trade-off is that with more layers we have more parameters, so it may be harder (i.e. computationally intensive) to train the neural network. But we have the guarantee that the model has enough freedom such that there are parameters that will give the correct output. \n",
    "\n",
    "Because this model is so flexible, the problem is reduced to that of learning: do the same gradient descent method on this much larger model (but more efficiently!) and we can make it classify our data correctly. This is the power of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-heart",
   "metadata": {},
   "source": [
    "# Multiple neural network layers with `Flux.jl`\n",
    "\n",
    "In a previous notebook, we saw that one layer of neurons wasn't enough to distinguish between three types of fruit (apples, bananas *and* grapes), since the data is quite complex. To solve this problem, we need to use more layers, so heading into the territory of **deep learning**!\n",
    "\n",
    "By adding another layer between the inputs and the output neurons, a so-called \"hidden layer\", we will get our first serious **neural network**, looking something like this:\n",
    "\n",
    "We will continue to use two input data and try to classify into three types, so we will have three output neurons. We have chosen to add a single \"hidden layer\" in between, and have arbitrarily chosen to put 4 neurons there.\n",
    "\n",
    "Much of the *art* of deep learning is choosing a suitable structure for the neural network that will allow the model to be sufficiently complex to model the data well, but sufficiently simple to allow the parameters to be learned in a reasonable length of time.\n",
    "\n",
    "## Read in and process data\n",
    "\n",
    "As before, let's load some pre-processed data using code we've seen in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"draw_neural_net.jl\")\n",
    "draw_network([2, 4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using Flux\n",
    "using Flux: onehot\n",
    "\n",
    "apples_1 = CSV.read(\"data/Apple_Golden_1.dat\",DataFrame, delim='\\t')\n",
    "apples_2 = CSV.read(\"data/Apple_Golden_2.dat\",DataFrame, delim='\\t')\n",
    "apples_3 = CSV.read(\"data/Apple_Golden_3.dat\",DataFrame, delim='\\t')\n",
    "bananas = CSV.read(\"data/Banana.dat\", delim='\\t')\n",
    "grapes_1 = CSV.read(\"data/Grape_White.dat\", delim='\\t')\n",
    "grapes_2 = CSV.read(\"data/Grape_White_2.dat\", delim='\\t');\n",
    "\n",
    "apples = vcat(apples_1, apples_2, apples_3)\n",
    "grapes = vcat(grapes_1, grapes_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = :red\n",
    "col2 = :blue\n",
    "\n",
    "x_apples  = [ [apples_1[i, col1], apples_1[i, col2]] for i in 1:size(apples_1)[1] ]\n",
    "append!(x_apples, [ [apples_2[i, col1], apples_2[i, col2]] for i in 1:size(apples_2)[1] ])\n",
    "append!(x_apples, [ [apples_3[i, col1], apples_3[i, col2]] for i in 1:size(apples_3)[1] ])\n",
    "\n",
    "x_bananas = [ [bananas[i, col1], bananas[i, col2]] for i in 1:size(bananas)[1] ]\n",
    "\n",
    "x_grapes = [ [grapes_1[i, col1], grapes_1[i, col2]] for i in 1:size(grapes_1)[1] ]\n",
    "append!(x_grapes, [ [grapes_2[i, col1], grapes_2[i, col2]] for i in 1:size(grapes_2)[1] ])\n",
    "\n",
    "xs = vcat(x_apples, x_bananas, x_grapes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ones(length(x_apples)); 2*ones(length(x_bananas)); 3*ones(length(x_grapes))];\n",
    "\n",
    "ys = [onehot(label, 1:3) for label in labels];  # onehotbatch(labels, 1:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 2\n",
    "hidden = 4\n",
    "outputs = 3\n",
    "\n",
    "layer1 = Dense(inputs, hidden, σ)\n",
    "layer2 = Dense(hidden, outputs, σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(layer1, layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-extent",
   "metadata": {},
   "source": [
    "We now we wish to classify the three types of fruit, so we again use one-hot vectors to represent the desired outputs $y^{(i)}$:\n",
    "\n",
    "The input data is in `xs` and the one-hot vectors are in `ys`.\n",
    "\n",
    "## Multiple layers in Flux\n",
    "\n",
    "Let's tell Flux what structure we want the network to have. We first specify the number of neurons in each layer, and then construct each layer as a `Dense` layer:\n",
    "\n",
    "To stitch together multiple layers to make a multi-layer network, we use Flux's `Chain` function:\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "What is the internal structure and sub-structure of this `model` object?\n",
    "\n",
    "## Training the model\n",
    "\n",
    "We have now set up a model and we have some training data.\n",
    "How do we train the model on the data?\n",
    "    \n",
    "The amazing thing is that the rest of the code in `Flux` is **exactly the same as before**. This is possible thanks to the design of Julia itself, and of the `Flux` package.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Train the model as before, now using the popular `ADAM` optimizer. You may need to train the network for longer than before, since we have many more parameters.\n",
    "\n",
    "## Visualizing the results\n",
    "\n",
    "What does this neural network represent? It is simply a more complicated function with two inputs and three outputs, i.e. a function $f: \\mathbb{R}^2 \\to \\mathbb{R}^3$. \n",
    "Before, with a single layer, each component of the function $f$ basically corresponded to a hyperplane; now it will instead be a **more complicated nonlinear function** of the input data!\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Visualize each component of the output separately as a heatmap and/or contours superimposed on the data. Interpret the results.\n",
    "\n",
    "## What we have learned\n",
    "\n",
    "Adding an intermediate layer allows the network to start to deform the separating surfaces that it is learning into more complicated, nonlinear (curved) shapes. This allows it to separate data that were previously unable to be separated!\n",
    "\n",
    "However, using only two features means that data from different classes overlaps. To distinguish it we would need to use more features.\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Use three features (red, green and blue) and build a network with one hidden layer. Does this help to distinguish the data better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-married",
   "metadata": {},
   "source": [
    "# Learning to recognize handwritten digits using a neural network\n",
    "\n",
    "We have now reached the point where we can tackle a very interesting task: applying the knowledge we have gained with machine learning in general, and `Flux.jl` in particular, to create a neural network that can recognize handwritten digits! The data are from a data set called MNIST, which has become a classic in the machine learning world.\n",
    "\n",
    "[We could also try to apply the techniques to the original images of fruit instead. However, the fruit images are much larger than the MNIST images, which makes the learning a suitable neural network too slow.]\n",
    "\n",
    "## Data munging\n",
    "\n",
    "As we know, the first difficulty with any new data set is locating it, understanding what format it is stored in, reading it in and decoding it into a useful data structure in Julia.\n",
    "\n",
    "The original MNIST data is available [here](http://yann.lecun.com/exdb/mnist); see also the [Wikipedia page](https://en.wikipedia.org/wiki/MNIST_database). However, the format that the data is stored in is rather obscure.\n",
    "\n",
    "Fortunately, various packages in Julia provide nicer interfaces to access it. We will use the one provided by `Flux.jl`.\n",
    "\n",
    "The data are images of handwritten digits, and the corresponding labels that were determined by hand (i.e. by humans). Our job will be to get the computer to **learn** to recognize digits by learning, as usual, the function that relates the input and output data.\n",
    "\n",
    "### Loading and examining the data\n",
    "\n",
    "First we load the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Flux.Data.MNIST\n",
    "\n",
    "labels = MNIST.labels();\n",
    "images = MNIST.images();  \n",
    "# the semi-colon (`;`) here is important: \n",
    "# it prevents Julia from displaying the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-background",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Examine the `labels` data. Then examine the first few images. *Do not try to view the whole of the `images` object!* Try to drill down to discover how the data is laid out.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Convert the first image to a matrix of `Float64`.\n",
    "\n",
    "### Munging the data\n",
    "\n",
    "In the previous notebooks, we arranged the input data for Flux as a `Vector` of `Vector`s.\n",
    "Now we will use an alternative arrangement, as a matrix, since that allows `Flux` to use matrix operations, which are more efficient.\n",
    "\n",
    "The column $i$ of the matrix is a vector consisting of the $i$th data point $\\mathbf{x}^{(i)}$.  Similarly, the desired outputs are given as a matrix, with the $i$th column being the desired output $\\mathbf{y}^{(i)}$.\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "An image is a matrix of colours, but now we need a vector instead. To do so, we just arrange all of the elements of the matrix in a certain way into a single list; fortunately, Julia already provides the function `vec` to do so!\n",
    "\n",
    "1. Which order does `vec` use? [This reflects the underlying way in which the matrix is stored in memory.]\n",
    "\n",
    "2. How can you convert an image into a `Vector` of `Float64`?\n",
    "\n",
    "3. Define a variable $n$ that is the length of these vectors.\n",
    "\n",
    "#### Exercise 4\n",
    "Make a function `rewrite` that accepts a range and converts that range of images to floating-point vectors and stacks them horizontally using `hcat` and the \"splat\" operator `...`. \n",
    "\n",
    "We also want a matrix of one-hot vectors. `Flux` provides a function `onehotbatch` to do this (you will need to import it). It works like `onehot`, but takes in a vector of labels and outputs a matrix `Y`.\n",
    "\n",
    "Return the pair `(X, Y)`.\n",
    "\n",
    "## Setting up the neural network\n",
    "\n",
    "Now we must set up a neural network. Since the data is complicated, we may expect to need several layers.\n",
    "But we can start with a single layer.\n",
    "\n",
    "- The network will take as inputs the vectors $\\mathbf{x}^{(i)}$, so the input layer has $n$ nodes. \n",
    "\n",
    "- The output will be a one-hot vector encoding the digit from 1 to 9 or 0 that is desired. There are 10 possible categories, so we need an output layer of size 10. \n",
    "\n",
    "It is then our task as neural network designers to insert layers between these input and output layers, whose weights will be tuned during the learning process. *This is an art, not a science*! But major advances have come from finding a good structure for the network.\n",
    "\n",
    "### Softmax\n",
    "\n",
    "We will make a network with a single layer; let's choose each neuron in the layer to use the `relu` activation function. \n",
    "The output `relu` can be arbitrarily large, but in the end we will wish to compare the network's output with one-hot vectors, i.e. values between $0$ and $1$.\n",
    "\n",
    "In order to make this work, we will thus use an extra function at the end that takes in a vector of arbitrary real numbers and maps it (\"squashes it down\") to a vector of numbers between $0$ and $1$.\n",
    "\n",
    "The most used function with this property is $\\mathrm{softmax}$. Firstly we take the exponential of each input variable to make them positive. Then we divide by the sum to make sure they lie between $0$ and $1$.\n",
    "\n",
    "$$\\mathrm{softmax}(\\mathbf{x})_i := \\frac{\\exp (x_i)}{\\sum_j \\exp(x_j)}$$\n",
    "\n",
    "Note that here we have written the result for the $i$th component of the function $\\mathbf{R}^n \\to \\mathbf{R}^n$. Note also that the function returns a vector of numbers that are positive, and whose components sum to $1$. Thus, in fact, they can be thought of as probabilities.\n",
    "\n",
    "In the neural network context, using a `softmax` after the final layer thus allows us to interpret the outputs as probabilities, in our case the probability that the network assigns that a given image represents each possible output value ($0$-$9$)!\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Make a neural network with one single layer, using the function $\\sigma$, and a final `softmax`.\n",
    "\n",
    "## Training\n",
    "\n",
    "As we know, **training** consists of iteratively adjusting the model's parameters to decrease the `loss` function. Which parameters need to be adjusted? All of them!\n",
    "\n",
    "Since the `loss` function contains a call to the `model` function, calling `back!` on the result of the loss function updates the information about the gradient of the loss function with respect to *every node in the network!*:\n",
    "\n",
    "This is what is going on inside the `train!` function. \n",
    "In fact, `train!(loss, data, opt)` iterates over each object in `data` and runs this function.\n",
    "For this reason, `data` must consist of an iterable object that returns pairs `(X, Y)` at each step.\n",
    "\n",
    "Alternatively, we can make one call to the `train!` function iterate over several copies of `data`, using `repeated`. This is an **iterator**; it does not copy the data 100 times, which would be very wasteful; it just gives an object that repeatedly loops over the same data:\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Train the model on a subset of $N$ images with $N = 5000$.\n",
    "\n",
    "This is (approximately) equivalent to just doing a `for` loop to run the previous `train!` command 100 times.\n",
    "\n",
    "### Using callbacks\n",
    "\n",
    "The `train!` function can take an optional keyword argument, `cb` (short for \"*c*all*b*ack\"). A callback function is a function that you provide as an argument to a function `f`, which \"calls back\" your function every so often.\n",
    "\n",
    "This provides the possibility to provide a function that is called at each step or every so often during the training process.\n",
    "A common use case is to provide a visual trace of the training process by printing out the current value of the `loss` function:\n",
    "\n",
    "However, it is expensive to calculate the complete `loss` function and it is not necessary to output it every step. So `Flux` also provides a function `throttle`, that provides a mechanism to call a given function at most once every certain number of seconds:\n",
    "\n",
    "## Testing phase\n",
    "\n",
    "We now have trained a model, i.e. we have found the parameters `W` and `b` for the network layer(s). In order to **test** if the learning procedure was really successful, we check how well the resulting trained network performs when we test it with images that the network has not yet seen! \n",
    "\n",
    "Often, a dataset is split up into \"training data\" and \"test (or validation) data\" for this purpose, and indeed the MNIST data set has a separate pool of training data. We can instead use the images that we have not included in our reduced training process.\n",
    "\n",
    "#### Exercise 8\n",
    "\n",
    "Use the `indmax` function to write a function `prediction` that reports which digit `model` predicts, as the index with the maximum weight.\n",
    "\n",
    "#### Exercise 9\n",
    "\n",
    "Count the number of correct predictions over the whole data set, and hence the percentage of images that are correctly predicted. [This percentage is what is used to compare different machine learning techniques.]\n",
    "\n",
    "## Improving the prediction\n",
    "\n",
    "So far we have used a single layer. In order to improve the prediction, we probably need to use more layers.\n",
    "\n",
    "#### Exercise 10\n",
    "\n",
    "Introduce an intermediate, hidden layer. Does it give a better prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loss(X, Y)\n",
    "\n",
    "Flux.Tracker.back!(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ((X, Y), )  # one-element tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Base.Iterators.repeated((X, Y), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5_000\n",
    "X, Y = rewrite(1:N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time Flux.train!(loss, data, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time Flux.train!(loss, dataset, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback() = @show(loss(X, Y))\n",
    "\n",
    "Flux.train!(loss, data, opt; cb = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, dataset, opt; cb = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, dataset, opt; cb = Flux.throttle(callback, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:100\n",
    "    Flux.train!(loss, dataset, opt; cb = Flux.throttle(callback, 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = rewrite(N+1:N+100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(images[N+1])\n",
    "labels[N+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model(X_test[:,1]) Y_test[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(X_test[:,1], Y_test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-confusion",
   "metadata": {},
   "source": [
    "# Autodiff:  <br> Calculus  from another angle \n",
    "(and the special role played by Julia's multiple dispatch and compiler technology)\n",
    "\n",
    "\n",
    "   At the heart of modern machine learning, so popular in (2018),  is an optimization\n",
    "problem.  Optimization means gradients, so suddenly differentiation, especially automatic differentiation, is exciting.\n",
    "\n",
    "\n",
    "  The first time one  hears about automatic differentiation, it is easy to imagine what it is.  Surely it is  straightforward symbolic differentiation applied to code.  One imagines   automatically doing what is  learned  in a calculus class. \n",
    "  <img src=\"http://www2.bc.cc.ca.us/resperic/math6a/lectures/ch5/1/IntegralTable.gif\" width=\"190\">\n",
    "  .... and anyway if it is not that, then it must be finite differences, like one learns in a numerical computing class.\n",
    "  \n",
    "<img src=\"http://image.mathcaptain.com/cms/images/122/Diff%202.png\" width=\"150\">\n",
    "\n",
    "## Babylonian sqrt\n",
    "\n",
    "We start with a simple example, the computation of sqrt(x), where  how autodiff works comes as both a mathematical surprise, and a computing wonder.  The example is  the Babylonian algorithm, known to mankind for millenia, to compute sqrt(x):  \n",
    "\n",
    "\n",
    " > Repeat $ t \\leftarrow  (t+x/t) / 2 $ until $t$ converges to $\\sqrt{x}$.\n",
    " \n",
    " Each iteration has one add and two divides. For illustration purposes, 10 iterations suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "function Babylonian(x; N = 10) \n",
    "    t = (1+x)/2\n",
    "    for i = 2:N; t=(t + x/t)/2  end    \n",
    "    t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "α = π\n",
    "Babylonian(α), √α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=2; Babylonian(x),√x  # Type \\sqrt+<tab> to get the symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"Plotly\")\n",
    "using Plots\n",
    "using Plotly\n",
    "plotly()\n",
    "gr()\n",
    "pyplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Warning first plots load packages, takes time\n",
    "i = 0:.01:49\n",
    "\n",
    "plot([x->Babylonian(x,N=i) for i=1:5],i,label=[\"Iteration $j\" for i=1:1,j=1:5])\n",
    "\n",
    "plot!(sqrt,i,c=\"black\",label=\"sqrt\",\n",
    "      title = \"Those Babylonians really knew how to √\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-above",
   "metadata": {},
   "source": [
    "## ...and now the derivative, almost by magic\n",
    "\n",
    "Eight lines of Julia!  No mention of 1/2 over sqrt(x).\n",
    "D for \"dual number\", invented by the famous algebraist Clifford in 1873.\n",
    "\n",
    "The same algorithm with no rewrite at all computes properly\n",
    "the derivative as the check shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct D <: Number  # D is a function-derivative pair\n",
    "    f::Tuple{Float64,Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-rover",
   "metadata": {},
   "source": [
    "Sum Rule: (x+y)' = x' + y' <br>\n",
    "Quotient Rule: (x/y)' = (yx'-xy') / y^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: +, /, convert, promote_rule\n",
    "+(x::D, y::D) = D(x.f .+ y.f)\n",
    "/(x::D, y::D) = D((x.f[1]/y.f[1], (y.f[1]*x.f[2] - x.f[1]*y.f[2])/y.f[1]^2))\n",
    "convert(::Type{D}, x::Real) = D((x,zero(x)))\n",
    "promote_rule(::Type{D}, ::Type{<:Number}) = D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=49; Babylonian(D((x,1))), (√x,.5/√x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=π; Babylonian(D((x,1))), (√x,.5/√x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-claim",
   "metadata": {},
   "source": [
    "## It just works!\n",
    "\n",
    "How does it work?  We will explain in a moment.  Right now marvel that it does.  Note we did not\n",
    "import any autodiff package.  Everything is just basic vanilla Julia.\n",
    "\n",
    "## The assembler\n",
    "\n",
    "Most folks don't read assembler, but one can see that it is short.\n",
    "The shortness is a clue that suggests speed!\n",
    "\n",
    "## Symbolically\n",
    "\n",
    "We haven't yet explained how it works, but it may be of some value to understand that the below is mathematically\n",
    "equivalent, though not what the computation is doing.\n",
    "\n",
    "Notice in the below that Babylonian works on SymPy symbols.\n",
    "\n",
    "Note: Python and Julia are good friends.  It's not a competition!  Watch how nicely we can use the same code now with SymPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function Babylonian(x; N = 10) \n",
    "    t = (1+x)/2\n",
    "    for i = 2:N; t=(t + x/t)/2  end    \n",
    "    t\n",
    "end  \n",
    "@code_native(Babylonian(D((2,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "Pkg.add(\"SymPy\")\n",
    "using SymPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbols(\"x\")\n",
    "display(\"Iterations as a function of x\")\n",
    "for k = 1:5\n",
    " display( simplify(Babylonian(x,N=k)))\n",
    "end\n",
    "\n",
    "display(\"Derivatives as a function of x\")\n",
    "for k = 1:5\n",
    " display(simplify(diff(simplify(Babylonian(x,N=k)),x)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "function dBabylonian(x; N = 10) \n",
    "    t = (1+x)/2\n",
    "    t′ = 1/2\n",
    "    for i = 1:N;  \n",
    "        t = (t+x/t)/2; \n",
    "        t′= (t′+(t-x*t′)/t^2)/2; \n",
    "    end    \n",
    "    t′\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-feelings",
   "metadata": {},
   "source": [
    "The code is computing answers mathematically equivalent to the functions above, but not symbolically, numerically.\n",
    "\n",
    "## How autodiff is getting the answer\n",
    "Let us by hand take the \"derivative\" of the Babylonian iteration with respect to x. Specifically t′=dt/dx.  This is the old fashioned way of a human rewriting code.\n",
    "\n",
    "See this rewritten code gets the right answer.  So the trick is for the computer system to do it for you, and without any loss of speed or convenience.\n",
    "\n",
    "What just happened?  Answer: We created an iteration by hand for t′ given our iteration for t. Then we ran the iteration alongside the iteration for t.\n",
    "\n",
    "How did this work?  It created the same derivative iteration that we did by hand, using very general rules that are set once and need not be written by hand.\n",
    "\n",
    "Important:: The derivative is substituted before the JIT compiler, and thus efficient compiled code is executed.\n",
    "\n",
    "## Dual Number Notation\n",
    "\n",
    "Instead of D(a,b) we can write a + b ϵ, where ϵ satisfies ϵ^2=0.  (Some people like to recall imaginary numbers where an i is introduced with i^2=-1.) \n",
    "\n",
    "Others like to think of how engineers just drop the O(ϵ^2) terms.\n",
    "\n",
    "The four rules are\n",
    "\n",
    "$ (a+b\\epsilon) \\pm (c+d\\epsilon) = (a+c) \\pm (b+d)\\epsilon$\n",
    "\n",
    "$ (a+b\\epsilon) * (c+d\\epsilon) = (ac) + (bc+ad)\\epsilon$\n",
    "\n",
    "$ (a+b\\epsilon) / (c+d\\epsilon) = (a/c) + (bc-ad)/d^2 \\epsilon $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = π; dBabylonian(x), .5/√x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(D((x,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.show(io::IO,x::D) = print(io,x.f[1],\" + \",x.f[2],\" ϵ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the last two rules\n",
    "import Base: -,*\n",
    "-(x::D, y::D) = D(x.f .- y.f)\n",
    "*(x::D, y::D) = D((x.f[1]*y.f[1], (x.f[2]*y.f[1] + x.f[1]*y.f[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "D((1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "D((0,1))^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "D((2,1)) ^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ = D((0,1))\n",
    "@code_native(ϵ^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ * ϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/(1+ϵ)  # Exact power series:  1-ϵ+ϵ²-ϵ³-..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1+ϵ)^5 ## Note this just works (we didn't train powers)!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-circle",
   "metadata": {},
   "source": [
    "## Generalization to arbitrary roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "function nthroot(x, n=2; t=1, N = 10) \n",
    "    for i = 1:N;   t += (x/t^(n-1)-t)/n; end   \n",
    "    t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "nthroot(2,3), ∛2 # take a cube root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "nthroot(2+ϵ,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "nthroot(7,12), 7^(1/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2.0\n",
    "nthroot( x+ϵ,3), ∛x, 1/x^(2/3)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-vocabulary",
   "metadata": {},
   "source": [
    "## Forward Diff\n",
    "Now that you understand it, you can use the official package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"ForwardDiff\")\n",
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.derivative(sqrt, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "@which ForwardDiff.derivative(sqrt, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "setprecision(3000)\n",
    "round.(Float64.(log10.([Babylonian(BigFloat(2),N=k) for k=1:10] - √BigFloat(2))),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct D1{T} <: Number  # D is a function-derivative pair\n",
    "    f::Tuple{T,T}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = D((2.0,1.0))\n",
    "z1 = D1((BigFloat(2.0),BigFloat(1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: +, /, convert, promote_rule\n",
    "+(x::D1, y::D1) = D1(x.f .+ y.f)\n",
    "/(x::D1, y::D1) = D1((x.f[1]/y.f[1], (y.f[1]*x.f[2] - x.f[1]*y.f[2])/y.f[1]^2))\n",
    "convert(::Type{D1{T}}, x::Real) where {T} = D1((convert(T, x), zero(T)))\n",
    "promote_rule(::Type{D1{T}}, ::Type{S}) where {T,S<:Number} = D1{promote_type(T,S)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = randn(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.gradient(x->x'A*x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "(A+A')*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "Strang = SymTridiagonal(2*ones(n),-ones(n-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-bacteria",
   "metadata": {},
   "source": [
    "##  But wait there's more!\n",
    "\n",
    "Many packages need to be taught how to compute autodiffs of matrix factorications such as the svd or lu.  Julia will \"just do it,\" no\n",
    "teaching necessary for reasons such as the above.  This is illustrated in another notebook, not included here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-theme",
   "metadata": {},
   "source": [
    "# Express path to classifying images\n",
    "\n",
    "In this notebook, we will show how to run classification software similar to how Google images works.\n",
    "\n",
    "Julia allows us to load in various pre-trained models for classifying images, via the `Metalhead.jl` package.\n",
    "\n",
    "Let's download an image of an elephant:\n",
    "\n",
    "We'll use the VGG19 model, which is a deep convolutional neural network trained on a subset of the ImageNet database. As this is your first notebook, very likely the words \"convolutional\", and \"neural net,\" and \"deep,\" may seem mysterious.  At the end of this course these words will no longer be mysterious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"Metalhead\")\n",
    "using Images\n",
    "using Metalhead  # To run type <shift> + enter\n",
    "using Metalhead: classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"http://www.mikebirkhead.com/images/EyeForAnElephant.jpg\", \"elephant.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load(\"elephant.jpg\") # open up a new cell type ESC + b (for below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i=1:28\n",
    "  println(vgg.layers[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(vgg, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load(\"data/philip.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(vgg, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metalhead.imagenet_classes[rand(1:1000,1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = Metalhead.forward(vgg, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = sortperm(probs)\n",
    "probs[273]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Metalhead.imagenet_classes(vgg)[perm] probs[perm]][end:-1:end-10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-income",
   "metadata": {},
   "source": [
    "## What is going on here?\n",
    "\n",
    "VGG19 classifies images according to the following 1000 different classes:\n",
    "\n",
    "The model is a Convolutional Neural Network (CNN), made up of a sequence of layers of \"neurons\" with interconnections. The huge number of parameters making up these interconnections have previously been learnt to correctly predict a set of training images representing each class.\n",
    "\n",
    "Running the model on an image spits out the probability that the model assigns to each class:\n",
    "\n",
    "We can now see which are the most likely few labels:\n",
    "\n",
    "## What are the questions to get a successful classifier via machine learning?\n",
    "\n",
    "The key questions to obtain a successful classifier in machine learning are:\n",
    "\n",
    "- How do we define a suitable model that can model the data adequately?\n",
    "\n",
    "- How do we train it on suitably labelled data?\n",
    "\n",
    "These are the questions that this course is designed to address."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0-beta1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
